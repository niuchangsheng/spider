"""
åŠ¨æ€é¡µé¢è§£æå™¨
ç”¨äºè§£æä½¿ç”¨Ajaxå¼‚æ­¥åŠ è½½å†…å®¹çš„åŠ¨æ€ç½‘é¡µ
"""
from typing import List, Dict, Optional
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from loguru import logger
import re

from parsers.base import BaseParser


class DynamicPageParser(BaseParser):
    """
    åŠ¨æ€é¡µé¢è§£æå™¨
    
    ç»§æ‰¿ BaseParserï¼Œæ·»åŠ åŠ¨æ€é¡µé¢ç‰¹æœ‰åŠŸèƒ½ï¼š
    - è§£ææ–‡ç« åˆ—è¡¨
    - æå–æ–‡ç« åŸºæœ¬ä¿¡æ¯ï¼ˆæ ‡é¢˜ã€ä½œè€…ã€æ—¥æœŸã€æ‘˜è¦ã€é“¾æ¥ï¼‰
    - æ£€æµ‹"æŸ¥çœ‹æ›´å¤š"æŒ‰é’®åŠå…¶åŠ è½½å‚æ•°
    - æ”¯æŒå¤šç§æ—¥æœŸæ ¼å¼ï¼ˆç›¸å¯¹æ—¶é—´ã€ç»å¯¹æ—¶é—´ï¼‰
    - åŸå›¾URLæ™ºèƒ½æå–
    
    Example:
        parser = DynamicPageParser(config)
        articles = parser.parse_articles(html)
        has_more = parser.has_load_more_button(html)
    """
    
    def __init__(self, config):
        """
        åˆå§‹åŒ–åŠ¨æ€é¡µé¢è§£æå™¨
        
        Args:
            config: é…ç½®å¯¹è±¡ï¼ŒåŒ…å«é€‰æ‹©å™¨ç­‰ä¿¡æ¯
        """
        super().__init__(config)
        
        # ä¿å­˜configå¼•ç”¨ï¼ˆåŠ¨æ€é¡µé¢éœ€è¦å®Œæ•´configï¼‰
        self.config = config
        
        # é»˜è®¤æ–‡ç« é€‰æ‹©å™¨ï¼ˆå¯é€šè¿‡é…ç½®è¦†ç›–ï¼‰
        self.article_selector = getattr(config.bbs, 'article_selector', '.article')
        self.title_selector = getattr(config.bbs, 'title_selector', '.title')
        self.author_selector = getattr(config.bbs, 'author_selector', '.author')
        self.date_selector = getattr(config.bbs, 'date_selector', '.date')
        self.summary_selector = getattr(config.bbs, 'summary_selector', '.body')
        self.link_selector = getattr(config.bbs, 'link_selector', 'a[href]')
        
        logger.debug(f"ğŸ”§ åŠ¨æ€é¡µé¢è§£æå™¨åˆå§‹åŒ–å®Œæˆ")
        logger.debug(f"   æ–‡ç« é€‰æ‹©å™¨: {self.article_selector}")
    
    def parse_articles(self, html: str) -> List[Dict]:
        """
        è§£ææ–‡ç« åˆ—è¡¨
        
        ä»HTMLä¸­æå–æ‰€æœ‰æ–‡ç« çš„åŸºæœ¬ä¿¡æ¯ã€‚
        
        Args:
            html: HTMLå†…å®¹
        
        Returns:
            æ–‡ç« ä¿¡æ¯åˆ—è¡¨ï¼Œæ¯ä¸ªæ–‡ç« æ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«ï¼š
            - article_id: æ–‡ç« ID
            - title: æ ‡é¢˜
            - author: ä½œè€…
            - date: å‘å¸ƒæ—¥æœŸ
            - summary: æ‘˜è¦
            - url: è¯¦æƒ…é“¾æ¥
        """
        soup = BeautifulSoup(html, 'html.parser')
        articles = []
        
        # æŸ¥æ‰¾æ‰€æœ‰æ–‡ç« å…ƒç´ 
        article_elements = soup.select(self.article_selector)
        
        logger.debug(f"ğŸ” æ‰¾åˆ° {len(article_elements)} ä¸ªæ–‡ç« å…ƒç´ ")
        
        for i, elem in enumerate(article_elements, 1):
            try:
                article = self._extract_article_info(elem)
                if article:
                    articles.append(article)
                    logger.debug(f"   âœ“ æ–‡ç«  #{i}: {article.get('title', 'N/A')[:30]}")
                else:
                    logger.debug(f"   âœ— æ–‡ç«  #{i}: æå–å¤±è´¥ï¼ˆç¼ºå°‘å¿…è¦å­—æ®µï¼‰")
            except Exception as e:
                logger.error(f"âŒ è§£ææ–‡ç«  #{i} å¤±è´¥: {e}")
                continue
        
        logger.info(f"ğŸ“„ æˆåŠŸè§£æ {len(articles)}/{len(article_elements)} ç¯‡æ–‡ç« ")
        
        return articles
    
    def _extract_article_info(self, element) -> Optional[Dict]:
        """ä»å•ä¸ªæ–‡ç« å…ƒç´ ä¸­æå–ä¿¡æ¯"""
        # æå–æ ‡é¢˜
        title_elem = element.select_one(self.title_selector)
        title = title_elem.get_text(strip=True) if title_elem else None
        
        # æå–ä½œè€…
        author_elem = element.select_one(self.author_selector)
        author = author_elem.get_text(strip=True) if author_elem else "æœªçŸ¥ä½œè€…"
        
        # æå–æ—¥æœŸ
        date_elem = element.select_one(self.date_selector)
        date = date_elem.get_text(strip=True) if date_elem else None
        
        # æå–æ‘˜è¦
        summary_elem = element.select_one(self.summary_selector)
        summary = summary_elem.get_text(strip=True) if summary_elem else ""
        
        # æå–é“¾æ¥ï¼ˆä¼˜å…ˆä»æ ‡é¢˜ä¸­æå–ï¼Œé¿å…æå–åˆ°å›¾ç‰‡é“¾æ¥ï¼‰
        url = None
        
        # æ–¹æ³•1: ä»æ ‡é¢˜å…ƒç´ ä¸­æå–é“¾æ¥ï¼ˆæœ€å¯é ï¼‰
        if title_elem:
            title_link = title_elem.find('a', href=True)
            if title_link:
                url = title_link.get('href')
        
        # æ–¹æ³•2: å¦‚æœæ ‡é¢˜ä¸­æ²¡æœ‰é“¾æ¥ï¼Œä½¿ç”¨link_selector
        if not url:
            link_elem = element.select_one(self.link_selector)
            if link_elem:
                candidate_url = link_elem.get('href', '')
                # æ’é™¤å›¾ç‰‡é“¾æ¥ï¼ˆåŒ…å«å›¾ç‰‡æ‰©å±•åæˆ–å›¾ç‰‡åŸŸåï¼‰
                if candidate_url and not any(ext in candidate_url.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp', 'res.xdcdn.net']):
                    url = candidate_url
        
        # æ–¹æ³•3: å¦‚æœè¿˜æ˜¯æ²¡æœ‰ï¼Œå°è¯•æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥ï¼Œé€‰æ‹©æœ€åƒæ–‡ç« é“¾æ¥çš„
        if not url:
            all_links = element.select('a[href]')
            for link in all_links:
                href = link.get('href', '')
                # æ’é™¤å›¾ç‰‡é“¾æ¥
                if any(ext in href.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp', 'res.xdcdn.net']):
                    continue
                # ä¼˜å…ˆé€‰æ‹©åŒ…å«æ•°å­—çš„é“¾æ¥ï¼ˆå¯èƒ½æ˜¯æ–‡ç« IDï¼‰
                if href and any(c.isdigit() for c in href):
                    url = href
                    break
            # å¦‚æœè¿˜æ˜¯æ²¡æœ‰ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªéå›¾ç‰‡é“¾æ¥
            if not url and all_links:
                for link in all_links:
                    href = link.get('href', '')
                    if not any(ext in href.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp', 'res.xdcdn.net']):
                        url = href
                        break
        
        # éªŒè¯å¿…éœ€å­—æ®µ
        if not title or not url:
            logger.debug(f"   âš ï¸  ç¼ºå°‘å¿…éœ€å­—æ®µ: title={bool(title)}, url={bool(url)}")
            return None
        
        # æå–æ–‡ç« IDï¼ˆä»URLä¸­ï¼‰
        article_id = self._extract_article_id(url)
        
        # ç¡®ä¿URLæ˜¯å®Œæ•´çš„
        if url and not url.startswith('http'):
            base_url = self.config.bbs.base_url
            if url.startswith('/'):
                url = f"{base_url}{url}"
            else:
                url = f"{base_url}/{url}"
        
        return {
            'article_id': article_id,
            'title': title,
            'author': author,
            'date': date,
            'summary': summary,
            'url': url
        }
    
    def _extract_article_id(self, url: str) -> str:
        """ä»URLä¸­æå–æ–‡ç« ID"""
        patterns = [
            r'/(\d+)/?$',           # æœ«å°¾çš„æ•°å­—: /15537
            r'/(\d+)[?&#]',         # æ•°å­—åè·Ÿå‚æ•°: /15537?xx
            r'[?&]id=(\d+)',        # URLå‚æ•°: ?id=15537
            r'[?&]article_id=(\d+)', # URLå‚æ•°: ?article_id=15537
            r'/article/(\d+)',      # è·¯å¾„ä¸­: /article/15537
            r'/news/(\d+)',         # è·¯å¾„ä¸­: /news/15537
        ]
        return self._extract_id(url, patterns)
    
    def has_load_more_button(self, html: str) -> bool:
        """æ£€æŸ¥é¡µé¢æ˜¯å¦è¿˜æœ‰"æŸ¥çœ‹æ›´å¤š"æŒ‰é’®"""
        soup = BeautifulSoup(html, 'html.parser')
        
        # æŸ¥æ‰¾"æŸ¥çœ‹æ›´å¤š"æŒ‰é’®ï¼ˆå¤šç§å¯èƒ½çš„é€‰æ‹©å™¨ï¼‰
        load_more_selectors = [
            'a.more[data-action="switch_page"]',  # ç¥ä»™é“å®˜ç½‘æ ¼å¼
            'a.load-more',                        # é€šç”¨æ ¼å¼1
            'button.load-more',                   # é€šç”¨æ ¼å¼2
            '[data-action*="load"]',              # åŒ…å«loadçš„data-action
            '[data-action*="more"]',              # åŒ…å«moreçš„data-action
        ]
        
        for selector in load_more_selectors:
            element = soup.select_one(selector)
            if element:
                logger.debug(f"âœ“ æ‰¾åˆ°'æŸ¥çœ‹æ›´å¤š'æŒ‰é’®: {selector}")
                return True
        
        # ä¹Ÿæ£€æŸ¥æ–‡æœ¬å†…å®¹
        more_texts = ['æŸ¥çœ‹æ›´å¤š', 'load more', 'åŠ è½½æ›´å¤š', 'show more', 'æ›´å¤š']
        for text in more_texts:
            if soup.find(string=lambda t: text in t.lower() if t else False):
                logger.debug(f"âœ“ æ‰¾åˆ°'æŸ¥çœ‹æ›´å¤š'æ–‡æœ¬: {text}")
                return True
        
        logger.debug("âœ— æœªæ‰¾åˆ°'æŸ¥çœ‹æ›´å¤š'æŒ‰é’®")
        return False
    
    def get_next_page_number(self, html: str) -> Optional[int]:
        """è·å–ä¸‹ä¸€é¡µçš„é¡µç """
        soup = BeautifulSoup(html, 'html.parser')
        
        # æŸ¥æ‰¾å¸¦æœ‰data-pageå±æ€§çš„å…ƒç´ 
        load_more = soup.select_one('[data-page]')
        if load_more:
            try:
                page_num = int(load_more.get('data-page'))
                logger.debug(f"âœ“ ä¸‹ä¸€é¡µé¡µç : {page_num}")
                return page_num
            except (ValueError, TypeError) as e:
                logger.warning(f"âš ï¸  æ— æ³•è§£æé¡µç : {e}")
                return None
        
        logger.debug("âœ— æœªæ‰¾åˆ°data-pageå±æ€§")
        return None
    
    def parse_article_detail(self, html: str, url: str) -> Dict:
        """è§£ææ–‡ç« è¯¦æƒ…é¡µ"""
        soup = BeautifulSoup(html, 'html.parser')
        
        # æŸ¥æ‰¾æ–‡ç« å†…å®¹å®¹å™¨ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼Œè¶Šç²¾ç¡®çš„é€‰æ‹©å™¨è¶Šé å‰ï¼‰
        content_selectors = [
            '.article .body',           # ç¥ä»™é“å®˜ç½‘ - æ–‡ç« æ­£æ–‡ï¼ˆæœ€ç²¾ç¡®ï¼‰
            '#single .article .body',   # ç¥ä»™é“å®˜ç½‘ - å¸¦IDçš„æ›´ç²¾ç¡®é€‰æ‹©å™¨
            '.article-body',            # å¸¸è§å˜ä½“
            '.article-content',         # å¸¸è§å˜ä½“
            '.post-body',               # åšå®¢ç±»
            '.post-content',            # åšå®¢ç±»
            '.news-detail',             # æ–°é—»ç±»
            '.news-content',            # æ–°é—»ç±»
            '.detail-content',          # è¯¦æƒ…é¡µ
            'article .content',         # HTML5 articleæ ‡ç­¾
            '#content',                 # é€šç”¨ID
            '.content',                 # é€šç”¨ç±»ï¼ˆå¯èƒ½å¤ªå®½æ³›ï¼‰
        ]
        
        content_elem = None
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                logger.info(f"âœ“ æ‰¾åˆ°å†…å®¹å®¹å™¨: {selector}")
                text = content_elem.get_text(strip=True)[:100]
                logger.info(f"   å®¹å™¨å†…å®¹é¢„è§ˆ: {text}...")
                imgs = content_elem.find_all('img')
                logger.info(f"   å®¹å™¨å†…å›¾ç‰‡æ•°: {len(imgs)}")
                break
        
        if not content_elem:
            content_elem = soup.find('body')
            logger.info("âš ï¸  æœªæ‰¾åˆ°ç‰¹å®šå®¹å™¨ï¼Œä½¿ç”¨æ•´ä¸ªbodyä½œä¸ºå†…å®¹")
            if content_elem:
                imgs = content_elem.find_all('img')
                logger.info(f"   Bodyå†…å›¾ç‰‡æ•°: {len(imgs)}")
        
        # æå–æ–‡æœ¬å†…å®¹
        content = content_elem.get_text(strip=True) if content_elem else ""
        
        # æå–å›¾ç‰‡ï¼ˆä¼˜å…ˆè·å–åŸå›¾URLï¼‰
        images = []
        if content_elem:
            # æ–¹æ³•1: ä» <a> æ ‡ç­¾è·å–åŸå›¾é“¾æ¥
            for a_tag in content_elem.find_all('a'):
                href = a_tag.get('href', '')
                if href and any(ext in href.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp']):
                    if not href.startswith('http'):
                        href = urljoin(url, href)
                    if href not in images:
                        images.append(href)
            
            # æ–¹æ³•2: ä» <img> æ ‡ç­¾è·å–
            if not images:
                img_tags = content_elem.find_all('img')
                for img in img_tags:
                    original_src = self._get_image_url(img)
                    if original_src:
                        if not original_src.startswith('http'):
                            original_src = urljoin(url, original_src)
                        if original_src not in images:
                            images.append(original_src)
        
        logger.debug(f"âœ“ æå–åˆ° {len(images)} å¼ å›¾ç‰‡")
        
        article_id = self._extract_article_id(url)
        
        return {
            'article_id': article_id,
            'url': url,
            'content': content,
            'images': images
        }
    
    def _get_image_url(self, img_tag) -> Optional[str]:
        """
        é‡å†™åŸºç±»æ–¹æ³•ï¼šä» img æ ‡ç­¾è·å–åŸå›¾ URL
        
        åŠ¨æ€é¡µé¢ç‰¹æœ‰çš„å›¾ç‰‡æå–é€»è¾‘ï¼Œä¼˜å…ˆè·å–åŸå›¾ï¼š
        1. srcset ä¸­æœ€å¤§å°ºå¯¸çš„å›¾ç‰‡
        2. data-srcï¼ˆæ‡’åŠ è½½åŸå›¾ï¼‰
        3. srcï¼ˆå¯èƒ½æ˜¯ç¼©ç•¥å›¾ï¼‰å¹¶å°è¯•å»é™¤å°ºå¯¸åç¼€
        """
        # æ–¹æ³•1: ä» srcset è·å–æœ€å¤§å°ºå¯¸çš„å›¾ç‰‡
        srcset = img_tag.get('srcset', '')
        if srcset:
            max_width = 0
            max_url = None
            for item in srcset.split(','):
                item = item.strip()
                if ' ' in item:
                    parts = item.rsplit(' ', 1)
                    url_part = parts[0].strip()
                    size_part = parts[1].strip()
                    width_match = re.search(r'(\d+)w', size_part)
                    if width_match:
                        width = int(width_match.group(1))
                        if width > max_width:
                            max_width = width
                            max_url = url_part
            if max_url:
                return max_url
        
        # æ–¹æ³•2: ä» data-src è·å–ï¼ˆæ‡’åŠ è½½ï¼‰
        data_src = img_tag.get('data-src')
        if data_src:
            return data_src
        
        # æ–¹æ³•3: ä» src è·å–ï¼Œå¹¶å°è¯•å»é™¤å°ºå¯¸åç¼€è·å–åŸå›¾
        src = img_tag.get('src', '')
        if src:
            original_url = re.sub(r'-\d+x\d+(\.[a-zA-Z]+)$', r'\1', src)
            return original_url
        
        return None
