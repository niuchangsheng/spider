"""
CLIå‘½ä»¤å¤„ç†å‡½æ•°
"""
import asyncio
import re
import os
from typing import Dict, Any
from urllib.parse import urlparse
from loguru import logger

from config import Config, ConfigLoader, get_example_config, get_forum_boards, get_forum_urls, get_news_urls
from spiders import SpiderFactory
from spiders.dynamic_news_spider import DynamicNewsCrawler
from core.downloader import ImageDownloader
from core.checkpoint import CheckpointManager


def _extract_image_filename(url: str) -> str:
    """
    ä»å›¾ç‰‡URLæå–åŸå§‹æ–‡ä»¶å
    
    å¤„ç†é€»è¾‘ï¼š
    1. ä»URLè·¯å¾„æå–æ–‡ä»¶å
    2. å»æ‰å°ºå¯¸åç¼€ï¼ˆå¦‚ -1024x481ï¼‰
    3. ä¿ç•™åŸå§‹æ‰©å±•å
    
    Args:
        url: å›¾ç‰‡URL
    
    Returns:
        æ¸…ç†åçš„æ–‡ä»¶å
    """
    try:
        parsed = urlparse(url)
        filename = os.path.basename(parsed.path)
        
        # å»æ‰å°ºå¯¸åç¼€ï¼ˆå¦‚ -1024x481, -300x200 ç­‰ï¼‰
        clean_name = re.sub(r'-\d+x\d+', '', filename)
        
        # å»æ‰æŸ¥è¯¢å‚æ•°å¯èƒ½å¸¦æ¥çš„åç¼€
        clean_name = clean_name.split('?')[0]
        
        # å¦‚æœæ–‡ä»¶åä¸ºç©ºæˆ–æ— æ•ˆï¼Œç”Ÿæˆä¸€ä¸ªé»˜è®¤å
        if not clean_name or clean_name == '.' or '.' not in clean_name:
            import hashlib
            hash_name = hashlib.md5(url.encode()).hexdigest()[:12]
            clean_name = f"{hash_name}.jpg"
        
        return clean_name
    except Exception:
        import hashlib
        hash_name = hashlib.md5(url.encode()).hexdigest()[:12]
        return f"{hash_name}.jpg"


async def handle_crawl_url(args):
    """å¤„ç† crawl-url å­å‘½ä»¤"""
    print(f"\nğŸ“Œ å‘½ä»¤: çˆ¬å–å•ä¸ªURL")
    print(f"URL: {args.url}")
    
    # 1. åŠ è½½é…ç½®
    if args.auto_detect:
        logger.info(f"ğŸŒ è‡ªåŠ¨æ£€æµ‹é…ç½®: {args.url}")
        config = ConfigLoader.auto_detect(args.url)
    elif args.preset:
        logger.info(f"ğŸ“‹ ä½¿ç”¨è®ºå›ç±»å‹é¢„è®¾: {args.preset}")
        config = ConfigLoader.load(args.preset)
    elif args.config:
        logger.info(f"ğŸ“ ä½¿ç”¨é…ç½®æ–‡ä»¶: {args.config}")
        config = get_example_config(args.config)
    else:
        logger.error("âŒ è¯·æŒ‡å®šé…ç½®æ¥æº: --auto-detect æˆ– --preset æˆ– --config")
        return
    
    # 2. åˆ›å»ºçˆ¬è™«
    spider = SpiderFactory.create(config=config)
    
    # 3. çˆ¬å–URL
    async with spider:
        thread_id = spider.parser._extract_thread_id(args.url)
        thread_info = {
            'url': args.url,
            'thread_id': thread_id,
            'title': f'Thread-{thread_id}',
            'board': config.bbs.name
        }
        
        logger.info(f"ğŸš€ å¼€å§‹çˆ¬å–URL...")
        await spider.crawl_thread(thread_info)
        logger.info(f"âœ… çˆ¬å–å®Œæˆ")
        
        # è¾“å‡ºç»Ÿè®¡
        print_statistics(spider)


async def handle_crawl_urls(args):
    """å¤„ç† crawl-urls å­å‘½ä»¤"""
    print(f"\nğŸ“Œ å‘½ä»¤: çˆ¬å–é…ç½®ä¸­çš„URLåˆ—è¡¨")
    print(f"é…ç½®: {args.config}")
    
    # 1. åŠ è½½é…ç½®
    logger.info(f"ğŸ“ ä½¿ç”¨é…ç½®æ–‡ä»¶: {args.config}")
    config = get_example_config(args.config)
    
    # 2. è·å–URLåˆ—è¡¨
    urls = get_forum_urls(args.config)
    logger.info(f"ğŸ“ ä»é…ç½®æ–‡ä»¶åŠ è½½URL: {len(urls)} ä¸ª")
    
    if not urls:
        logger.error("âŒ é…ç½®æ–‡ä»¶ä¸­æ²¡æœ‰URLsï¼")
        return
    
    # 3. åˆ›å»ºçˆ¬è™«å¹¶å¹¶å‘çˆ¬å–
    spider = SpiderFactory.create(config=config)
    
    async with spider:
        logger.info(f"ğŸš€ å¼€å§‹å¹¶å‘çˆ¬å– {len(urls)} ä¸ªURL...")
        tasks = []
        for url in urls:
            thread_id = spider.parser._extract_thread_id(url)
            thread_info = {
                'url': url,
                'thread_id': thread_id,
                'title': f'Thread-{thread_id}',
                'board': config.bbs.name
            }
            tasks.append(spider.crawl_thread(thread_info))
        
        # ä½¿ç”¨ asyncio.gather å¹¶å‘æ‰§è¡Œ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for r in results if not isinstance(r, Exception))
        failed_count = len(results) - success_count
        logger.info(f"âœ… å®Œæˆ: æˆåŠŸ {success_count}, å¤±è´¥ {failed_count}")
        
        # è¾“å‡ºç»Ÿè®¡
        print_statistics(spider)


async def handle_crawl_board(args):
    """å¤„ç† crawl-board å­å‘½ä»¤"""
    print(f"\nğŸ“Œ å‘½ä»¤: çˆ¬å–å•ä¸ªæ¿å—")
    print(f"æ¿å—URL: {args.board_url}")
    if args.max_pages:
        print(f"æœ€å¤§é¡µæ•°: {args.max_pages}")
    else:
        print(f"æœ€å¤§é¡µæ•°: ä¸é™åˆ¶ï¼ˆçˆ¬å–æ‰€æœ‰é¡µï¼‰")
    
    # é˜Ÿåˆ—ç›¸å…³å‚æ•°
    if hasattr(args, 'max_workers') and args.max_workers:
        print(f"å¹¶å‘æ•°: {args.max_workers} (å‘½ä»¤è¡ŒæŒ‡å®š)")
    if hasattr(args, 'use_adaptive_queue') and args.use_adaptive_queue:
        print(f"é˜Ÿåˆ—æ¨¡å¼: è‡ªé€‚åº”é˜Ÿåˆ—")
    elif hasattr(args, 'use_async_queue') and args.use_async_queue is False:
        print(f"é˜Ÿåˆ—æ¨¡å¼: ä¸²è¡Œæ¨¡å¼ï¼ˆç¦ç”¨å¼‚æ­¥é˜Ÿåˆ—ï¼‰")
    else:
        print(f"é˜Ÿåˆ—æ¨¡å¼: å¼‚æ­¥é˜Ÿåˆ—ï¼ˆé»˜è®¤ï¼‰")
    
    # 1. åŠ è½½é…ç½®
    if args.auto_detect:
        logger.info(f"ğŸŒ è‡ªåŠ¨æ£€æµ‹é…ç½®: {args.board_url}")
        config = ConfigLoader.auto_detect(args.board_url)
    elif args.preset:
        logger.info(f"ğŸ“‹ ä½¿ç”¨è®ºå›ç±»å‹é¢„è®¾: {args.preset}")
        config = ConfigLoader.load(args.preset)
    elif args.config:
        logger.info(f"ğŸ“ ä½¿ç”¨é…ç½®æ–‡ä»¶: {args.config}")
        config = get_example_config(args.config)
    else:
        logger.error("âŒ è¯·æŒ‡å®šé…ç½®æ¥æº: --auto-detect æˆ– --preset æˆ– --config")
        return
    
    # 2. åº”ç”¨é˜Ÿåˆ—ç›¸å…³é…ç½®
    if hasattr(args, 'max_workers') and args.max_workers:
        config.crawler.max_concurrent_requests = args.max_workers
    if hasattr(args, 'use_adaptive_queue') and args.use_adaptive_queue is not None:
        config.crawler.use_adaptive_queue = args.use_adaptive_queue
    if hasattr(args, 'use_async_queue') and args.use_async_queue is not None:
        config.crawler.use_async_queue = args.use_async_queue
    
    # 3. åˆ›å»ºçˆ¬è™«
    spider = SpiderFactory.create(config=config)
    
    # 4. çˆ¬å–æ¿å—
    async with spider:
        logger.info(f"ğŸš€ å¼€å§‹çˆ¬å–æ¿å—...")
        await spider.crawl_board(
            board_url=args.board_url,
            board_name=config.bbs.name,
            max_pages=args.max_pages,
            resume=args.resume,
            start_page=args.start_page
        )
        logger.info(f"âœ… çˆ¬å–å®Œæˆ")
        
        # è¾“å‡ºç»Ÿè®¡
        print_statistics(spider)


async def handle_crawl_boards(args):
    """å¤„ç† crawl-boards å­å‘½ä»¤"""
    print(f"\nğŸ“Œ å‘½ä»¤: çˆ¬å–é…ç½®ä¸­çš„æ‰€æœ‰æ¿å—")
    print(f"é…ç½®: {args.config}")
    if args.max_pages:
        print(f"æ¯ä¸ªæ¿å—æœ€å¤§é¡µæ•°: {args.max_pages}")
    else:
        print(f"æ¯ä¸ªæ¿å—æœ€å¤§é¡µæ•°: ä¸é™åˆ¶ï¼ˆçˆ¬å–æ‰€æœ‰é¡µï¼‰")
    
    # é˜Ÿåˆ—ç›¸å…³å‚æ•°
    if hasattr(args, 'max_workers') and args.max_workers:
        print(f"å¹¶å‘æ•°: {args.max_workers} (å‘½ä»¤è¡ŒæŒ‡å®š)")
    if hasattr(args, 'use_adaptive_queue') and args.use_adaptive_queue:
        print(f"é˜Ÿåˆ—æ¨¡å¼: è‡ªé€‚åº”é˜Ÿåˆ—")
    elif hasattr(args, 'use_async_queue') and args.use_async_queue is False:
        print(f"é˜Ÿåˆ—æ¨¡å¼: ä¸²è¡Œæ¨¡å¼ï¼ˆç¦ç”¨å¼‚æ­¥é˜Ÿåˆ—ï¼‰")
    else:
        print(f"é˜Ÿåˆ—æ¨¡å¼: å¼‚æ­¥é˜Ÿåˆ—ï¼ˆé»˜è®¤ï¼‰")
    
    # 1. åŠ è½½é…ç½®
    logger.info(f"ğŸ“ ä½¿ç”¨é…ç½®æ–‡ä»¶: {args.config}")
    config = get_example_config(args.config)
    
    # 2. åº”ç”¨é˜Ÿåˆ—ç›¸å…³é…ç½®
    if hasattr(args, 'max_workers') and args.max_workers:
        config.crawler.max_concurrent_requests = args.max_workers
    if hasattr(args, 'use_adaptive_queue') and args.use_adaptive_queue is not None:
        config.crawler.use_adaptive_queue = args.use_adaptive_queue
    if hasattr(args, 'use_async_queue') and args.use_async_queue is not None:
        config.crawler.use_async_queue = args.use_async_queue
    
    # 3. è·å–æ¿å—åˆ—è¡¨
    boards_info = get_forum_boards(args.config)
    logger.info(f"ğŸ“ ä»é…ç½®æ–‡ä»¶åŠ è½½æ¿å—: {len(boards_info)} ä¸ª")
    
    if not boards_info:
        logger.error("âŒ é…ç½®æ–‡ä»¶ä¸­æ²¡æœ‰æ¿å—ï¼")
        return
    
    # 4. åˆ›å»ºçˆ¬è™«å¹¶å¹¶å‘çˆ¬å–
    spider = SpiderFactory.create(config=config)
    
    async with spider:
        if args.max_pages:
            logger.info(f"ğŸš€ å¼€å§‹å¹¶å‘çˆ¬å– {len(boards_info)} ä¸ªæ¿å—ï¼ˆæ¯ä¸ªæœ€å¤š {args.max_pages} é¡µï¼‰...")
        else:
            logger.info(f"ğŸš€ å¼€å§‹å¹¶å‘çˆ¬å– {len(boards_info)} ä¸ªæ¿å—ï¼ˆçˆ¬å–æ‰€æœ‰é¡µé¢ï¼‰...")
        
        tasks = []
        for board in boards_info:
            logger.info(f"ğŸ“ æ¿å—: {board['name']} - {board['url']}")
            tasks.append(spider.crawl_board(
                board_url=board['url'],
                board_name=board['name'],
                max_pages=args.max_pages,
                resume=args.resume,
                start_page=args.start_page
            ))
        
        # ä½¿ç”¨ asyncio.gather å¹¶å‘æ‰§è¡Œ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for r in results if not isinstance(r, Exception))
        failed_count = len(results) - success_count
        logger.info(f"âœ… å®Œæˆ: æˆåŠŸ {success_count}, å¤±è´¥ {failed_count}")
        
        # è¾“å‡ºç»Ÿè®¡
        print_statistics(spider)


async def _crawl_single_news_url(crawler, url, args, config):
    """çˆ¬å–å•ä¸ªæ–°é—»URLçš„è¾…åŠ©å‡½æ•°"""
    logger.info(f"ğŸš€ å¼€å§‹çˆ¬å–åŠ¨æ€æ–°é—»é¡µé¢: {url}")
    
    # é€‰æ‹©çˆ¬å–æ–¹å¼
    if args.method == 'ajax':
        articles = await crawler.crawl_dynamic_page_ajax(
            url,
            max_pages=args.max_pages,
            resume=args.resume,
            start_page=args.start_page
        )
    else:  # selenium
        articles = await crawler.crawl_dynamic_page_selenium(
            url,
            max_clicks=args.max_pages
        )
    
    if not articles:
        logger.warning(f"âš ï¸  {url} æ²¡æœ‰æ‰¾åˆ°æ–‡ç« ")
        return 0, 0
    
    logger.info(f"âœ… {url} å‘ç° {len(articles)} ç¯‡æ–‡ç« ")
    
    downloaded_images = 0
    total_images = 0
    
    # æ˜¯å¦ä¸‹è½½æ–‡ç« è¯¦æƒ…å’Œå›¾ç‰‡
    if args.download_images:
        logger.info(f"ğŸš€ å¼€å§‹ä¸‹è½½æ–‡ç« è¯¦æƒ…å’Œå›¾ç‰‡...")
        
        # è·å–é˜Ÿåˆ—é…ç½®
        use_queue = getattr(config.crawler, 'use_async_queue', True)
        if hasattr(args, 'use_async_queue') and args.use_async_queue is not None:
            use_queue = args.use_async_queue
        
        max_workers = getattr(args, 'max_workers', None) or config.crawler.max_concurrent_requests
        use_adaptive = getattr(config.crawler, 'use_adaptive_queue', False)
        if hasattr(args, 'use_adaptive_queue') and args.use_adaptive_queue is not None:
            use_adaptive = args.use_adaptive_queue
        
        # çˆ¬å–æ–‡ç« è¯¦æƒ…ï¼ˆä½¿ç”¨é˜Ÿåˆ—ï¼‰
        full_articles = await crawler.crawl_articles_batch(
            articles,
            use_queue=use_queue,
            max_workers=max_workers,
            use_adaptive=use_adaptive
        )
        
        # ä»URLæå–åŸŸåä½œä¸ºå­˜å‚¨ç›®å½•
        domain = urlparse(url).netloc  # å¦‚ sxd.xd.com
        save_dir = config.image.download_dir / domain
        save_dir.mkdir(parents=True, exist_ok=True)
        
        async with ImageDownloader() as downloader:
            # å‡†å¤‡å›¾ç‰‡ä¸‹è½½ä»»åŠ¡åˆ—è¡¨
            image_tasks = []
            for article in full_articles:
                images = article.get('images', [])
                if not images:
                    continue
                
                total_images += len(images)
                article_id = article.get('article_id', 'unknown')
                
                for img_url in images:
                    # ä»å›¾ç‰‡URLæå–åŸå§‹æ–‡ä»¶å
                    img_filename = _extract_image_filename(img_url)
                    # ç”Ÿæˆæœ€ç»ˆæ–‡ä»¶å: [article_id]_[åŸå§‹å›¾ç‰‡å]
                    final_filename = f"{article_id}_{img_filename}"
                    save_path = save_dir / final_filename
                    
                    metadata = {
                        'article_id': article_id,
                        'title': article.get('title', ''),
                        'article_url': article.get('url', ''),
                        'image_url': img_url
                    }
                    
                    image_tasks.append({
                        'url': img_url,
                        'save_path': save_path,
                        'metadata': metadata
                    })
            
            # ä½¿ç”¨é˜Ÿåˆ—å¹¶å‘ä¸‹è½½å›¾ç‰‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if use_queue and image_tasks:
                from core.crawl_queue import CrawlQueue, AdaptiveCrawlQueue
                
                workers = max_workers or config.crawler.max_concurrent_requests or 5
                queue_size = config.crawler.queue_size or 1000
                
                if use_adaptive:
                    queue = AdaptiveCrawlQueue(
                        initial_workers=workers,
                        max_workers=workers * 2,
                        min_workers=1,
                        queue_size=queue_size
                    )
                    logger.info(f"ğŸ¯ ä½¿ç”¨è‡ªé€‚åº”é˜Ÿåˆ—ä¸‹è½½å›¾ç‰‡: åˆå§‹å¹¶å‘={workers}")
                else:
                    queue = CrawlQueue(max_workers=workers, queue_size=queue_size)
                    logger.info(f"ğŸš€ ä½¿ç”¨å¼‚æ­¥é˜Ÿåˆ—ä¸‹è½½å›¾ç‰‡: å¹¶å‘æ•°={workers}")
                
                # å®šä¹‰å›¾ç‰‡ä¸‹è½½ä»»åŠ¡å‡½æ•°
                downloaded_count = 0
                results_container = []
                
                async def download_image_task_with_result(task_info):
                    result = await downloader.download_image(
                        task_info['url'],
                        task_info['save_path'],
                        task_info['metadata']
                    )
                    if result.get('success'):
                        results_container.append(1)
                    return result.get('success', False)
                
                await queue.run(image_tasks, download_image_task_with_result)
                downloaded_images = len(results_container)
            else:
                # ä¸²è¡Œä¸‹è½½ï¼ˆå…¼å®¹æ¨¡å¼ï¼‰
                logger.debug("ğŸ“ ä½¿ç”¨ä¸²è¡Œæ¨¡å¼ä¸‹è½½å›¾ç‰‡")
                for task_info in image_tasks:
                    result = await downloader.download_image(
                        task_info['url'],
                        task_info['save_path'],
                        task_info['metadata']
                    )
                    if result.get('success'):
                        downloaded_images += 1
                    
                    # æ·»åŠ å»¶è¿Ÿ
                    await asyncio.sleep(config.crawler.download_delay)
        
        logger.success(f"âœ… {url} å›¾ç‰‡ä¸‹è½½å®Œæˆ: {downloaded_images}/{total_images}")
    
    return len(articles), downloaded_images


async def handle_crawl_news(args):
    """å¤„ç† crawl-news å­å‘½ä»¤"""
    print(f"\nğŸ“Œ å‘½ä»¤: çˆ¬å–åŠ¨æ€æ–°é—»é¡µé¢")
    print(f"æ–¹å¼: {args.method}")
    if args.max_pages:
        print(f"æœ€å¤§é¡µæ•°: {args.max_pages}")
    else:
        print(f"æœ€å¤§é¡µæ•°: ä¸é™åˆ¶ï¼ˆçˆ¬å–æ‰€æœ‰é¡µï¼‰")
    
    # é˜Ÿåˆ—ç›¸å…³å‚æ•°
    if hasattr(args, 'max_workers') and args.max_workers:
        print(f"å¹¶å‘æ•°: {args.max_workers} (å‘½ä»¤è¡ŒæŒ‡å®š)")
    if hasattr(args, 'use_adaptive_queue') and args.use_adaptive_queue:
        print(f"é˜Ÿåˆ—æ¨¡å¼: è‡ªé€‚åº”é˜Ÿåˆ—")
    elif hasattr(args, 'use_async_queue') and args.use_async_queue is False:
        print(f"é˜Ÿåˆ—æ¨¡å¼: ä¸²è¡Œæ¨¡å¼ï¼ˆç¦ç”¨å¼‚æ­¥é˜Ÿåˆ—ï¼‰")
    else:
        print(f"é˜Ÿåˆ—æ¨¡å¼: å¼‚æ­¥é˜Ÿåˆ—ï¼ˆé»˜è®¤ï¼‰")
    
    # ç¡®å®šè¦çˆ¬å–çš„URLåˆ—è¡¨
    news_urls = []
    
    if args.config:
        # ä»é…ç½®æ–‡ä»¶è¯»å–news URLs
        logger.info(f"ğŸ“ ä½¿ç”¨é…ç½®æ–‡ä»¶: {args.config}")
        config = get_example_config(args.config)
        news_urls = get_news_urls(args.config)
        
        if not news_urls:
            logger.error(f"âŒ é…ç½®æ–‡ä»¶ {args.config} ä¸­æ²¡æœ‰æ‰¾åˆ° news_urlsï¼")
            return
        
        logger.info(f"ğŸ“ ä»é…ç½®æ–‡ä»¶åŠ è½½ {len(news_urls)} ä¸ªæ–°é—»URL")
        for url in news_urls:
            print(f"  - {url}")
    elif args.url:
        # ä½¿ç”¨å‘½ä»¤è¡Œæä¾›çš„å•ä¸ªURL
        news_urls = [args.url]
        print(f"URL: {args.url}")
        
        # åˆ›å»ºé»˜è®¤é…ç½®
        logger.info(f"ğŸŒ ä½¿ç”¨é»˜è®¤é…ç½®")
        parsed = urlparse(args.url)
        base_url = f"{parsed.scheme}://{parsed.netloc}"
        
        config = Config(
            bbs={
                "name": "åŠ¨æ€æ–°é—»ç½‘ç«™",
                "base_url": base_url,
                "forum_type": "custom"
            }
        )
    else:
        logger.error("âŒ è¯·æä¾›URLæˆ–ä½¿ç”¨--configå‚æ•°æŒ‡å®šé…ç½®æ–‡ä»¶ï¼")
        return
    
    # åº”ç”¨é˜Ÿåˆ—ç›¸å…³é…ç½®
    if hasattr(args, 'max_workers') and args.max_workers:
        config.crawler.max_concurrent_requests = args.max_workers
    if hasattr(args, 'use_adaptive_queue') and args.use_adaptive_queue is not None:
        config.crawler.use_adaptive_queue = args.use_adaptive_queue
    if hasattr(args, 'use_async_queue') and args.use_async_queue is not None:
        config.crawler.use_async_queue = args.use_async_queue
    
    # åˆ›å»ºçˆ¬è™«
    crawler = DynamicNewsCrawler(config)
    
    # çˆ¬å–æ‰€æœ‰URL
    async with crawler:
        total_articles = 0
        total_downloaded_images = 0
        
        for url in news_urls:
            articles_count, images_count = await _crawl_single_news_url(
                crawler, url, args, config
            )
            total_articles += articles_count
            total_downloaded_images += images_count
        
        # è¾“å‡ºç»Ÿè®¡
        stats = crawler.get_statistics()
        print("\n" + "=" * 60)
        print("ğŸ“Š çˆ¬å–ç»Ÿè®¡:")
        print(f"  çˆ¬å–URLæ•°: {len(news_urls)}")
        print(f"  å‘ç°æ–‡ç« : {total_articles}")
        if args.download_images:
            print(f"  çˆ¬å–è¯¦æƒ…: {stats['articles_crawled']}")
            print(f"  çˆ¬å–å¤±è´¥: {stats['articles_failed']}")
            print(f"  ä¸‹è½½å›¾ç‰‡: {total_downloaded_images}")
        print("=" * 60)


def print_statistics(spider):
    """è¾“å‡ºç»Ÿè®¡ä¿¡æ¯"""
    stats = spider.get_statistics()
    print("\n" + "=" * 60)
    print("ğŸ“Š çˆ¬å–ç»Ÿè®¡:")
    print(f"  å¸–å­æ•°: {stats['threads_crawled']}")
    print(f"  å‘ç°å›¾ç‰‡: {stats['images_found']}")
    print(f"  ä¸‹è½½æˆåŠŸ: {stats['images_downloaded']}")
    print(f"  ä¸‹è½½å¤±è´¥: {stats['images_failed']}")
    print(f"  å»é‡è·³è¿‡: {stats['duplicates_skipped']}")
    print("=" * 60)


async def handle_checkpoint_status(args):
    """å¤„ç† checkpoint-status å­å‘½ä»¤ï¼ˆæ£€æŸ¥ç‚¹åŸºäº Storageï¼Œéœ€å…ˆè¿æ¥ï¼‰"""
    from core.storage import storage

    print(f"\nğŸ“Œ å‘½ä»¤: æŸ¥çœ‹æ£€æŸ¥ç‚¹çŠ¶æ€")
    print(f"ç½‘ç«™: {args.site}")
    print(f"æ¿å—: {args.board}")
    storage.connect()
    try:
        checkpoint = CheckpointManager(site=args.site, board=args.board)
        if args.clear:
            if checkpoint.exists():
                checkpoint.clear_checkpoint()
                print("âœ… æ£€æŸ¥ç‚¹å·²æ¸…é™¤")
            else:
                print("â„¹ï¸  æ²¡æœ‰æ‰¾åˆ°æ£€æŸ¥ç‚¹")
            return

        if not checkpoint.exists():
            print("â„¹ï¸  æ²¡æœ‰æ‰¾åˆ°æ£€æŸ¥ç‚¹")
            print(f"   å­˜å‚¨: {checkpoint.checkpoint_file}")
            return

        data = checkpoint.load_checkpoint()
        if not data:
            print("âŒ æ— æ³•åŠ è½½æ£€æŸ¥ç‚¹æ•°æ®")
            return

        print("\n" + "=" * 60)
        print("ğŸ“‚ æ£€æŸ¥ç‚¹ä¿¡æ¯:")
        print(f"  å­˜å‚¨: {checkpoint.checkpoint_file}")
        print(f"  çŠ¶æ€: {data.get('status', 'unknown')}")
        print(f"  å½“å‰é¡µ: {data.get('current_page', 0)}")
        print(f"  æœ€åå¸–å­ID: {data.get('last_thread_id', 'N/A')}")
        print(f"  åˆ›å»ºæ—¶é—´: {data.get('created_at', 'N/A')}")
        print(f"  æ›´æ–°æ—¶é—´: {data.get('last_update_time', 'N/A')}")

        seen_article_ids = data.get('seen_article_ids', [])
        if seen_article_ids:
            print(f"\nğŸ“‹ æ–‡ç« IDä¿¡æ¯:")
            print(f"  å·²çˆ¬å–æ–‡ç« æ•°: {len(seen_article_ids)}")
            if data.get('min_article_id'):
                print(f"  æœ€å°æ–‡ç« ID: {data.get('min_article_id')}")
            if data.get('max_article_id'):
                print(f"  æœ€å¤§æ–‡ç« ID: {data.get('max_article_id')}")

        stats = data.get('stats', {})
        if stats:
            print("\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
            print(f"  å·²çˆ¬å–å¸–å­: {stats.get('crawled_count', stats.get('articles_found', 0))}")
            print(f"  ä¸‹è½½å›¾ç‰‡: {stats.get('images_downloaded', 0)}")
            print(f"  å¤±è´¥æ•°: {stats.get('failed_count', 0)}")
            if 'last_error' in stats:
                print(f"  æœ€åé”™è¯¯: {stats['last_error']}")

        print("=" * 60)
    finally:
        storage.close()
