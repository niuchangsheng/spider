"""
CLIå‘½ä»¤å¤„ç†å‡½æ•°
"""
import asyncio
from typing import Dict, Any
from urllib.parse import urlparse
from loguru import logger

from config import Config, ConfigLoader, get_example_config, get_forum_boards, get_forum_urls, get_news_urls
from spiders import SpiderFactory
from spiders.dynamic_news_spider import DynamicNewsCrawler
from core.checkpoint import CheckpointManager


async def handle_crawl_url(args):
    """å¤„ç† crawl-url å­å‘½ä»¤"""
    print(f"\nğŸ“Œ å‘½ä»¤: çˆ¬å–å•ä¸ªURL")
    print(f"URL: {args.url}")
    
    # 1. åŠ è½½é…ç½®
    if args.auto_detect:
        logger.info(f"ğŸŒ è‡ªåŠ¨æ£€æµ‹é…ç½®: {args.url}")
        config = ConfigLoader.auto_detect(args.url)
    elif args.preset:
        logger.info(f"ğŸ“‹ ä½¿ç”¨è®ºå›ç±»å‹é¢„è®¾: {args.preset}")
        config = ConfigLoader.load(args.preset)
    elif args.config:
        logger.info(f"ğŸ“ ä½¿ç”¨é…ç½®æ–‡ä»¶: {args.config}")
        config = get_example_config(args.config)
    else:
        logger.error("âŒ è¯·æŒ‡å®šé…ç½®æ¥æº: --auto-detect æˆ– --preset æˆ– --config")
        return
    
    # 2. åˆ›å»ºçˆ¬è™«
    spider = SpiderFactory.create(config=config)
    
    # 3. çˆ¬å–URL
    async with spider:
        thread_id = spider.parser._extract_thread_id(args.url)
        thread_info = {
            'url': args.url,
            'thread_id': thread_id,
            'title': f'Thread-{thread_id}',
            'board': config.bbs.name
        }
        
        logger.info(f"ğŸš€ å¼€å§‹çˆ¬å–URL...")
        await spider.crawl_thread(thread_info)
        logger.info(f"âœ… çˆ¬å–å®Œæˆ")
        
        # è¾“å‡ºç»Ÿè®¡
        print_statistics(spider)


async def handle_crawl_urls(args):
    """å¤„ç† crawl-urls å­å‘½ä»¤"""
    print(f"\nğŸ“Œ å‘½ä»¤: çˆ¬å–é…ç½®ä¸­çš„URLåˆ—è¡¨")
    print(f"é…ç½®: {args.config}")
    
    # 1. åŠ è½½é…ç½®
    logger.info(f"ğŸ“ ä½¿ç”¨é…ç½®æ–‡ä»¶: {args.config}")
    config = get_example_config(args.config)
    
    # 2. è·å–URLåˆ—è¡¨
    urls = get_forum_urls(args.config)
    logger.info(f"ğŸ“ ä»é…ç½®æ–‡ä»¶åŠ è½½URL: {len(urls)} ä¸ª")
    
    if not urls:
        logger.error("âŒ é…ç½®æ–‡ä»¶ä¸­æ²¡æœ‰URLsï¼")
        return
    
    # 3. åˆ›å»ºçˆ¬è™«å¹¶å¹¶å‘çˆ¬å–
    spider = SpiderFactory.create(config=config)
    
    async with spider:
        logger.info(f"ğŸš€ å¼€å§‹å¹¶å‘çˆ¬å– {len(urls)} ä¸ªURL...")
        tasks = []
        for url in urls:
            thread_id = spider.parser._extract_thread_id(url)
            thread_info = {
                'url': url,
                'thread_id': thread_id,
                'title': f'Thread-{thread_id}',
                'board': config.bbs.name
            }
            tasks.append(spider.crawl_thread(thread_info))
        
        # ä½¿ç”¨ asyncio.gather å¹¶å‘æ‰§è¡Œ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for r in results if not isinstance(r, Exception))
        failed_count = len(results) - success_count
        logger.info(f"âœ… å®Œæˆ: æˆåŠŸ {success_count}, å¤±è´¥ {failed_count}")
        
        # è¾“å‡ºç»Ÿè®¡
        print_statistics(spider)


async def handle_crawl_board(args):
    """å¤„ç† crawl-board å­å‘½ä»¤"""
    print(f"\nğŸ“Œ å‘½ä»¤: çˆ¬å–å•ä¸ªæ¿å—")
    print(f"æ¿å—URL: {args.board_url}")
    if args.max_pages:
        print(f"æœ€å¤§é¡µæ•°: {args.max_pages}")
    else:
        print(f"æœ€å¤§é¡µæ•°: ä¸é™åˆ¶ï¼ˆçˆ¬å–æ‰€æœ‰é¡µï¼‰")
    
    # é˜Ÿåˆ—ç›¸å…³å‚æ•°
    if hasattr(args, 'max_workers') and args.max_workers:
        print(f"å¹¶å‘æ•°: {args.max_workers} (å‘½ä»¤è¡ŒæŒ‡å®š)")
    if hasattr(args, 'use_adaptive_queue') and args.use_adaptive_queue:
        print(f"é˜Ÿåˆ—æ¨¡å¼: è‡ªé€‚åº”é˜Ÿåˆ—")
    elif hasattr(args, 'use_async_queue') and args.use_async_queue is False:
        print(f"é˜Ÿåˆ—æ¨¡å¼: ä¸²è¡Œæ¨¡å¼ï¼ˆç¦ç”¨å¼‚æ­¥é˜Ÿåˆ—ï¼‰")
    else:
        print(f"é˜Ÿåˆ—æ¨¡å¼: å¼‚æ­¥é˜Ÿåˆ—ï¼ˆé»˜è®¤ï¼‰")
    
    # 1. åŠ è½½é…ç½®
    if args.auto_detect:
        logger.info(f"ğŸŒ è‡ªåŠ¨æ£€æµ‹é…ç½®: {args.board_url}")
        config = ConfigLoader.auto_detect(args.board_url)
    elif args.preset:
        logger.info(f"ğŸ“‹ ä½¿ç”¨è®ºå›ç±»å‹é¢„è®¾: {args.preset}")
        config = ConfigLoader.load(args.preset)
    elif args.config:
        logger.info(f"ğŸ“ ä½¿ç”¨é…ç½®æ–‡ä»¶: {args.config}")
        config = get_example_config(args.config)
    else:
        logger.error("âŒ è¯·æŒ‡å®šé…ç½®æ¥æº: --auto-detect æˆ– --preset æˆ– --config")
        return
    
    # 2. åº”ç”¨é˜Ÿåˆ—ç›¸å…³é…ç½®
    if hasattr(args, 'max_workers') and args.max_workers:
        config.crawler.max_concurrent_requests = args.max_workers
    if hasattr(args, 'use_adaptive_queue') and args.use_adaptive_queue is not None:
        config.crawler.use_adaptive_queue = args.use_adaptive_queue
    if hasattr(args, 'use_async_queue') and args.use_async_queue is not None:
        config.crawler.use_async_queue = args.use_async_queue
    
    # 3. åˆ›å»ºçˆ¬è™«
    spider = SpiderFactory.create(config=config)
    
    # 4. çˆ¬å–æ¿å—
    async with spider:
        logger.info(f"ğŸš€ å¼€å§‹çˆ¬å–æ¿å—...")
        await spider.crawl_board(
            board_url=args.board_url,
            board_name=config.bbs.name,
            max_pages=args.max_pages,
            resume=args.resume,
            start_page=args.start_page
        )
        logger.info(f"âœ… çˆ¬å–å®Œæˆ")
        
        # è¾“å‡ºç»Ÿè®¡
        print_statistics(spider)


async def handle_crawl_boards(args):
    """å¤„ç† crawl-boards å­å‘½ä»¤"""
    print(f"\nğŸ“Œ å‘½ä»¤: çˆ¬å–é…ç½®ä¸­çš„æ‰€æœ‰æ¿å—")
    print(f"é…ç½®: {args.config}")
    if args.max_pages:
        print(f"æ¯ä¸ªæ¿å—æœ€å¤§é¡µæ•°: {args.max_pages}")
    else:
        print(f"æ¯ä¸ªæ¿å—æœ€å¤§é¡µæ•°: ä¸é™åˆ¶ï¼ˆçˆ¬å–æ‰€æœ‰é¡µï¼‰")
    
    # é˜Ÿåˆ—ç›¸å…³å‚æ•°
    if hasattr(args, 'max_workers') and args.max_workers:
        print(f"å¹¶å‘æ•°: {args.max_workers} (å‘½ä»¤è¡ŒæŒ‡å®š)")
    if hasattr(args, 'use_adaptive_queue') and args.use_adaptive_queue:
        print(f"é˜Ÿåˆ—æ¨¡å¼: è‡ªé€‚åº”é˜Ÿåˆ—")
    elif hasattr(args, 'use_async_queue') and args.use_async_queue is False:
        print(f"é˜Ÿåˆ—æ¨¡å¼: ä¸²è¡Œæ¨¡å¼ï¼ˆç¦ç”¨å¼‚æ­¥é˜Ÿåˆ—ï¼‰")
    else:
        print(f"é˜Ÿåˆ—æ¨¡å¼: å¼‚æ­¥é˜Ÿåˆ—ï¼ˆé»˜è®¤ï¼‰")
    
    # 1. åŠ è½½é…ç½®
    logger.info(f"ğŸ“ ä½¿ç”¨é…ç½®æ–‡ä»¶: {args.config}")
    config = get_example_config(args.config)
    
    # 2. åº”ç”¨é˜Ÿåˆ—ç›¸å…³é…ç½®
    if hasattr(args, 'max_workers') and args.max_workers:
        config.crawler.max_concurrent_requests = args.max_workers
    if hasattr(args, 'use_adaptive_queue') and args.use_adaptive_queue is not None:
        config.crawler.use_adaptive_queue = args.use_adaptive_queue
    if hasattr(args, 'use_async_queue') and args.use_async_queue is not None:
        config.crawler.use_async_queue = args.use_async_queue
    
    # 3. è·å–æ¿å—åˆ—è¡¨
    boards_info = get_forum_boards(args.config)
    logger.info(f"ğŸ“ ä»é…ç½®æ–‡ä»¶åŠ è½½æ¿å—: {len(boards_info)} ä¸ª")
    
    if not boards_info:
        logger.error("âŒ é…ç½®æ–‡ä»¶ä¸­æ²¡æœ‰æ¿å—ï¼")
        return
    
    # 4. åˆ›å»ºçˆ¬è™«å¹¶å¹¶å‘çˆ¬å–
    spider = SpiderFactory.create(config=config)
    
    async with spider:
        if args.max_pages:
            logger.info(f"ğŸš€ å¼€å§‹å¹¶å‘çˆ¬å– {len(boards_info)} ä¸ªæ¿å—ï¼ˆæ¯ä¸ªæœ€å¤š {args.max_pages} é¡µï¼‰...")
        else:
            logger.info(f"ğŸš€ å¼€å§‹å¹¶å‘çˆ¬å– {len(boards_info)} ä¸ªæ¿å—ï¼ˆçˆ¬å–æ‰€æœ‰é¡µé¢ï¼‰...")
        
        tasks = []
        for board in boards_info:
            logger.info(f"ğŸ“ æ¿å—: {board['name']} - {board['url']}")
            tasks.append(spider.crawl_board(
                board_url=board['url'],
                board_name=board['name'],
                max_pages=args.max_pages,
                resume=args.resume,
                start_page=args.start_page
            ))
        
        # ä½¿ç”¨ asyncio.gather å¹¶å‘æ‰§è¡Œ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for r in results if not isinstance(r, Exception))
        failed_count = len(results) - success_count
        logger.info(f"âœ… å®Œæˆ: æˆåŠŸ {success_count}, å¤±è´¥ {failed_count}")
        
        # è¾“å‡ºç»Ÿè®¡
        print_statistics(spider)


async def handle_crawl_news(args):
    """å¤„ç† crawl-news å­å‘½ä»¤"""
    print(f"\nğŸ“Œ å‘½ä»¤: çˆ¬å–åŠ¨æ€æ–°é—»é¡µé¢")
    print(f"æ–¹å¼: {args.method}")
    if args.max_pages:
        print(f"æœ€å¤§é¡µæ•°: {args.max_pages}")
    else:
        print(f"æœ€å¤§é¡µæ•°: ä¸é™åˆ¶ï¼ˆçˆ¬å–æ‰€æœ‰é¡µï¼‰")
    
    # é˜Ÿåˆ—ç›¸å…³å‚æ•°
    if hasattr(args, 'max_workers') and args.max_workers:
        print(f"å¹¶å‘æ•°: {args.max_workers} (å‘½ä»¤è¡ŒæŒ‡å®š)")
    if hasattr(args, 'use_adaptive_queue') and args.use_adaptive_queue:
        print(f"é˜Ÿåˆ—æ¨¡å¼: è‡ªé€‚åº”é˜Ÿåˆ—")
    elif hasattr(args, 'use_async_queue') and args.use_async_queue is False:
        print(f"é˜Ÿåˆ—æ¨¡å¼: ä¸²è¡Œæ¨¡å¼ï¼ˆç¦ç”¨å¼‚æ­¥é˜Ÿåˆ—ï¼‰")
    else:
        print(f"é˜Ÿåˆ—æ¨¡å¼: å¼‚æ­¥é˜Ÿåˆ—ï¼ˆé»˜è®¤ï¼‰")
    
    # ç¡®å®šè¦çˆ¬å–çš„URLåˆ—è¡¨
    news_urls = []
    
    if args.config:
        # ä»é…ç½®æ–‡ä»¶è¯»å–news URLs
        logger.info(f"ğŸ“ ä½¿ç”¨é…ç½®æ–‡ä»¶: {args.config}")
        config = get_example_config(args.config)
        news_urls = get_news_urls(args.config)
        
        if not news_urls:
            logger.error(f"âŒ é…ç½®æ–‡ä»¶ {args.config} ä¸­æ²¡æœ‰æ‰¾åˆ° news_urlsï¼")
            return
        
        logger.info(f"ğŸ“ ä»é…ç½®æ–‡ä»¶åŠ è½½ {len(news_urls)} ä¸ªæ–°é—»URL")
        for url in news_urls:
            print(f"  - {url}")
    elif args.url:
        # ä½¿ç”¨å‘½ä»¤è¡Œæä¾›çš„å•ä¸ªURL
        news_urls = [args.url]
        print(f"URL: {args.url}")
        
        # åˆ›å»ºé»˜è®¤é…ç½®
        logger.info(f"ğŸŒ ä½¿ç”¨é»˜è®¤é…ç½®")
        parsed = urlparse(args.url)
        base_url = f"{parsed.scheme}://{parsed.netloc}"
        
        config = Config(
            bbs={
                "name": "åŠ¨æ€æ–°é—»ç½‘ç«™",
                "base_url": base_url,
                "forum_type": "custom"
            }
        )
    else:
        logger.error("âŒ è¯·æä¾›URLæˆ–ä½¿ç”¨--configå‚æ•°æŒ‡å®šé…ç½®æ–‡ä»¶ï¼")
        return
    
    # åº”ç”¨é˜Ÿåˆ—ç›¸å…³é…ç½®
    if hasattr(args, 'max_workers') and args.max_workers:
        config.crawler.max_concurrent_requests = args.max_workers
    if hasattr(args, 'use_adaptive_queue') and args.use_adaptive_queue is not None:
        config.crawler.use_adaptive_queue = args.use_adaptive_queue
    if hasattr(args, 'use_async_queue') and args.use_async_queue is not None:
        config.crawler.use_async_queue = args.use_async_queue
    
    crawler = DynamicNewsCrawler(config)
    total_articles = 0
    total_downloaded_images = 0
    async with crawler:
        for url in news_urls:
            articles_count, images_count = await crawler.crawl_news_and_download_images(
                url,
                max_pages=args.max_pages,
                resume=args.resume,
                start_page=args.start_page,
                download_images=args.download_images,
                method=args.method,
            )
            total_articles += articles_count
            total_downloaded_images += images_count
    stats = crawler.get_statistics()
    print("\n" + "=" * 60)
    print("ğŸ“Š çˆ¬å–ç»Ÿè®¡:")
    print(f"  çˆ¬å–URLæ•°: {len(news_urls)}")
    print(f"  å‘ç°æ–‡ç« : {total_articles}")
    if args.download_images:
        print(f"  çˆ¬å–è¯¦æƒ…: {stats['articles_crawled']}")
        print(f"  çˆ¬å–å¤±è´¥: {stats['articles_failed']}")
        print(f"  ä¸‹è½½å›¾ç‰‡: {total_downloaded_images}")
    print("=" * 60)


def print_statistics(spider):
    """è¾“å‡ºç»Ÿè®¡ä¿¡æ¯"""
    stats = spider.get_statistics()
    print("\n" + "=" * 60)
    print("ğŸ“Š çˆ¬å–ç»Ÿè®¡:")
    print(f"  å¸–å­æ•°: {stats['threads_crawled']}")
    print(f"  å‘ç°å›¾ç‰‡: {stats['images_found']}")
    print(f"  ä¸‹è½½æˆåŠŸ: {stats['images_downloaded']}")
    print(f"  ä¸‹è½½å¤±è´¥: {stats['images_failed']}")
    print(f"  å»é‡è·³è¿‡: {stats['duplicates_skipped']}")
    print("=" * 60)


async def handle_checkpoint_status(args):
    """å¤„ç† checkpoint-status å­å‘½ä»¤ï¼ˆæ£€æŸ¥ç‚¹åŸºäº Storageï¼Œéœ€å…ˆè¿æ¥ï¼‰"""
    from core.storage import storage

    print(f"\nğŸ“Œ å‘½ä»¤: æŸ¥çœ‹æ£€æŸ¥ç‚¹çŠ¶æ€")
    print(f"ç½‘ç«™: {args.site}")
    print(f"æ¿å—: {args.board}")
    storage.connect()
    try:
        checkpoint = CheckpointManager(site=args.site, board=args.board)
        if args.clear:
            if checkpoint.exists():
                checkpoint.clear_checkpoint()
                print("âœ… æ£€æŸ¥ç‚¹å·²æ¸…é™¤")
            else:
                print("â„¹ï¸  æ²¡æœ‰æ‰¾åˆ°æ£€æŸ¥ç‚¹")
            return

        if not checkpoint.exists():
            print("â„¹ï¸  æ²¡æœ‰æ‰¾åˆ°æ£€æŸ¥ç‚¹")
            print(f"   å­˜å‚¨: {checkpoint.checkpoint_file}")
            return

        data = checkpoint.load_checkpoint()
        if not data:
            print("âŒ æ— æ³•åŠ è½½æ£€æŸ¥ç‚¹æ•°æ®")
            return

        print("\n" + "=" * 60)
        print("ğŸ“‚ æ£€æŸ¥ç‚¹ä¿¡æ¯:")
        print(f"  å­˜å‚¨: {checkpoint.checkpoint_file}")
        print(f"  çŠ¶æ€: {data.get('status', 'unknown')}")
        print(f"  å½“å‰é¡µ: {data.get('current_page', 0)}")
        print(f"  æœ€åå¸–å­ID: {data.get('last_thread_id', 'N/A')}")
        print(f"  åˆ›å»ºæ—¶é—´: {data.get('created_at', 'N/A')}")
        print(f"  æ›´æ–°æ—¶é—´: {data.get('last_update_time', 'N/A')}")

        seen_article_ids = data.get('seen_article_ids', [])
        if seen_article_ids:
            print(f"\nğŸ“‹ æ–‡ç« IDä¿¡æ¯:")
            print(f"  å·²çˆ¬å–æ–‡ç« æ•°: {len(seen_article_ids)}")
            if data.get('min_article_id'):
                print(f"  æœ€å°æ–‡ç« ID: {data.get('min_article_id')}")
            if data.get('max_article_id'):
                print(f"  æœ€å¤§æ–‡ç« ID: {data.get('max_article_id')}")

        stats = data.get('stats', {})
        if stats:
            print("\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
            print(f"  å·²çˆ¬å–å¸–å­: {stats.get('crawled_count', stats.get('articles_found', 0))}")
            print(f"  ä¸‹è½½å›¾ç‰‡: {stats.get('images_downloaded', 0)}")
            print(f"  å¤±è´¥æ•°: {stats.get('failed_count', 0)}")
            if 'last_error' in stats:
                print(f"  æœ€åé”™è¯¯: {stats['last_error']}")

        print("=" * 60)
    finally:
        storage.close()
