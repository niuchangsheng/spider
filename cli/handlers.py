"""
CLIå‘½ä»¤å¤„ç†å‡½æ•°
"""
import asyncio
import re
import os
from typing import Dict, Any
from urllib.parse import urlparse
from loguru import logger

from config import Config, ConfigLoader, get_example_config, get_forum_boards, get_forum_urls
from spiders import SpiderFactory
from spiders.dynamic_news_spider import DynamicNewsCrawler
from core.downloader import ImageDownloader


def _extract_image_filename(url: str) -> str:
    """
    ä»å›¾ç‰‡URLæå–åŸå§‹æ–‡ä»¶å
    
    å¤„ç†é€»è¾‘ï¼š
    1. ä»URLè·¯å¾„æå–æ–‡ä»¶å
    2. å»æ‰å°ºå¯¸åç¼€ï¼ˆå¦‚ -1024x481ï¼‰
    3. ä¿ç•™åŸå§‹æ‰©å±•å
    
    Args:
        url: å›¾ç‰‡URL
    
    Returns:
        æ¸…ç†åçš„æ–‡ä»¶å
    """
    try:
        parsed = urlparse(url)
        filename = os.path.basename(parsed.path)
        
        # å»æ‰å°ºå¯¸åç¼€ï¼ˆå¦‚ -1024x481, -300x200 ç­‰ï¼‰
        clean_name = re.sub(r'-\d+x\d+', '', filename)
        
        # å»æ‰æŸ¥è¯¢å‚æ•°å¯èƒ½å¸¦æ¥çš„åç¼€
        clean_name = clean_name.split('?')[0]
        
        # å¦‚æœæ–‡ä»¶åä¸ºç©ºæˆ–æ— æ•ˆï¼Œç”Ÿæˆä¸€ä¸ªé»˜è®¤å
        if not clean_name or clean_name == '.' or '.' not in clean_name:
            import hashlib
            hash_name = hashlib.md5(url.encode()).hexdigest()[:12]
            clean_name = f"{hash_name}.jpg"
        
        return clean_name
    except Exception:
        import hashlib
        hash_name = hashlib.md5(url.encode()).hexdigest()[:12]
        return f"{hash_name}.jpg"


async def handle_crawl_url(args):
    """å¤„ç† crawl-url å­å‘½ä»¤"""
    print(f"\nğŸ“Œ å‘½ä»¤: çˆ¬å–å•ä¸ªURL")
    print(f"URL: {args.url}")
    
    # 1. åŠ è½½é…ç½®
    if args.auto_detect:
        logger.info(f"ğŸŒ è‡ªåŠ¨æ£€æµ‹é…ç½®: {args.url}")
        config = ConfigLoader.auto_detect(args.url)
    elif args.preset:
        logger.info(f"ğŸ“‹ ä½¿ç”¨è®ºå›ç±»å‹é¢„è®¾: {args.preset}")
        config = ConfigLoader.load(args.preset)
    elif args.config:
        logger.info(f"ğŸ“ ä½¿ç”¨é…ç½®æ–‡ä»¶: {args.config}")
        config = get_example_config(args.config)
    else:
        logger.error("âŒ è¯·æŒ‡å®šé…ç½®æ¥æº: --auto-detect æˆ– --preset æˆ– --config")
        return
    
    # 2. åˆ›å»ºçˆ¬è™«
    spider = SpiderFactory.create(config=config)
    
    # 3. çˆ¬å–URL
    async with spider:
        thread_id = spider.parser._extract_thread_id(args.url)
        thread_info = {
            'url': args.url,
            'thread_id': thread_id,
            'title': f'Thread-{thread_id}',
            'board': config.bbs.name
        }
        
        logger.info(f"ğŸš€ å¼€å§‹çˆ¬å–URL...")
        await spider.crawl_thread(thread_info)
        logger.info(f"âœ… çˆ¬å–å®Œæˆ")
        
        # è¾“å‡ºç»Ÿè®¡
        print_statistics(spider)


async def handle_crawl_urls(args):
    """å¤„ç† crawl-urls å­å‘½ä»¤"""
    print(f"\nğŸ“Œ å‘½ä»¤: çˆ¬å–é…ç½®ä¸­çš„URLåˆ—è¡¨")
    print(f"é…ç½®: {args.config}")
    
    # 1. åŠ è½½é…ç½®
    logger.info(f"ğŸ“ ä½¿ç”¨é…ç½®æ–‡ä»¶: {args.config}")
    config = get_example_config(args.config)
    
    # 2. è·å–URLåˆ—è¡¨
    urls = get_forum_urls(args.config)
    logger.info(f"ğŸ“ ä»é…ç½®æ–‡ä»¶åŠ è½½URL: {len(urls)} ä¸ª")
    
    if not urls:
        logger.error("âŒ é…ç½®æ–‡ä»¶ä¸­æ²¡æœ‰URLsï¼")
        return
    
    # 3. åˆ›å»ºçˆ¬è™«å¹¶å¹¶å‘çˆ¬å–
    spider = SpiderFactory.create(config=config)
    
    async with spider:
        logger.info(f"ğŸš€ å¼€å§‹å¹¶å‘çˆ¬å– {len(urls)} ä¸ªURL...")
        tasks = []
        for url in urls:
            thread_id = spider.parser._extract_thread_id(url)
            thread_info = {
                'url': url,
                'thread_id': thread_id,
                'title': f'Thread-{thread_id}',
                'board': config.bbs.name
            }
            tasks.append(spider.crawl_thread(thread_info))
        
        # ä½¿ç”¨ asyncio.gather å¹¶å‘æ‰§è¡Œ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for r in results if not isinstance(r, Exception))
        failed_count = len(results) - success_count
        logger.info(f"âœ… å®Œæˆ: æˆåŠŸ {success_count}, å¤±è´¥ {failed_count}")
        
        # è¾“å‡ºç»Ÿè®¡
        print_statistics(spider)


async def handle_crawl_board(args):
    """å¤„ç† crawl-board å­å‘½ä»¤"""
    print(f"\nğŸ“Œ å‘½ä»¤: çˆ¬å–å•ä¸ªæ¿å—")
    print(f"æ¿å—URL: {args.board_url}")
    if args.max_pages:
        print(f"æœ€å¤§é¡µæ•°: {args.max_pages}")
    else:
        print(f"æœ€å¤§é¡µæ•°: ä¸é™åˆ¶ï¼ˆçˆ¬å–æ‰€æœ‰é¡µï¼‰")
    
    # 1. åŠ è½½é…ç½®
    if args.auto_detect:
        logger.info(f"ğŸŒ è‡ªåŠ¨æ£€æµ‹é…ç½®: {args.board_url}")
        config = ConfigLoader.auto_detect(args.board_url)
    elif args.preset:
        logger.info(f"ğŸ“‹ ä½¿ç”¨è®ºå›ç±»å‹é¢„è®¾: {args.preset}")
        config = ConfigLoader.load(args.preset)
    elif args.config:
        logger.info(f"ğŸ“ ä½¿ç”¨é…ç½®æ–‡ä»¶: {args.config}")
        config = get_example_config(args.config)
    else:
        logger.error("âŒ è¯·æŒ‡å®šé…ç½®æ¥æº: --auto-detect æˆ– --preset æˆ– --config")
        return
    
    # 2. åˆ›å»ºçˆ¬è™«
    spider = SpiderFactory.create(config=config)
    
    # 3. çˆ¬å–æ¿å—
    async with spider:
        logger.info(f"ğŸš€ å¼€å§‹çˆ¬å–æ¿å—...")
        await spider.crawl_board(
            board_url=args.board_url,
            board_name=config.bbs.name,
            max_pages=args.max_pages
        )
        logger.info(f"âœ… çˆ¬å–å®Œæˆ")
        
        # è¾“å‡ºç»Ÿè®¡
        print_statistics(spider)


async def handle_crawl_boards(args):
    """å¤„ç† crawl-boards å­å‘½ä»¤"""
    print(f"\nğŸ“Œ å‘½ä»¤: çˆ¬å–é…ç½®ä¸­çš„æ‰€æœ‰æ¿å—")
    print(f"é…ç½®: {args.config}")
    if args.max_pages:
        print(f"æ¯ä¸ªæ¿å—æœ€å¤§é¡µæ•°: {args.max_pages}")
    else:
        print(f"æ¯ä¸ªæ¿å—æœ€å¤§é¡µæ•°: ä¸é™åˆ¶ï¼ˆçˆ¬å–æ‰€æœ‰é¡µï¼‰")
    
    # 1. åŠ è½½é…ç½®
    logger.info(f"ğŸ“ ä½¿ç”¨é…ç½®æ–‡ä»¶: {args.config}")
    config = get_example_config(args.config)
    
    # 2. è·å–æ¿å—åˆ—è¡¨
    boards_info = get_forum_boards(args.config)
    logger.info(f"ğŸ“ ä»é…ç½®æ–‡ä»¶åŠ è½½æ¿å—: {len(boards_info)} ä¸ª")
    
    if not boards_info:
        logger.error("âŒ é…ç½®æ–‡ä»¶ä¸­æ²¡æœ‰æ¿å—ï¼")
        return
    
    # 3. åˆ›å»ºçˆ¬è™«å¹¶å¹¶å‘çˆ¬å–
    spider = SpiderFactory.create(config=config)
    
    async with spider:
        if args.max_pages:
            logger.info(f"ğŸš€ å¼€å§‹å¹¶å‘çˆ¬å– {len(boards_info)} ä¸ªæ¿å—ï¼ˆæ¯ä¸ªæœ€å¤š {args.max_pages} é¡µï¼‰...")
        else:
            logger.info(f"ğŸš€ å¼€å§‹å¹¶å‘çˆ¬å– {len(boards_info)} ä¸ªæ¿å—ï¼ˆçˆ¬å–æ‰€æœ‰é¡µé¢ï¼‰...")
        
        tasks = []
        for board in boards_info:
            logger.info(f"ğŸ“ æ¿å—: {board['name']} - {board['url']}")
            tasks.append(spider.crawl_board(
                board_url=board['url'],
                board_name=board['name'],
                max_pages=args.max_pages
            ))
        
        # ä½¿ç”¨ asyncio.gather å¹¶å‘æ‰§è¡Œ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for r in results if not isinstance(r, Exception))
        failed_count = len(results) - success_count
        logger.info(f"âœ… å®Œæˆ: æˆåŠŸ {success_count}, å¤±è´¥ {failed_count}")
        
        # è¾“å‡ºç»Ÿè®¡
        print_statistics(spider)


async def handle_crawl_news(args):
    """å¤„ç† crawl-news å­å‘½ä»¤"""
    print(f"\nğŸ“Œ å‘½ä»¤: çˆ¬å–åŠ¨æ€æ–°é—»é¡µé¢")
    print(f"URL: {args.url}")
    print(f"æ–¹å¼: {args.method}")
    if args.max_pages:
        print(f"æœ€å¤§é¡µæ•°: {args.max_pages}")
    else:
        print(f"æœ€å¤§é¡µæ•°: ä¸é™åˆ¶ï¼ˆçˆ¬å–æ‰€æœ‰é¡µï¼‰")
    
    # 1. åŠ è½½é…ç½®
    if args.config:
        logger.info(f"ğŸ“ ä½¿ç”¨é…ç½®æ–‡ä»¶: {args.config}")
        config = get_example_config(args.config)
    else:
        # åˆ›å»ºé»˜è®¤é…ç½®
        logger.info(f"ğŸŒ ä½¿ç”¨é»˜è®¤é…ç½®")
        parsed = urlparse(args.url)
        base_url = f"{parsed.scheme}://{parsed.netloc}"
        
        config = Config(
            bbs={
                "name": "åŠ¨æ€æ–°é—»ç½‘ç«™",
                "base_url": base_url,
                "forum_type": "custom"
            }
        )
    
    # 2. åˆ›å»ºçˆ¬è™«
    crawler = DynamicNewsCrawler(config)
    
    # 3. çˆ¬å–é¡µé¢
    async with crawler:
        logger.info(f"ğŸš€ å¼€å§‹çˆ¬å–åŠ¨æ€æ–°é—»é¡µé¢...")
        
        # é€‰æ‹©çˆ¬å–æ–¹å¼
        if args.method == 'ajax':
            articles = await crawler.crawl_dynamic_page_ajax(
                args.url,
                max_pages=args.max_pages
            )
        else:  # selenium
            articles = await crawler.crawl_dynamic_page_selenium(
                args.url,
                max_clicks=args.max_pages
            )
        
        if not articles:
            logger.warning("âš ï¸  æ²¡æœ‰æ‰¾åˆ°æ–‡ç« ")
            return
        
        logger.info(f"âœ… å‘ç° {len(articles)} ç¯‡æ–‡ç« ")
        
        # 4. æ˜¯å¦ä¸‹è½½æ–‡ç« è¯¦æƒ…å’Œå›¾ç‰‡
        if args.download_images:
            logger.info(f"ğŸš€ å¼€å§‹ä¸‹è½½æ–‡ç« è¯¦æƒ…å’Œå›¾ç‰‡...")
            
            # çˆ¬å–æ–‡ç« è¯¦æƒ…
            full_articles = await crawler.crawl_articles_batch(articles)
            
            # ä¸‹è½½å›¾ç‰‡
            total_images = 0
            downloaded_images = 0
            
            # ä»URLæå–åŸŸåä½œä¸ºå­˜å‚¨ç›®å½•
            domain = urlparse(args.url).netloc  # å¦‚ sxd.xd.com
            save_dir = config.image.download_dir / domain
            save_dir.mkdir(parents=True, exist_ok=True)
            
            async with ImageDownloader() as downloader:
                for article in full_articles:
                    images = article.get('images', [])
                    if not images:
                        continue
                    
                    total_images += len(images)
                    article_id = article.get('article_id', 'unknown')
                    
                    # é€ä¸ªä¸‹è½½å›¾ç‰‡ï¼Œä½¿ç”¨è‡ªå®šä¹‰æ–‡ä»¶åæ ¼å¼
                    for img_url in images:
                        # ä»å›¾ç‰‡URLæå–åŸå§‹æ–‡ä»¶å
                        img_filename = _extract_image_filename(img_url)
                        # ç”Ÿæˆæœ€ç»ˆæ–‡ä»¶å: [article_id]_[åŸå§‹å›¾ç‰‡å]
                        final_filename = f"{article_id}_{img_filename}"
                        save_path = save_dir / final_filename
                        
                        # ä¸‹è½½å›¾ç‰‡
                        metadata = {
                            'article_id': article_id,
                            'title': article.get('title', ''),
                            'article_url': article.get('url', ''),
                            'image_url': img_url
                        }
                        
                        result = await downloader.download_image(img_url, save_path, metadata)
                        if result.get('success'):
                            downloaded_images += 1
                        
                        # æ·»åŠ å»¶è¿Ÿ
                        await asyncio.sleep(config.crawler.download_delay)
            
            logger.success(f"âœ… å›¾ç‰‡ä¸‹è½½å®Œæˆ: {downloaded_images}/{total_images}")
        
        # 5. è¾“å‡ºç»Ÿè®¡
        stats = crawler.get_statistics()
        print("\n" + "=" * 60)
        print("ğŸ“Š çˆ¬å–ç»Ÿè®¡:")
        print(f"  å‘ç°æ–‡ç« : {stats['articles_found']}")
        if args.download_images:
            print(f"  çˆ¬å–è¯¦æƒ…: {stats['articles_crawled']}")
            print(f"  çˆ¬å–å¤±è´¥: {stats['articles_failed']}")
            print(f"  ä¸‹è½½å›¾ç‰‡: {downloaded_images if 'downloaded_images' in locals() else 0}")
        print("=" * 60)


def print_statistics(spider):
    """è¾“å‡ºç»Ÿè®¡ä¿¡æ¯"""
    stats = spider.get_statistics()
    print("\n" + "=" * 60)
    print("ğŸ“Š çˆ¬å–ç»Ÿè®¡:")
    print(f"  å¸–å­æ•°: {stats['threads_crawled']}")
    print(f"  å‘ç°å›¾ç‰‡: {stats['images_found']}")
    print(f"  ä¸‹è½½æˆåŠŸ: {stats['images_downloaded']}")
    print(f"  ä¸‹è½½å¤±è´¥: {stats['images_failed']}")
    print(f"  å»é‡è·³è¿‡: {stats['duplicates_skipped']}")
    print("=" * 60)
