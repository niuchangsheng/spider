"""
CLIå‘½ä»¤å¤„ç†å‡½æ•°
"""
import asyncio
from typing import Dict, Any
from urllib.parse import urlparse
from loguru import logger

from config import Config, ConfigLoader, get_example_config, get_forum_boards, get_forum_urls, get_news_urls
from spiders import SpiderFactory
from spiders.dynamic_news_spider import DynamicNewsCrawler
from core.checkpoint import CheckpointManager


async def handle_crawl_bbs(args):
    """BBSï¼šçˆ¬å–å•ä¸ªå¸–å­(--url)æˆ–å•ä¸ªæ¿å—(--board)ï¼›--url/--board æ›¿æ¢ config å†… urls"""
    if not getattr(args, 'url', None) and not getattr(args, 'board', None):
        logger.error("âŒ crawl-bbs å¿…é¡»æŒ‡å®š --url æˆ– --boardï¼›çˆ¬å– config å†…å…¨éƒ¨è¯·ç”¨ crawl --config xindong")
        return
    if getattr(args, 'url', None) and getattr(args, 'board', None):
        logger.error("âŒ crawl-bbs åªèƒ½æŒ‡å®š --url æˆ– --board å…¶ä¸€")
        return

    if not args.config:
        logger.error("âŒ crawl-bbs è¯·æŒ‡å®š --config ä»¥æä¾› BBS é…ç½®")
        return

    config = get_example_config(args.config)
    if config.crawler_type != "bbs":
        logger.error(f"âŒ é…ç½® {args.config} çš„ crawler_type ä¸æ˜¯ bbs")
        return

    if getattr(args, 'max_workers', None):
        config.crawler.max_concurrent_requests = args.max_workers
    if getattr(args, 'use_adaptive_queue', None) is not None:
        config.crawler.use_adaptive_queue = args.use_adaptive_queue
    if getattr(args, 'use_async_queue', None) is not None:
        config.crawler.use_async_queue = args.use_async_queue

    spider = SpiderFactory.create(config=config)
    async with spider:
        if args.url:
            print(f"\nğŸ“Œ å‘½ä»¤: crawl-bbs å•å¸–")
            print(f"URL: {args.url}")
            thread_id = spider.parser._extract_thread_id(args.url)
            thread_info = {
                'url': args.url,
                'thread_id': thread_id,
                'title': f'Thread-{thread_id}',
                'board': config.bbs.name,
            }
            await spider.crawl_thread(thread_info)
        else:
            print(f"\nğŸ“Œ å‘½ä»¤: crawl-bbs å•æ¿å—")
            print(f"æ¿å—URL: {args.board}")
            await spider.crawl_board(
                board_url=args.board,
                board_name=config.bbs.name,
                max_pages=getattr(args, 'max_pages', None),
                resume=getattr(args, 'resume', True),
                start_page=getattr(args, 'start_page', None),
            )
        print_statistics(spider)


async def handle_crawl_news(args):
    """çˆ¬å–åŠ¨æ€æ–°é—»ï¼ˆå¿…é¡»æŒ‡å®š URLï¼›çˆ¬å…¨é‡è¯·ç”¨ crawl --config sxdï¼‰"""
    # URL æ¥è‡ª positional æˆ– --url
    news_url = getattr(args, 'url', None) or getattr(args, 'news_url', None)
    if not news_url:
        logger.error("âŒ crawl-news å¿…é¡»æŒ‡å®š URLï¼ˆä½ç½®å‚æ•°æˆ– --urlï¼‰ï¼›çˆ¬å–é…ç½®å†…å…¨éƒ¨è¯·ç”¨ crawl --config sxd")
        return

    news_urls = [news_url]
    print(f"\nğŸ“Œ å‘½ä»¤: çˆ¬å–åŠ¨æ€æ–°é—»é¡µé¢")
    print(f"URL: {news_url}")
    print(f"æ–¹å¼: {args.method}")
    if args.max_pages:
        print(f"æœ€å¤§é¡µæ•°: {args.max_pages}")
    else:
        print(f"æœ€å¤§é¡µæ•°: ä¸é™åˆ¶ï¼ˆçˆ¬å–æ‰€æœ‰é¡µï¼‰")
    if hasattr(args, 'max_workers') and args.max_workers:
        print(f"å¹¶å‘æ•°: {args.max_workers} (å‘½ä»¤è¡ŒæŒ‡å®š)")
    if hasattr(args, 'use_adaptive_queue') and args.use_adaptive_queue:
        print(f"é˜Ÿåˆ—æ¨¡å¼: è‡ªé€‚åº”é˜Ÿåˆ—")
    elif hasattr(args, 'use_async_queue') and args.use_async_queue is False:
        print(f"é˜Ÿåˆ—æ¨¡å¼: ä¸²è¡Œæ¨¡å¼ï¼ˆç¦ç”¨å¼‚æ­¥é˜Ÿåˆ—ï¼‰")
    else:
        print(f"é˜Ÿåˆ—æ¨¡å¼: å¼‚æ­¥é˜Ÿåˆ—ï¼ˆé»˜è®¤ï¼‰")

    if args.config:
        logger.info(f"ğŸ“ ä½¿ç”¨é…ç½®æ–‡ä»¶: {args.config}")
        config = get_example_config(args.config)
    else:
        parsed = urlparse(news_url)
        base_url = f"{parsed.scheme}://{parsed.netloc}"
        config = Config(
            bbs={"name": "åŠ¨æ€æ–°é—»ç½‘ç«™", "base_url": base_url, "forum_type": "custom"}
        )
    
    # åº”ç”¨é˜Ÿåˆ—ç›¸å…³é…ç½®
    if hasattr(args, 'max_workers') and args.max_workers:
        config.crawler.max_concurrent_requests = args.max_workers
    if hasattr(args, 'use_adaptive_queue') and args.use_adaptive_queue is not None:
        config.crawler.use_adaptive_queue = args.use_adaptive_queue
    if hasattr(args, 'use_async_queue') and args.use_async_queue is not None:
        config.crawler.use_async_queue = args.use_async_queue
    
    crawler = DynamicNewsCrawler(config)
    total_articles = 0
    total_downloaded_images = 0
    async with crawler:
        for url in news_urls:
            articles_count, images_count = await crawler.crawl_news_and_download_images(
                url,
                max_pages=args.max_pages,
                resume=args.resume,
                start_page=args.start_page,
                download_images=args.download_images,
                method=args.method,
            )
            total_articles += articles_count
            total_downloaded_images += images_count
    stats = crawler.get_statistics()
    print("\n" + "=" * 60)
    print("ğŸ“Š çˆ¬å–ç»Ÿè®¡:")
    print(f"  çˆ¬å–URLæ•°: {len(news_urls)}")
    print(f"  å‘ç°æ–‡ç« : {total_articles}")
    if args.download_images:
        print(f"  çˆ¬å–è¯¦æƒ…: {stats['articles_crawled']}")
        print(f"  çˆ¬å–å¤±è´¥: {stats['articles_failed']}")
        print(f"  ä¸‹è½½å›¾ç‰‡: {total_downloaded_images}")
    print("=" * 60)


def print_statistics(spider):
    """è¾“å‡ºç»Ÿè®¡ä¿¡æ¯"""
    stats = spider.get_statistics()
    print("\n" + "=" * 60)
    print("ğŸ“Š çˆ¬å–ç»Ÿè®¡:")
    print(f"  å¸–å­æ•°: {stats['threads_crawled']}")
    print(f"  å‘ç°å›¾ç‰‡: {stats['images_found']}")
    print(f"  ä¸‹è½½æˆåŠŸ: {stats['images_downloaded']}")
    print(f"  ä¸‹è½½å¤±è´¥: {stats['images_failed']}")
    print(f"  å»é‡è·³è¿‡: {stats['duplicates_skipped']}")
    print("=" * 60)


async def handle_crawl(args):
    """ç»Ÿä¸€çˆ¬å–ï¼š--config å…¨é‡ï¼›--url/--board å•ç›®æ ‡ï¼ˆå¯ --auto-detectï¼‰ï¼›ç”± config å†³å®š BBS/æ–°é—»"""
    single_url = getattr(args, 'url', None)
    single_board = getattr(args, 'board', None)
    auto_detect = getattr(args, 'auto_detect', False)
    config_name = getattr(args, 'config', None)

    # å•ç›®æ ‡æ¨¡å¼ï¼š--url æˆ– --board
    if single_url or single_board:
        if single_url and single_board:
            logger.error("âŒ åªèƒ½æŒ‡å®š --url æˆ– --board å…¶ä¸€")
            return
        if single_board:
            if not config_name:
                logger.error("âŒ crawl --board éœ€é…åˆ --config")
                return
            config = get_example_config(config_name)
            if config.crawler_type != "bbs":
                logger.error(f"âŒ é…ç½® {config_name} çš„ crawler_type ä¸æ˜¯ bbs")
                return
            if getattr(args, 'max_workers', None):
                config.crawler.max_concurrent_requests = args.max_workers
            if getattr(args, 'use_adaptive_queue', None) is not None:
                config.crawler.use_adaptive_queue = args.use_adaptive_queue
            if getattr(args, 'use_async_queue', None) is not None:
                config.crawler.use_async_queue = args.use_async_queue
            spider = SpiderFactory.create(config=config)
            async with spider:
                await spider.crawl_board(
                    board_url=single_board,
                    board_name=config.bbs.name,
                    max_pages=getattr(args, 'max_pages', None),
                    resume=getattr(args, 'resume', True),
                    start_page=getattr(args, 'start_page', None),
                )
            print_statistics(spider)
            return
        # single_url
        if auto_detect:
            config = ConfigLoader.auto_detect(single_url)
            spider = SpiderFactory.create(config=config)
            async with spider:
                thread_id = spider.parser._extract_thread_id(single_url)
                await spider.crawl_thread({
                    'url': single_url,
                    'thread_id': thread_id,
                    'title': f'Thread-{thread_id}',
                    'board': config.bbs.name,
                })
            print_statistics(spider)
            return
        if not config_name:
            logger.error("âŒ crawl --url è¯·æŒ‡å®š --config æˆ– --auto-detect")
            return
        config = get_example_config(config_name)
        if getattr(args, 'max_workers', None):
            config.crawler.max_concurrent_requests = args.max_workers
        if getattr(args, 'use_adaptive_queue', None) is not None:
            config.crawler.use_adaptive_queue = args.use_adaptive_queue
        if getattr(args, 'use_async_queue', None) is not None:
            config.crawler.use_async_queue = args.use_async_queue
        if config.crawler_type == "news":
            crawler = DynamicNewsCrawler(config)
            async with crawler:
                a, i = await crawler.crawl_news_and_download_images(
                    single_url,
                    max_pages=getattr(args, 'max_pages', None),
                    resume=getattr(args, 'resume', True),
                    start_page=getattr(args, 'start_page', None),
                    download_images=getattr(args, 'download_images', False),
                    method=getattr(args, 'method', 'ajax'),
                )
            stats = crawler.get_statistics()
            print("\n" + "=" * 60)
            print("ğŸ“Š çˆ¬å–ç»Ÿè®¡: å‘ç°æ–‡ç« ", a, "ä¸‹è½½å›¾ç‰‡", i)
            print("=" * 60)
        else:
            spider = SpiderFactory.create(config=config)
            async with spider:
                thread_id = spider.parser._extract_thread_id(single_url)
                await spider.crawl_thread({
                    'url': single_url,
                    'thread_id': thread_id,
                    'title': f'Thread-{thread_id}',
                    'board': config.bbs.name,
                })
            print_statistics(spider)
        return

    # å…¨é‡æ¨¡å¼ï¼šå¿…é¡» --config
    if not config_name:
        logger.error("âŒ crawl è¯·æŒ‡å®š --configï¼ˆçˆ¬å…¨é‡ï¼‰æˆ– --url/--boardï¼ˆçˆ¬å•ç›®æ ‡ï¼‰")
        return
    config = get_example_config(config_name)
    if getattr(args, 'max_workers', None):
        config.crawler.max_concurrent_requests = args.max_workers
    if getattr(args, 'use_adaptive_queue', None) is not None:
        config.crawler.use_adaptive_queue = args.use_adaptive_queue
    if getattr(args, 'use_async_queue', None) is not None:
        config.crawler.use_async_queue = args.use_async_queue

    if config.crawler_type == "news":
        print(f"\nğŸ“Œ å‘½ä»¤: çˆ¬å–ï¼ˆç”± config å†³å®šï¼‰â€” ç±»å‹: æ–°é—» (crawler_type=news)")
        news_urls = config.get_page_urls() or get_news_urls(config_name)
        if not news_urls:
            logger.error(f"âŒ é…ç½® {config_name} ä¸­æœªæ‰¾åˆ° urls")
            return
        logger.info(f"ğŸ“ é…ç½®: {config_name}ï¼Œæ–°é—» URL æ•°: {len(news_urls)}")
        crawler = DynamicNewsCrawler(config)
        total_articles, total_images = 0, 0
        async with crawler:
            for url in news_urls:
                a, i = await crawler.crawl_news_and_download_images(
                    url,
                    max_pages=getattr(args, 'max_pages', None),
                    resume=getattr(args, 'resume', True),
                    start_page=getattr(args, 'start_page', None),
                    download_images=getattr(args, 'download_images', False),
                    method=getattr(args, 'method', 'ajax'),
                )
                total_articles += a
                total_images += i
        stats = crawler.get_statistics()
        print("\n" + "=" * 60)
        print("ğŸ“Š çˆ¬å–ç»Ÿè®¡:")
        print(f"  æ–°é—» URL æ•°: {len(news_urls)}")
        print(f"  å‘ç°æ–‡ç« : {total_articles}")
        print(f"  ä¸‹è½½å›¾ç‰‡: {total_images}")
        if getattr(args, 'download_images', False):
            print(f"  çˆ¬å–è¯¦æƒ…: {stats.get('articles_crawled', 0)}")
            print(f"  çˆ¬å–å¤±è´¥: {stats.get('articles_failed', 0)}")
        print("=" * 60)
        return

    # BBSï¼šçˆ¬å–é…ç½®ä¸­çš„æ¿å— + å¸–å­ URL
    print(f"\nğŸ“Œ å‘½ä»¤: çˆ¬å–ï¼ˆç”± config å†³å®šï¼‰â€” ç±»å‹: BBS (crawler_type=bbs)")
    boards_info = config.get_boards() or get_forum_boards(config_name)
    urls = config.get_page_urls() or get_forum_urls(config_name)
    if not boards_info and not urls:
        logger.error("âŒ é…ç½®ä¸­æ—¢æ—  boards ä¹Ÿæ—  urls")
        return
    logger.info(f"ğŸ“ é…ç½®: {config_name}ï¼Œæ¿å—: {len(boards_info)}ï¼Œå¸–å­ URL: {len(urls)}")
    spider = SpiderFactory.create(config=config)
    async with spider:
        tasks = []
        for board in boards_info:
            tasks.append(spider.crawl_board(
                board_url=board["url"],
                board_name=board["name"],
                max_pages=getattr(args, 'max_pages', None),
                resume=getattr(args, 'resume', True),
                start_page=getattr(args, 'start_page', None),
            ))
        for url in urls:
            thread_id = spider.parser._extract_thread_id(url)
            tasks.append(spider.crawl_thread({
                "url": url,
                "thread_id": thread_id,
                "title": f"Thread-{thread_id}",
                "board": config.bbs.name,
            }))
        if tasks:
            results = await asyncio.gather(*tasks, return_exceptions=True)
            ok = sum(1 for r in results if not isinstance(r, Exception))
            logger.info(f"âœ… å®Œæˆ: æˆåŠŸ {ok}, å¤±è´¥ {len(results) - ok}")
        print_statistics(spider)


async def handle_checkpoint_status(args):
    """å¤„ç† checkpoint-status å­å‘½ä»¤ï¼ˆæ£€æŸ¥ç‚¹åŸºäº Storageï¼Œéœ€å…ˆè¿æ¥ï¼‰"""
    from core.storage import storage

    print(f"\nğŸ“Œ å‘½ä»¤: æŸ¥çœ‹æ£€æŸ¥ç‚¹çŠ¶æ€")
    print(f"ç½‘ç«™: {args.site}")
    print(f"æ¿å—: {args.board}")
    storage.connect()
    try:
        checkpoint = CheckpointManager(site=args.site, board=args.board)
        if args.clear:
            if checkpoint.exists():
                checkpoint.clear_checkpoint()
                print("âœ… æ£€æŸ¥ç‚¹å·²æ¸…é™¤")
            else:
                print("â„¹ï¸  æ²¡æœ‰æ‰¾åˆ°æ£€æŸ¥ç‚¹")
            return

        if not checkpoint.exists():
            print("â„¹ï¸  æ²¡æœ‰æ‰¾åˆ°æ£€æŸ¥ç‚¹")
            print(f"   å­˜å‚¨: {checkpoint.checkpoint_file}")
            return

        data = checkpoint.load_checkpoint()
        if not data:
            print("âŒ æ— æ³•åŠ è½½æ£€æŸ¥ç‚¹æ•°æ®")
            return

        print("\n" + "=" * 60)
        print("ğŸ“‚ æ£€æŸ¥ç‚¹ä¿¡æ¯:")
        print(f"  å­˜å‚¨: {checkpoint.checkpoint_file}")
        print(f"  çŠ¶æ€: {data.get('status', 'unknown')}")
        print(f"  å½“å‰é¡µ: {data.get('current_page', 0)}")
        print(f"  æœ€åå¸–å­ID: {data.get('last_thread_id', 'N/A')}")
        print(f"  åˆ›å»ºæ—¶é—´: {data.get('created_at', 'N/A')}")
        print(f"  æ›´æ–°æ—¶é—´: {data.get('last_update_time', 'N/A')}")

        seen_article_ids = data.get('seen_article_ids', [])
        if seen_article_ids:
            print(f"\nğŸ“‹ æ–‡ç« IDä¿¡æ¯:")
            print(f"  å·²çˆ¬å–æ–‡ç« æ•°: {len(seen_article_ids)}")
            if data.get('min_article_id'):
                print(f"  æœ€å°æ–‡ç« ID: {data.get('min_article_id')}")
            if data.get('max_article_id'):
                print(f"  æœ€å¤§æ–‡ç« ID: {data.get('max_article_id')}")

        stats = data.get('stats', {})
        if stats:
            print("\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
            print(f"  å·²çˆ¬å–å¸–å­: {stats.get('crawled_count', stats.get('articles_found', 0))}")
            print(f"  ä¸‹è½½å›¾ç‰‡: {stats.get('images_downloaded', 0)}")
            print(f"  å¤±è´¥æ•°: {stats.get('failed_count', 0)}")
            if 'last_error' in stats:
                print(f"  æœ€åé”™è¯¯: {stats['last_error']}")

        print("=" * 60)
    finally:
        storage.close()
