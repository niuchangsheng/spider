"""
CLIå‘½ä»¤å¤„ç†å‡½æ•°
"""
import asyncio
from typing import Dict, Any
from urllib.parse import urlparse
from loguru import logger

from config import Config, ConfigLoader, get_example_config, get_forum_boards, get_forum_urls, get_news_urls
from spiders import SpiderFactory
from spiders.dynamic_news_spider import DynamicNewsCrawler
from core.checkpoint import CheckpointManager


async def handle_crawl_bbs(args):
    """BBSï¼šçˆ¬å–å•ä¸ªå¸–å­æˆ–æ¿å—ï¼Œä½ç½®å‚æ•°ä¸º targetï¼Œ--type thread|board åŒºåˆ†ï¼›æ”¯æŒ --auto-detect"""
    if getattr(args, 'auto_detect', False):
        if args.config:
            logger.error("âŒ crawl-bbs è¯·åªæŒ‡å®š --config æˆ– --auto-detect å…¶ä¸€")
            return
        logger.info(f"ğŸŒ è‡ªåŠ¨æ£€æµ‹é…ç½®: {args.target}")
        config = ConfigLoader.auto_detect(args.target)
    else:
        if not args.config:
            logger.error("âŒ crawl-bbs è¯·æŒ‡å®š --config æˆ– --auto-detect")
            return
        config = get_example_config(args.config)
    if config.crawler_type != "bbs":
        logger.error(f"âŒ é…ç½®çš„ crawler_type ä¸æ˜¯ bbs")
        return

    if getattr(args, 'max_workers', None):
        config.crawler.max_concurrent_requests = args.max_workers
    if getattr(args, 'use_adaptive_queue', None) is not None:
        config.crawler.use_adaptive_queue = args.use_adaptive_queue
    if getattr(args, 'use_async_queue', None) is not None:
        config.crawler.use_async_queue = args.use_async_queue

    spider = SpiderFactory.create(config=config)
    async with spider:
        if args.type == "thread":
            print(f"\nğŸ“Œ å‘½ä»¤: crawl-bbs å•å¸–")
            print(f"URL: {args.target}")
            thread_id = spider.parser._extract_thread_id(args.target)
            await spider.crawl_thread({
                'url': args.target,
                'thread_id': thread_id,
                'title': f'Thread-{thread_id}',
                'board': config.bbs.name,
            })
        else:
            print(f"\nğŸ“Œ å‘½ä»¤: crawl-bbs å•æ¿å—")
            print(f"æ¿å—URL: {args.target}")
            await spider.crawl_board(
                board_url=args.target,
                board_name=config.bbs.name,
                max_pages=getattr(args, 'max_pages', None),
                resume=getattr(args, 'resume', True),
                start_page=getattr(args, 'start_page', None),
            )
        print_statistics(spider)


async def handle_crawl_news(args):
    """çˆ¬å–åŠ¨æ€æ–°é—»å•é¡µï¼ˆå¿…é¡»ä¼  URLï¼‰ï¼›çˆ¬å…¨é‡ç”¨ crawl --config sxd"""
    news_urls = [args.url]
    print(f"\nğŸ“Œ å‘½ä»¤: çˆ¬å–åŠ¨æ€æ–°é—»é¡µé¢")
    print(f"URL: {args.url}")
    print(f"æ–¹å¼: {args.method}")
    if args.max_pages:
        print(f"æœ€å¤§é¡µæ•°: {args.max_pages}")
    else:
        print(f"æœ€å¤§é¡µæ•°: ä¸é™åˆ¶ï¼ˆçˆ¬å–æ‰€æœ‰é¡µï¼‰")
    if hasattr(args, 'max_workers') and args.max_workers:
        print(f"å¹¶å‘æ•°: {args.max_workers} (å‘½ä»¤è¡ŒæŒ‡å®š)")
    if hasattr(args, 'use_adaptive_queue') and args.use_adaptive_queue:
        print(f"é˜Ÿåˆ—æ¨¡å¼: è‡ªé€‚åº”é˜Ÿåˆ—")
    elif hasattr(args, 'use_async_queue') and args.use_async_queue is False:
        print(f"é˜Ÿåˆ—æ¨¡å¼: ä¸²è¡Œæ¨¡å¼ï¼ˆç¦ç”¨å¼‚æ­¥é˜Ÿåˆ—ï¼‰")
    else:
        print(f"é˜Ÿåˆ—æ¨¡å¼: å¼‚æ­¥é˜Ÿåˆ—ï¼ˆé»˜è®¤ï¼‰")

    if args.config:
        logger.info(f"ğŸ“ ä½¿ç”¨é…ç½®æ–‡ä»¶: {args.config}")
        config = get_example_config(args.config)
    else:
        parsed = urlparse(args.url)
        base_url = f"{parsed.scheme}://{parsed.netloc}"
        config = Config(
            bbs={"name": "åŠ¨æ€æ–°é—»ç½‘ç«™", "base_url": base_url, "forum_type": "custom"}
        )

    # åº”ç”¨é˜Ÿåˆ—ç›¸å…³é…ç½®
    if hasattr(args, 'max_workers') and args.max_workers:
        config.crawler.max_concurrent_requests = args.max_workers
    if hasattr(args, 'use_adaptive_queue') and args.use_adaptive_queue is not None:
        config.crawler.use_adaptive_queue = args.use_adaptive_queue
    if hasattr(args, 'use_async_queue') and args.use_async_queue is not None:
        config.crawler.use_async_queue = args.use_async_queue
    
    crawler = DynamicNewsCrawler(config)
    total_articles = 0
    total_downloaded_images = 0
    async with crawler:
        for url in news_urls:
            articles_count, images_count = await crawler.crawl_news_and_download_images(
                url,
                max_pages=args.max_pages,
                resume=args.resume,
                start_page=args.start_page,
                download_images=args.download_images,
                method=args.method,
            )
            total_articles += articles_count
            total_downloaded_images += images_count
    stats = crawler.get_statistics()
    print("\n" + "=" * 60)
    print("ğŸ“Š çˆ¬å–ç»Ÿè®¡:")
    print(f"  çˆ¬å–URLæ•°: {len(news_urls)}")
    print(f"  å‘ç°æ–‡ç« : {total_articles}")
    if args.download_images:
        print(f"  çˆ¬å–è¯¦æƒ…: {stats['articles_crawled']}")
        print(f"  çˆ¬å–å¤±è´¥: {stats['articles_failed']}")
        print(f"  ä¸‹è½½å›¾ç‰‡: {total_downloaded_images}")
    print("=" * 60)


def print_statistics(spider):
    """è¾“å‡ºç»Ÿè®¡ä¿¡æ¯"""
    stats = spider.get_statistics()
    print("\n" + "=" * 60)
    print("ğŸ“Š çˆ¬å–ç»Ÿè®¡:")
    print(f"  å¸–å­æ•°: {stats['threads_crawled']}")
    print(f"  å‘ç°å›¾ç‰‡: {stats['images_found']}")
    print(f"  ä¸‹è½½æˆåŠŸ: {stats['images_downloaded']}")
    print(f"  ä¸‹è½½å¤±è´¥: {stats['images_failed']}")
    print(f"  å»é‡è·³è¿‡: {stats['duplicates_skipped']}")
    print("=" * 60)


async def handle_crawl(args):
    """ç»Ÿä¸€çˆ¬å–ï¼šä»… --config å…¨é‡ï¼Œç”± config å†³å®š BBS/æ–°é—»"""
    config_name = args.config
    config = get_example_config(config_name)
    if getattr(args, 'max_workers', None):
        config.crawler.max_concurrent_requests = args.max_workers
    if getattr(args, 'use_adaptive_queue', None) is not None:
        config.crawler.use_adaptive_queue = args.use_adaptive_queue
    if getattr(args, 'use_async_queue', None) is not None:
        config.crawler.use_async_queue = args.use_async_queue

    if config.crawler_type == "news":
        print(f"\nğŸ“Œ å‘½ä»¤: çˆ¬å–ï¼ˆç”± config å†³å®šï¼‰â€” ç±»å‹: æ–°é—» (crawler_type=news)")
        news_urls = config.get_page_urls() or get_news_urls(config_name)
        if not news_urls:
            logger.error(f"âŒ é…ç½® {config_name} ä¸­æœªæ‰¾åˆ° urls")
            return
        logger.info(f"ğŸ“ é…ç½®: {config_name}ï¼Œæ–°é—» URL æ•°: {len(news_urls)}")
        crawler = DynamicNewsCrawler(config)
        total_articles, total_images = 0, 0
        async with crawler:
            for url in news_urls:
                a, i = await crawler.crawl_news_and_download_images(
                    url,
                    max_pages=getattr(args, 'max_pages', None),
                    resume=getattr(args, 'resume', True),
                    start_page=getattr(args, 'start_page', None),
                    download_images=getattr(args, 'download_images', False),
                    method=getattr(args, 'method', 'ajax'),
                )
                total_articles += a
                total_images += i
        stats = crawler.get_statistics()
        print("\n" + "=" * 60)
        print("ğŸ“Š çˆ¬å–ç»Ÿè®¡:")
        print(f"  æ–°é—» URL æ•°: {len(news_urls)}")
        print(f"  å‘ç°æ–‡ç« : {total_articles}")
        print(f"  ä¸‹è½½å›¾ç‰‡: {total_images}")
        if getattr(args, 'download_images', False):
            print(f"  çˆ¬å–è¯¦æƒ…: {stats.get('articles_crawled', 0)}")
            print(f"  çˆ¬å–å¤±è´¥: {stats.get('articles_failed', 0)}")
        print("=" * 60)
        return

    # BBSï¼šçˆ¬å–é…ç½®ä¸­çš„æ¿å— + å¸–å­ URL
    print(f"\nğŸ“Œ å‘½ä»¤: çˆ¬å–ï¼ˆç”± config å†³å®šï¼‰â€” ç±»å‹: BBS (crawler_type=bbs)")
    boards_info = config.get_boards() or get_forum_boards(config_name)
    page_entries = config.get_page_entries() or [{"url": u, "name": None} for u in get_forum_urls(config_name)]
    if not boards_info and not page_entries:
        logger.error("âŒ é…ç½®ä¸­æ—¢æ—  boards ä¹Ÿæ—  urls")
        return
    logger.info(f"ğŸ“ é…ç½®: {config_name}ï¼Œæ¿å—: {len(boards_info)}ï¼Œå¸–å­ URL: {len(page_entries)}")
    spider = SpiderFactory.create(config=config)
    async with spider:
        tasks = []
        for board in boards_info:
            tasks.append(spider.crawl_board(
                board_url=board["url"],
                board_name=board["name"],
                max_pages=getattr(args, 'max_pages', None),
                resume=getattr(args, 'resume', True),
                start_page=getattr(args, 'start_page', None),
            ))
        for entry in page_entries:
            url = entry["url"]
            thread_id = spider.parser._extract_thread_id(url)
            title = entry.get("name") or f"Thread-{thread_id}"
            tasks.append(spider.crawl_thread({
                "url": url,
                "thread_id": thread_id,
                "title": title,
                "board": config.bbs.name,
            }))
        if tasks:
            results = await asyncio.gather(*tasks, return_exceptions=True)
            ok = sum(1 for r in results if not isinstance(r, Exception))
            logger.info(f"âœ… å®Œæˆ: æˆåŠŸ {ok}, å¤±è´¥ {len(results) - ok}")
        print_statistics(spider)


async def handle_checkpoint_status(args):
    """å¤„ç† checkpoint-status å­å‘½ä»¤ï¼ˆæ£€æŸ¥ç‚¹åŸºäº Storageï¼Œéœ€å…ˆè¿æ¥ï¼‰"""
    from core.storage import storage

    print(f"\nğŸ“Œ å‘½ä»¤: æŸ¥çœ‹æ£€æŸ¥ç‚¹çŠ¶æ€")
    print(f"ç½‘ç«™: {args.site}")
    print(f"æ¿å—: {args.board}")
    storage.connect()
    try:
        checkpoint = CheckpointManager(site=args.site, board=args.board)
        if args.clear:
            if checkpoint.exists():
                checkpoint.clear_checkpoint()
                print("âœ… æ£€æŸ¥ç‚¹å·²æ¸…é™¤")
            else:
                print("â„¹ï¸  æ²¡æœ‰æ‰¾åˆ°æ£€æŸ¥ç‚¹")
            return

        if not checkpoint.exists():
            print("â„¹ï¸  æ²¡æœ‰æ‰¾åˆ°æ£€æŸ¥ç‚¹")
            print(f"   å­˜å‚¨: {checkpoint.checkpoint_file}")
            return

        data = checkpoint.load_checkpoint()
        if not data:
            print("âŒ æ— æ³•åŠ è½½æ£€æŸ¥ç‚¹æ•°æ®")
            return

        print("\n" + "=" * 60)
        print("ğŸ“‚ æ£€æŸ¥ç‚¹ä¿¡æ¯:")
        print(f"  å­˜å‚¨: {checkpoint.checkpoint_file}")
        print(f"  çŠ¶æ€: {data.get('status', 'unknown')}")
        print(f"  å½“å‰é¡µ: {data.get('current_page', 0)}")
        print(f"  æœ€åå¸–å­ID: {data.get('last_thread_id', 'N/A')}")
        print(f"  åˆ›å»ºæ—¶é—´: {data.get('created_at', 'N/A')}")
        print(f"  æ›´æ–°æ—¶é—´: {data.get('last_update_time', 'N/A')}")

        seen_article_ids = data.get('seen_article_ids', [])
        if seen_article_ids:
            print(f"\nğŸ“‹ æ–‡ç« IDä¿¡æ¯:")
            print(f"  å·²çˆ¬å–æ–‡ç« æ•°: {len(seen_article_ids)}")
            if data.get('min_article_id'):
                print(f"  æœ€å°æ–‡ç« ID: {data.get('min_article_id')}")
            if data.get('max_article_id'):
                print(f"  æœ€å¤§æ–‡ç« ID: {data.get('max_article_id')}")

        stats = data.get('stats', {})
        if stats:
            print("\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
            print(f"  å·²çˆ¬å–å¸–å­: {stats.get('crawled_count', stats.get('articles_found', 0))}")
            print(f"  ä¸‹è½½å›¾ç‰‡: {stats.get('images_downloaded', 0)}")
            print(f"  å¤±è´¥æ•°: {stats.get('failed_count', 0)}")
            if 'last_error' in stats:
                print(f"  æœ€åé”™è¯¯: {stats['last_error']}")

        print("=" * 60)
    finally:
        storage.close()
