"""
CLIå‘½ä»¤å¤„ç†å‡½æ•°
"""
import asyncio
import re
import os
from typing import Dict, Any
from urllib.parse import urlparse
from loguru import logger

from config import Config, ConfigLoader, get_example_config, get_forum_boards, get_forum_urls, get_news_urls
from spiders import SpiderFactory
from spiders.dynamic_news_spider import DynamicNewsCrawler
from core.downloader import ImageDownloader
from core.checkpoint import CheckpointManager


def _extract_image_filename(url: str) -> str:
    """
    ä»å›¾ç‰‡URLæå–åŸå§‹æ–‡ä»¶å
    
    å¤„ç†é€»è¾‘ï¼š
    1. ä»URLè·¯å¾„æå–æ–‡ä»¶å
    2. å»æ‰å°ºå¯¸åç¼€ï¼ˆå¦‚ -1024x481ï¼‰
    3. ä¿ç•™åŸå§‹æ‰©å±•å
    
    Args:
        url: å›¾ç‰‡URL
    
    Returns:
        æ¸…ç†åçš„æ–‡ä»¶å
    """
    try:
        parsed = urlparse(url)
        filename = os.path.basename(parsed.path)
        
        # å»æ‰å°ºå¯¸åç¼€ï¼ˆå¦‚ -1024x481, -300x200 ç­‰ï¼‰
        clean_name = re.sub(r'-\d+x\d+', '', filename)
        
        # å»æ‰æŸ¥è¯¢å‚æ•°å¯èƒ½å¸¦æ¥çš„åç¼€
        clean_name = clean_name.split('?')[0]
        
        # å¦‚æœæ–‡ä»¶åä¸ºç©ºæˆ–æ— æ•ˆï¼Œç”Ÿæˆä¸€ä¸ªé»˜è®¤å
        if not clean_name or clean_name == '.' or '.' not in clean_name:
            import hashlib
            hash_name = hashlib.md5(url.encode()).hexdigest()[:12]
            clean_name = f"{hash_name}.jpg"
        
        return clean_name
    except Exception:
        import hashlib
        hash_name = hashlib.md5(url.encode()).hexdigest()[:12]
        return f"{hash_name}.jpg"


async def handle_crawl_url(args):
    """å¤„ç† crawl-url å­å‘½ä»¤"""
    print(f"\nğŸ“Œ å‘½ä»¤: çˆ¬å–å•ä¸ªURL")
    print(f"URL: {args.url}")
    
    # 1. åŠ è½½é…ç½®
    if args.auto_detect:
        logger.info(f"ğŸŒ è‡ªåŠ¨æ£€æµ‹é…ç½®: {args.url}")
        config = ConfigLoader.auto_detect(args.url)
    elif args.preset:
        logger.info(f"ğŸ“‹ ä½¿ç”¨è®ºå›ç±»å‹é¢„è®¾: {args.preset}")
        config = ConfigLoader.load(args.preset)
    elif args.config:
        logger.info(f"ğŸ“ ä½¿ç”¨é…ç½®æ–‡ä»¶: {args.config}")
        config = get_example_config(args.config)
    else:
        logger.error("âŒ è¯·æŒ‡å®šé…ç½®æ¥æº: --auto-detect æˆ– --preset æˆ– --config")
        return
    
    # 2. åˆ›å»ºçˆ¬è™«
    spider = SpiderFactory.create(config=config)
    
    # 3. çˆ¬å–URL
    async with spider:
        thread_id = spider.parser._extract_thread_id(args.url)
        thread_info = {
            'url': args.url,
            'thread_id': thread_id,
            'title': f'Thread-{thread_id}',
            'board': config.bbs.name
        }
        
        logger.info(f"ğŸš€ å¼€å§‹çˆ¬å–URL...")
        await spider.crawl_thread(thread_info)
        logger.info(f"âœ… çˆ¬å–å®Œæˆ")
        
        # è¾“å‡ºç»Ÿè®¡
        print_statistics(spider)


async def handle_crawl_urls(args):
    """å¤„ç† crawl-urls å­å‘½ä»¤"""
    print(f"\nğŸ“Œ å‘½ä»¤: çˆ¬å–é…ç½®ä¸­çš„URLåˆ—è¡¨")
    print(f"é…ç½®: {args.config}")
    
    # 1. åŠ è½½é…ç½®
    logger.info(f"ğŸ“ ä½¿ç”¨é…ç½®æ–‡ä»¶: {args.config}")
    config = get_example_config(args.config)
    
    # 2. è·å–URLåˆ—è¡¨
    urls = get_forum_urls(args.config)
    logger.info(f"ğŸ“ ä»é…ç½®æ–‡ä»¶åŠ è½½URL: {len(urls)} ä¸ª")
    
    if not urls:
        logger.error("âŒ é…ç½®æ–‡ä»¶ä¸­æ²¡æœ‰URLsï¼")
        return
    
    # 3. åˆ›å»ºçˆ¬è™«å¹¶å¹¶å‘çˆ¬å–
    spider = SpiderFactory.create(config=config)
    
    async with spider:
        logger.info(f"ğŸš€ å¼€å§‹å¹¶å‘çˆ¬å– {len(urls)} ä¸ªURL...")
        tasks = []
        for url in urls:
            thread_id = spider.parser._extract_thread_id(url)
            thread_info = {
                'url': url,
                'thread_id': thread_id,
                'title': f'Thread-{thread_id}',
                'board': config.bbs.name
            }
            tasks.append(spider.crawl_thread(thread_info))
        
        # ä½¿ç”¨ asyncio.gather å¹¶å‘æ‰§è¡Œ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for r in results if not isinstance(r, Exception))
        failed_count = len(results) - success_count
        logger.info(f"âœ… å®Œæˆ: æˆåŠŸ {success_count}, å¤±è´¥ {failed_count}")
        
        # è¾“å‡ºç»Ÿè®¡
        print_statistics(spider)


async def handle_crawl_board(args):
    """å¤„ç† crawl-board å­å‘½ä»¤"""
    print(f"\nğŸ“Œ å‘½ä»¤: çˆ¬å–å•ä¸ªæ¿å—")
    print(f"æ¿å—URL: {args.board_url}")
    if args.max_pages:
        print(f"æœ€å¤§é¡µæ•°: {args.max_pages}")
    else:
        print(f"æœ€å¤§é¡µæ•°: ä¸é™åˆ¶ï¼ˆçˆ¬å–æ‰€æœ‰é¡µï¼‰")
    
    # 1. åŠ è½½é…ç½®
    if args.auto_detect:
        logger.info(f"ğŸŒ è‡ªåŠ¨æ£€æµ‹é…ç½®: {args.board_url}")
        config = ConfigLoader.auto_detect(args.board_url)
    elif args.preset:
        logger.info(f"ğŸ“‹ ä½¿ç”¨è®ºå›ç±»å‹é¢„è®¾: {args.preset}")
        config = ConfigLoader.load(args.preset)
    elif args.config:
        logger.info(f"ğŸ“ ä½¿ç”¨é…ç½®æ–‡ä»¶: {args.config}")
        config = get_example_config(args.config)
    else:
        logger.error("âŒ è¯·æŒ‡å®šé…ç½®æ¥æº: --auto-detect æˆ– --preset æˆ– --config")
        return
    
    # 2. åˆ›å»ºçˆ¬è™«
    spider = SpiderFactory.create(config=config)
    
    # 3. çˆ¬å–æ¿å—
    async with spider:
        logger.info(f"ğŸš€ å¼€å§‹çˆ¬å–æ¿å—...")
        await spider.crawl_board(
            board_url=args.board_url,
            board_name=config.bbs.name,
            max_pages=args.max_pages,
            resume=args.resume,
            start_page=args.start_page
        )
        logger.info(f"âœ… çˆ¬å–å®Œæˆ")
        
        # è¾“å‡ºç»Ÿè®¡
        print_statistics(spider)


async def handle_crawl_boards(args):
    """å¤„ç† crawl-boards å­å‘½ä»¤"""
    print(f"\nğŸ“Œ å‘½ä»¤: çˆ¬å–é…ç½®ä¸­çš„æ‰€æœ‰æ¿å—")
    print(f"é…ç½®: {args.config}")
    if args.max_pages:
        print(f"æ¯ä¸ªæ¿å—æœ€å¤§é¡µæ•°: {args.max_pages}")
    else:
        print(f"æ¯ä¸ªæ¿å—æœ€å¤§é¡µæ•°: ä¸é™åˆ¶ï¼ˆçˆ¬å–æ‰€æœ‰é¡µï¼‰")
    
    # 1. åŠ è½½é…ç½®
    logger.info(f"ğŸ“ ä½¿ç”¨é…ç½®æ–‡ä»¶: {args.config}")
    config = get_example_config(args.config)
    
    # 2. è·å–æ¿å—åˆ—è¡¨
    boards_info = get_forum_boards(args.config)
    logger.info(f"ğŸ“ ä»é…ç½®æ–‡ä»¶åŠ è½½æ¿å—: {len(boards_info)} ä¸ª")
    
    if not boards_info:
        logger.error("âŒ é…ç½®æ–‡ä»¶ä¸­æ²¡æœ‰æ¿å—ï¼")
        return
    
    # 3. åˆ›å»ºçˆ¬è™«å¹¶å¹¶å‘çˆ¬å–
    spider = SpiderFactory.create(config=config)
    
    async with spider:
        if args.max_pages:
            logger.info(f"ğŸš€ å¼€å§‹å¹¶å‘çˆ¬å– {len(boards_info)} ä¸ªæ¿å—ï¼ˆæ¯ä¸ªæœ€å¤š {args.max_pages} é¡µï¼‰...")
        else:
            logger.info(f"ğŸš€ å¼€å§‹å¹¶å‘çˆ¬å– {len(boards_info)} ä¸ªæ¿å—ï¼ˆçˆ¬å–æ‰€æœ‰é¡µé¢ï¼‰...")
        
        tasks = []
        for board in boards_info:
            logger.info(f"ğŸ“ æ¿å—: {board['name']} - {board['url']}")
            tasks.append(spider.crawl_board(
                board_url=board['url'],
                board_name=board['name'],
                max_pages=args.max_pages,
                resume=args.resume,
                start_page=args.start_page
            ))
        
        # ä½¿ç”¨ asyncio.gather å¹¶å‘æ‰§è¡Œ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for r in results if not isinstance(r, Exception))
        failed_count = len(results) - success_count
        logger.info(f"âœ… å®Œæˆ: æˆåŠŸ {success_count}, å¤±è´¥ {failed_count}")
        
        # è¾“å‡ºç»Ÿè®¡
        print_statistics(spider)


async def _crawl_single_news_url(crawler, url, args, config):
    """çˆ¬å–å•ä¸ªæ–°é—»URLçš„è¾…åŠ©å‡½æ•°"""
    logger.info(f"ğŸš€ å¼€å§‹çˆ¬å–åŠ¨æ€æ–°é—»é¡µé¢: {url}")
    
    # é€‰æ‹©çˆ¬å–æ–¹å¼
    if args.method == 'ajax':
        articles = await crawler.crawl_dynamic_page_ajax(
            url,
            max_pages=args.max_pages
        )
    else:  # selenium
        articles = await crawler.crawl_dynamic_page_selenium(
            url,
            max_clicks=args.max_pages
        )
    
    if not articles:
        logger.warning(f"âš ï¸  {url} æ²¡æœ‰æ‰¾åˆ°æ–‡ç« ")
        return 0, 0
    
    logger.info(f"âœ… {url} å‘ç° {len(articles)} ç¯‡æ–‡ç« ")
    
    downloaded_images = 0
    total_images = 0
    
    # æ˜¯å¦ä¸‹è½½æ–‡ç« è¯¦æƒ…å’Œå›¾ç‰‡
    if args.download_images:
        logger.info(f"ğŸš€ å¼€å§‹ä¸‹è½½æ–‡ç« è¯¦æƒ…å’Œå›¾ç‰‡...")
        
        # çˆ¬å–æ–‡ç« è¯¦æƒ…
        full_articles = await crawler.crawl_articles_batch(articles)
        
        # ä»URLæå–åŸŸåä½œä¸ºå­˜å‚¨ç›®å½•
        domain = urlparse(url).netloc  # å¦‚ sxd.xd.com
        save_dir = config.image.download_dir / domain
        save_dir.mkdir(parents=True, exist_ok=True)
        
        async with ImageDownloader() as downloader:
            for article in full_articles:
                images = article.get('images', [])
                if not images:
                    continue
                
                total_images += len(images)
                article_id = article.get('article_id', 'unknown')
                
                # é€ä¸ªä¸‹è½½å›¾ç‰‡ï¼Œä½¿ç”¨è‡ªå®šä¹‰æ–‡ä»¶åæ ¼å¼
                for img_url in images:
                    # ä»å›¾ç‰‡URLæå–åŸå§‹æ–‡ä»¶å
                    img_filename = _extract_image_filename(img_url)
                    # ç”Ÿæˆæœ€ç»ˆæ–‡ä»¶å: [article_id]_[åŸå§‹å›¾ç‰‡å]
                    final_filename = f"{article_id}_{img_filename}"
                    save_path = save_dir / final_filename
                    
                    # ä¸‹è½½å›¾ç‰‡
                    metadata = {
                        'article_id': article_id,
                        'title': article.get('title', ''),
                        'article_url': article.get('url', ''),
                        'image_url': img_url
                    }
                    
                    result = await downloader.download_image(img_url, save_path, metadata)
                    if result.get('success'):
                        downloaded_images += 1
                    
                    # æ·»åŠ å»¶è¿Ÿ
                    await asyncio.sleep(config.crawler.download_delay)
        
        logger.success(f"âœ… {url} å›¾ç‰‡ä¸‹è½½å®Œæˆ: {downloaded_images}/{total_images}")
    
    return len(articles), downloaded_images


async def handle_crawl_news(args):
    """å¤„ç† crawl-news å­å‘½ä»¤"""
    print(f"\nğŸ“Œ å‘½ä»¤: çˆ¬å–åŠ¨æ€æ–°é—»é¡µé¢")
    print(f"æ–¹å¼: {args.method}")
    if args.max_pages:
        print(f"æœ€å¤§é¡µæ•°: {args.max_pages}")
    else:
        print(f"æœ€å¤§é¡µæ•°: ä¸é™åˆ¶ï¼ˆçˆ¬å–æ‰€æœ‰é¡µï¼‰")
    
    # ç¡®å®šè¦çˆ¬å–çš„URLåˆ—è¡¨
    news_urls = []
    
    if args.config:
        # ä»é…ç½®æ–‡ä»¶è¯»å–news URLs
        logger.info(f"ğŸ“ ä½¿ç”¨é…ç½®æ–‡ä»¶: {args.config}")
        config = get_example_config(args.config)
        news_urls = get_news_urls(args.config)
        
        if not news_urls:
            logger.error(f"âŒ é…ç½®æ–‡ä»¶ {args.config} ä¸­æ²¡æœ‰æ‰¾åˆ° news_urlsï¼")
            return
        
        logger.info(f"ğŸ“ ä»é…ç½®æ–‡ä»¶åŠ è½½ {len(news_urls)} ä¸ªæ–°é—»URL")
        for url in news_urls:
            print(f"  - {url}")
    elif args.url:
        # ä½¿ç”¨å‘½ä»¤è¡Œæä¾›çš„å•ä¸ªURL
        news_urls = [args.url]
        print(f"URL: {args.url}")
        
        # åˆ›å»ºé»˜è®¤é…ç½®
        logger.info(f"ğŸŒ ä½¿ç”¨é»˜è®¤é…ç½®")
        parsed = urlparse(args.url)
        base_url = f"{parsed.scheme}://{parsed.netloc}"
        
        config = Config(
            bbs={
                "name": "åŠ¨æ€æ–°é—»ç½‘ç«™",
                "base_url": base_url,
                "forum_type": "custom"
            }
        )
    else:
        logger.error("âŒ è¯·æä¾›URLæˆ–ä½¿ç”¨--configå‚æ•°æŒ‡å®šé…ç½®æ–‡ä»¶ï¼")
        return
    
    # åˆ›å»ºçˆ¬è™«
    crawler = DynamicNewsCrawler(config)
    
    # çˆ¬å–æ‰€æœ‰URL
    async with crawler:
        total_articles = 0
        total_downloaded_images = 0
        
        for url in news_urls:
            articles_count, images_count = await _crawl_single_news_url(
                crawler, url, args, config
            )
            total_articles += articles_count
            total_downloaded_images += images_count
        
        # è¾“å‡ºç»Ÿè®¡
        stats = crawler.get_statistics()
        print("\n" + "=" * 60)
        print("ğŸ“Š çˆ¬å–ç»Ÿè®¡:")
        print(f"  çˆ¬å–URLæ•°: {len(news_urls)}")
        print(f"  å‘ç°æ–‡ç« : {total_articles}")
        if args.download_images:
            print(f"  çˆ¬å–è¯¦æƒ…: {stats['articles_crawled']}")
            print(f"  çˆ¬å–å¤±è´¥: {stats['articles_failed']}")
            print(f"  ä¸‹è½½å›¾ç‰‡: {total_downloaded_images}")
        print("=" * 60)


def print_statistics(spider):
    """è¾“å‡ºç»Ÿè®¡ä¿¡æ¯"""
    stats = spider.get_statistics()
    print("\n" + "=" * 60)
    print("ğŸ“Š çˆ¬å–ç»Ÿè®¡:")
    print(f"  å¸–å­æ•°: {stats['threads_crawled']}")
    print(f"  å‘ç°å›¾ç‰‡: {stats['images_found']}")
    print(f"  ä¸‹è½½æˆåŠŸ: {stats['images_downloaded']}")
    print(f"  ä¸‹è½½å¤±è´¥: {stats['images_failed']}")
    print(f"  å»é‡è·³è¿‡: {stats['duplicates_skipped']}")
    print("=" * 60)


async def handle_checkpoint_status(args):
    """å¤„ç† checkpoint-status å­å‘½ä»¤"""
    print(f"\nğŸ“Œ å‘½ä»¤: æŸ¥çœ‹æ£€æŸ¥ç‚¹çŠ¶æ€")
    print(f"ç½‘ç«™: {args.site}")
    print(f"æ¿å—: {args.board}")
    
    # åˆ›å»ºæ£€æŸ¥ç‚¹ç®¡ç†å™¨
    checkpoint = CheckpointManager(site=args.site, board=args.board)
    
    # å¦‚æœæŒ‡å®šæ¸…é™¤
    if args.clear:
        if checkpoint.exists():
            checkpoint.clear_checkpoint()
            print("âœ… æ£€æŸ¥ç‚¹å·²æ¸…é™¤")
        else:
            print("â„¹ï¸  æ²¡æœ‰æ‰¾åˆ°æ£€æŸ¥ç‚¹")
        return
    
    # æŸ¥çœ‹æ£€æŸ¥ç‚¹çŠ¶æ€
    if not checkpoint.exists():
        print("â„¹ï¸  æ²¡æœ‰æ‰¾åˆ°æ£€æŸ¥ç‚¹")
        print(f"   æ£€æŸ¥ç‚¹æ–‡ä»¶: {checkpoint.checkpoint_file}")
        return
    
    # åŠ è½½æ£€æŸ¥ç‚¹æ•°æ®
    data = checkpoint.load_checkpoint()
    if not data:
        print("âŒ æ— æ³•åŠ è½½æ£€æŸ¥ç‚¹æ•°æ®")
        return
    
    # æ˜¾ç¤ºæ£€æŸ¥ç‚¹ä¿¡æ¯
    print("\n" + "=" * 60)
    print("ğŸ“‚ æ£€æŸ¥ç‚¹ä¿¡æ¯:")
    print(f"  æ–‡ä»¶è·¯å¾„: {checkpoint.checkpoint_file}")
    print(f"  çŠ¶æ€: {data.get('status', 'unknown')}")
    print(f"  å½“å‰é¡µ: {data.get('current_page', 0)}")
    print(f"  æœ€åå¸–å­ID: {data.get('last_thread_id', 'N/A')}")
    print(f"  åˆ›å»ºæ—¶é—´: {data.get('created_at', 'N/A')}")
    print(f"  æ›´æ–°æ—¶é—´: {data.get('last_update_time', 'N/A')}")
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    stats = data.get('stats', {})
    if stats:
        print("\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"  å·²çˆ¬å–å¸–å­: {stats.get('crawled_count', 0)}")
        print(f"  ä¸‹è½½å›¾ç‰‡: {stats.get('images_downloaded', 0)}")
        print(f"  å¤±è´¥æ•°: {stats.get('failed_count', 0)}")
        if 'last_error' in stats:
            print(f"  æœ€åé”™è¯¯: {stats['last_error']}")
    
    print("=" * 60)
