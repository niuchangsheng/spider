# æ£€æŸ¥ç‚¹ç®¡ç†å™¨ä½¿ç”¨ç¤ºä¾‹

## åŸºæœ¬ä½¿ç”¨

### 1. åˆ›å»ºæ£€æŸ¥ç‚¹ç®¡ç†å™¨

```python
from core.checkpoint import CheckpointManager

# æ–¹å¼1: ç›´æ¥åˆ›å»º
checkpoint = CheckpointManager(
    site="https://sxd.xd.com/",
    board="all"
)

# æ–¹å¼2: ä½¿ç”¨ä¾¿æ·å‡½æ•°
from core.checkpoint import get_checkpoint_manager
checkpoint = get_checkpoint_manager("sxd.xd.com", "all")
```

### 2. ä¿å­˜æ£€æŸ¥ç‚¹

```python
# åœ¨çˆ¬å–è¿‡ç¨‹ä¸­ä¿å­˜æ£€æŸ¥ç‚¹
checkpoint.save_checkpoint(
    current_page=15,
    last_thread_id="12345",
    last_thread_url="https://sxd.xd.com/article/12345",
    status="running",
    stats={
        "crawled_count": 1500,
        "failed_count": 5,
        "images_downloaded": 3000
    }
)
```

### 3. åŠ è½½æ£€æŸ¥ç‚¹ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰

```python
# æ£€æŸ¥æ˜¯å¦æœ‰æ£€æŸ¥ç‚¹
if checkpoint.exists():
    checkpoint_data = checkpoint.load_checkpoint()
    
    if checkpoint_data:
        start_page = checkpoint_data['current_page']
        last_thread_id = checkpoint_data.get('last_thread_id')
        
        print(f"ğŸ”„ ä»æ£€æŸ¥ç‚¹æ¢å¤: ç¬¬ {start_page} é¡µ")
        print(f"   æœ€åçˆ¬å–çš„å¸–å­ID: {last_thread_id}")
        
        # ä» start_page ç»§ç»­çˆ¬å–
        # ...
```

### 4. æ ‡è®°å®Œæˆ

```python
# çˆ¬å–å®Œæˆåæ ‡è®°
checkpoint.mark_completed(final_stats={
    "total_crawled": 2000,
    "total_images": 5000
})
```

### 5. æ¸…é™¤æ£€æŸ¥ç‚¹

```python
# æ¸…é™¤æ£€æŸ¥ç‚¹ï¼ˆé‡æ–°å¼€å§‹ï¼‰
checkpoint.clear_checkpoint()
```

## åœ¨çˆ¬è™«ä¸­é›†æˆ

### ä¿®æ”¹ `spiders/bbs_spider.py`

```python
from core.checkpoint import CheckpointManager

class BBSSpider(BaseSpider):
    async def crawl_board(self, board_url: str, board_name: str, max_pages: Optional[int] = None):
        """çˆ¬å–æ¿å—ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰"""
        # 1. åˆ›å»ºæ£€æŸ¥ç‚¹ç®¡ç†å™¨
        checkpoint = CheckpointManager(
            site=self.config.bbs.base_url,
            board=board_name
        )
        
        # 2. åŠ è½½æ£€æŸ¥ç‚¹
        checkpoint_data = checkpoint.load_checkpoint()
        start_page = checkpoint_data['current_page'] if checkpoint_data else 1
        
        if checkpoint_data:
            logger.info(f"ğŸ”„ ä»æ£€æŸ¥ç‚¹æ¢å¤: ç¬¬ {start_page} é¡µ")
            if checkpoint_data.get('status') == 'completed':
                logger.info("âœ… è¯¥æ¿å—å·²å®Œæˆçˆ¬å–")
                return
        
        # 3. ä»æ£€æŸ¥ç‚¹ä½ç½®ç»§ç»­çˆ¬å–
        page_count = 0
        current_url = board_url
        
        for page in range(start_page, max_pages or float('inf')):
            logger.info(f"ğŸ“„ çˆ¬å–ç¬¬ {page} é¡µ...")
            
            # è·å–é¡µé¢
            html = await self.fetch_page(current_url)
            if not html:
                checkpoint.mark_error("æ— æ³•è·å–é¡µé¢")
                break
            
            # è§£æå¸–å­åˆ—è¡¨
            threads = self.parser.parse_thread_list(html, current_url)
            if not threads:
                logger.warning(f"âš ï¸  ç¬¬ {page} é¡µæ²¡æœ‰æ‰¾åˆ°å¸–å­")
                break
            
            # çˆ¬å–æ¯ä¸ªå¸–å­
            last_thread_id = None
            for thread_info in threads:
                await self.crawl_thread(thread_info)
                last_thread_id = thread_info.get('thread_id')
            
            # 4. ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆæ¯é¡µä¿å­˜ä¸€æ¬¡ï¼‰
            checkpoint.save_checkpoint(
                current_page=page + 1,  # ä¸‹ä¸€é¡µ
                last_thread_id=last_thread_id,
                last_thread_url=threads[-1].get('url') if threads else None,
                status="running",
                stats={
                    "crawled_count": self.stats['threads_crawled'],
                    "failed_count": self.stats['images_failed'],
                    "images_downloaded": self.stats['images_downloaded']
                }
            )
            
            # æŸ¥æ‰¾ä¸‹ä¸€é¡µ
            next_url = self.parser.find_next_page(html, current_url)
            if not next_url:
                logger.info("âœ… å·²åˆ°è¾¾æœ€åä¸€é¡µ")
                break
            
            current_url = next_url
            page_count += 1
        
        # 5. æ ‡è®°å®Œæˆ
        checkpoint.mark_completed(final_stats={
            "total_crawled": self.stats['threads_crawled'],
            "total_images": self.stats['images_downloaded']
        })
        
        logger.success(f"ğŸ‰ æ¿å—çˆ¬å–å®Œæˆ: {board_name}")
```

## æ£€æŸ¥ç‚¹æ–‡ä»¶æ ¼å¼

æ£€æŸ¥ç‚¹æ–‡ä»¶ä¿å­˜åœ¨ `checkpoints/` ç›®å½•ä¸‹ï¼Œæ ¼å¼ä¸º JSONï¼š

```json
{
  "site": "sxd.xd.com",
  "board": "all",
  "current_page": 15,
  "last_thread_id": "12345",
  "last_thread_url": "https://sxd.xd.com/article/12345",
  "status": "running",
  "created_at": "2026-02-06T10:30:00",
  "last_update_time": "2026-02-06T11:45:00",
  "stats": {
    "crawled_count": 1500,
    "failed_count": 5,
    "images_downloaded": 3000
  }
}
```

## å‘½ä»¤è¡Œä½¿ç”¨

### æŸ¥çœ‹æ£€æŸ¥ç‚¹çŠ¶æ€

```python
# æŸ¥çœ‹æ£€æŸ¥ç‚¹
from core.checkpoint import CheckpointManager

checkpoint = CheckpointManager("sxd.xd.com", "all")
if checkpoint.exists():
    data = checkpoint.load_checkpoint()
    print(f"çŠ¶æ€: {data['status']}")
    print(f"å½“å‰é¡µ: {data['current_page']}")
    print(f"å·²çˆ¬å–: {data['stats'].get('crawled_count', 0)}")
else:
    print("æ²¡æœ‰æ£€æŸ¥ç‚¹")
```

### æ¸…é™¤æ£€æŸ¥ç‚¹

```python
checkpoint = CheckpointManager("sxd.xd.com", "all")
checkpoint.clear_checkpoint()
print("æ£€æŸ¥ç‚¹å·²æ¸…é™¤")
```

## æ³¨æ„äº‹é¡¹

1. **æ£€æŸ¥ç‚¹æ–‡ä»¶ä½ç½®**: é»˜è®¤ä¿å­˜åœ¨é¡¹ç›®æ ¹ç›®å½•çš„ `checkpoints/` ç›®å½•
2. **æ–‡ä»¶å‘½å**: `{site}_{board}.json`ï¼Œç‰¹æ®Šå­—ç¬¦ä¼šè¢«æ›¿æ¢ä¸ºä¸‹åˆ’çº¿
3. **è‡ªåŠ¨åˆ›å»ºç›®å½•**: å¦‚æœ `checkpoints/` ç›®å½•ä¸å­˜åœ¨ï¼Œä¼šè‡ªåŠ¨åˆ›å»º
4. **ç¼–ç **: æ–‡ä»¶ä½¿ç”¨ UTF-8 ç¼–ç ï¼Œæ”¯æŒä¸­æ–‡
5. **çº¿ç¨‹å®‰å…¨**: å½“å‰å®ç°ä¸æ˜¯çº¿ç¨‹å®‰å…¨çš„ï¼Œå¤šè¿›ç¨‹/å¤šçº¿ç¨‹ä½¿ç”¨æ—¶éœ€è¦åŠ é”

## æ•…éšœæ¢å¤

å¦‚æœçˆ¬å–è¿‡ç¨‹ä¸­æ–­ï¼š

1. **è‡ªåŠ¨æ¢å¤**: ä¸‹æ¬¡è¿è¡Œæ—¶ä¼šè‡ªåŠ¨ä»æ£€æŸ¥ç‚¹æ¢å¤
2. **æ‰‹åŠ¨æ¢å¤**: å¯ä»¥æ‰‹åŠ¨ç¼–è¾‘ JSON æ–‡ä»¶ä¿®æ”¹ `current_page`
3. **é‡æ–°å¼€å§‹**: åˆ é™¤æ£€æŸ¥ç‚¹æ–‡ä»¶æˆ–è°ƒç”¨ `clear_checkpoint()`
