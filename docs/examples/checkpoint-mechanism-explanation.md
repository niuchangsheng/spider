# æ£€æŸ¥ç‚¹æœºåˆ¶è¯¦è§£

## ğŸ“‹ å½“å‰æ£€æŸ¥ç‚¹æœºåˆ¶

### 1. æ£€æŸ¥ç‚¹è®°å½•æ–¹å¼

å½“å‰æ£€æŸ¥ç‚¹æœºåˆ¶**åŸºäºé¡µç ï¼ˆpage-basedï¼‰**ï¼Œé€‚ç”¨äºä¼ ç»Ÿçš„åˆ†é¡µç½‘ç«™ã€‚

#### æ£€æŸ¥ç‚¹æ•°æ®ç»“æ„

```json
{
  "site": "sxd.xd.com",
  "board": "all",
  "current_page": 6,              // å½“å‰é¡µç ï¼ˆå…³é”®å­—æ®µï¼‰
  "last_thread_id": "article_25", // æœ€åçˆ¬å–çš„æ–‡ç« IDï¼ˆè¾…åŠ©ä¿¡æ¯ï¼‰
  "last_thread_url": "https://...",
  "status": "running",
  "stats": {
    "crawled_count": 25,
    "images_downloaded": 50
  }
}
```

#### ä¿å­˜æ—¶æœº

```python
# åœ¨ spiders/bbs_spider.py ä¸­
# æ¯çˆ¬å–ä¸€é¡µåä¿å­˜æ£€æŸ¥ç‚¹
checkpoint.save_checkpoint(
    current_page=page + 1,  # ä¸‹ä¸€é¡µ
    last_thread_id=last_thread_id,
    status="running",
    stats={...}
)
```

#### æ¢å¤é€»è¾‘

```python
# ä»æ£€æŸ¥ç‚¹æ¢å¤
checkpoint_data = checkpoint.load_checkpoint()
start_page = checkpoint_data['current_page']  # ä»è¿™ä¸€é¡µå¼€å§‹

# ç»§ç»­çˆ¬å–
for page in range(start_page, max_pages):
    # çˆ¬å–é€»è¾‘...
```

---

## âš ï¸ é—®é¢˜åˆ†æï¼šsxd.xd.com çš„ç‰¹æ®Šæƒ…å†µ

### é—®é¢˜åœºæ™¯

**sxd.xd.com çš„æ–‡ç« æ’åˆ—æ–¹å¼**ï¼š
- **å€’åºæ’åˆ—**ï¼šç¬¬ä¸€é¡µæ˜¯æœ€æ–°çš„æ–‡ç« ï¼ˆarticle_id æœ€å¤§ï¼‰
- **ç¤ºä¾‹**ï¼š
  ```
  ç¬¬1é¡µ: article_id = 15503, 15502, 15501, ... (æœ€å¤§)
  ç¬¬2é¡µ: article_id = 15470, 15469, 15468, ... (ä¸­ç­‰)
  ç¬¬3é¡µ: article_id = 15450, 15449, 15448, ... (è¾ƒå°)
  ```

### å½“å‰æœºåˆ¶çš„é—®é¢˜

#### é—®é¢˜1: åŸºäºé¡µç çš„æ£€æŸ¥ç‚¹ä¸é€‚ç”¨

**å½“å‰å®ç°**ï¼š
```python
# æ£€æŸ¥ç‚¹ä¿å­˜ï¼šcurrent_page = 6
# æ¢å¤æ—¶ï¼šä»ç¬¬6é¡µå¼€å§‹çˆ¬å–
```

**é—®é¢˜**ï¼š
- å¦‚æœä¸­æ–­åœ¨ç¬¬3é¡µï¼Œæ¢å¤æ—¶ä»ç¬¬6é¡µå¼€å§‹
- **ä¼šè·³è¿‡ç¬¬4ã€5é¡µçš„å†…å®¹** âŒ
- é¡µç å’Œ article_id æ²¡æœ‰ç›´æ¥å¯¹åº”å…³ç³»

#### é—®é¢˜2: DynamicNewsCrawler æœªé›†æˆæ£€æŸ¥ç‚¹

**å½“å‰çŠ¶æ€**ï¼š
- `BBSSpider.crawl_board()` âœ… å·²é›†æˆæ£€æŸ¥ç‚¹
- `DynamicNewsCrawler.crawl_dynamic_page_ajax()` âŒ **æœªé›†æˆæ£€æŸ¥ç‚¹**

**å½±å“**ï¼š
- `crawl-news` å‘½ä»¤ä¸æ”¯æŒæ–­ç‚¹ç»­ä¼ 
- ä¸­æ–­åéœ€è¦ä»å¤´å¼€å§‹

---

## ğŸ”§ è§£å†³æ–¹æ¡ˆ

### æ–¹æ¡ˆ1: åŸºäº article_id çš„æ£€æŸ¥ç‚¹ï¼ˆæ¨èï¼‰â­

**æ ¸å¿ƒæ€è·¯**ï¼šè®°å½•å·²çˆ¬å–çš„æœ€å° article_idï¼Œæ¢å¤æ—¶è·³è¿‡å·²çˆ¬å–çš„æ–‡ç« ã€‚

#### æ”¹è¿›åçš„æ£€æŸ¥ç‚¹æ•°æ®ç»“æ„

```json
{
  "site": "sxd.xd.com",
  "board": "all",
  "current_page": 6,
  "min_article_id": 15450,        // ğŸ†• å·²çˆ¬å–çš„æœ€å° article_id
  "max_article_id": 15503,        // ğŸ†• å·²çˆ¬å–çš„æœ€å¤§ article_id
  "last_thread_id": "article_15450",
  "seen_article_ids": [            // ğŸ†• å·²çˆ¬å–çš„ article_id é›†åˆï¼ˆå¯é€‰ï¼‰
    "15503", "15502", "15501", ...
  ],
  "status": "running",
  "stats": {...}
}
```

#### æ¢å¤é€»è¾‘

```python
# åŠ è½½æ£€æŸ¥ç‚¹
checkpoint_data = checkpoint.load_checkpoint()
min_article_id = checkpoint_data.get('min_article_id')

# çˆ¬å–æ—¶è·³è¿‡å·²çˆ¬å–çš„æ–‡ç« 
for article in articles:
    article_id = int(article['article_id'])
    
    # å¦‚æœ article_id >= min_article_idï¼Œè¯´æ˜å·²çˆ¬å–
    if min_article_id and article_id >= min_article_id:
        logger.info(f"â­ï¸  è·³è¿‡å·²çˆ¬å–æ–‡ç« : {article_id}")
        continue
    
    # çˆ¬å–æ–°æ–‡ç« 
    await crawl_article(article)
    
    # æ›´æ–°æœ€å° article_id
    if not min_article_id or article_id < min_article_id:
        min_article_id = article_id
        checkpoint.save_checkpoint(
            min_article_id=min_article_id,
            ...
        )
```

#### ä¼˜ç‚¹

- âœ… é€‚ç”¨äºå€’åºæ’åˆ—çš„ç½‘ç«™
- âœ… ä¸ä¾èµ–é¡µç ï¼Œæ›´å¯é 
- âœ… è‡ªåŠ¨è·³è¿‡å·²çˆ¬å–çš„æ–‡ç« 
- âœ… æ”¯æŒæ–‡ç« é¡ºåºå˜åŒ–ï¼ˆå¦‚æ–°æ–‡ç« æ’å…¥ï¼‰

#### ç¼ºç‚¹

- âš ï¸ éœ€è¦ article_id æ˜¯æ•°å­—ä¸”å¯æ¯”è¾ƒ
- âš ï¸ å¦‚æœ article_id ä¸æ˜¯æ•°å­—ï¼Œéœ€è¦å…¶ä»–ç­–ç•¥

---

### æ–¹æ¡ˆ2: åŸºäº URL å»é‡çš„æ£€æŸ¥ç‚¹

**æ ¸å¿ƒæ€è·¯**ï¼šè®°å½•å·²çˆ¬å–çš„ article_id é›†åˆï¼Œæ¢å¤æ—¶é€šè¿‡å»é‡æœºåˆ¶è·³è¿‡ã€‚

#### å®ç°æ–¹å¼

```python
# åœ¨ DynamicNewsCrawler ä¸­
seen_article_ids = set()

# ä»æ£€æŸ¥ç‚¹æ¢å¤
checkpoint_data = checkpoint.load_checkpoint()
if checkpoint_data:
    seen_article_ids = set(checkpoint_data.get('seen_article_ids', []))

# çˆ¬å–æ—¶å»é‡
for article in articles:
    article_id = article['article_id']
    
    if article_id in seen_article_ids:
        logger.info(f"â­ï¸  è·³è¿‡å·²çˆ¬å–æ–‡ç« : {article_id}")
        continue
    
    # çˆ¬å–æ–°æ–‡ç« 
    await crawl_article(article)
    seen_article_ids.add(article_id)
    
    # ä¿å­˜æ£€æŸ¥ç‚¹
    checkpoint.save_checkpoint(
        seen_article_ids=list(seen_article_ids),
        ...
    )
```

#### ä¼˜ç‚¹

- âœ… é€‚ç”¨äºä»»ä½•ç±»å‹çš„ article_idï¼ˆæ•°å­—ã€å­—ç¬¦ä¸²ï¼‰
- âœ… ç²¾ç¡®å»é‡ï¼Œä¸ä¼šé—æ¼
- âœ… å®ç°ç®€å•

#### ç¼ºç‚¹

- âš ï¸ å¦‚æœæ–‡ç« æ•°é‡å¾ˆå¤§ï¼Œseen_article_ids é›†åˆä¼šå¾ˆå¤§
- âš ï¸ æ£€æŸ¥ç‚¹æ–‡ä»¶ä¼šå˜å¤§

---

### æ–¹æ¡ˆ3: æ··åˆæ–¹æ¡ˆï¼ˆæœ€ä½³å®è·µï¼‰â­

**æ ¸å¿ƒæ€è·¯**ï¼šç»“åˆé¡µç å’Œ article_idï¼Œæä¾›åŒé‡ä¿éšœã€‚

#### æ£€æŸ¥ç‚¹æ•°æ®ç»“æ„

```json
{
  "site": "sxd.xd.com",
  "board": "all",
  "current_page": 6,              // é¡µç ï¼ˆç”¨äºå¿«é€Ÿå®šä½ï¼‰
  "min_article_id": 15450,        // æœ€å° article_idï¼ˆç”¨äºç²¾ç¡®å»é‡ï¼‰
  "last_thread_id": "article_15450",
  "crawl_direction": "desc",      // ğŸ†• çˆ¬å–æ–¹å‘ï¼šdesc(å€’åº) / asc(æ­£åº)
  "status": "running",
  "stats": {...}
}
```

#### æ¢å¤é€»è¾‘

```python
# åŠ è½½æ£€æŸ¥ç‚¹
checkpoint_data = checkpoint.load_checkpoint()
start_page = checkpoint_data.get('current_page', 1)
min_article_id = checkpoint_data.get('min_article_id')
crawl_direction = checkpoint_data.get('crawl_direction', 'desc')

# ä»æŒ‡å®šé¡µå¼€å§‹çˆ¬å–
for page in range(start_page, max_pages):
    articles = await fetch_page(page)
    
    for article in articles:
        article_id = int(article['article_id'])
        
        # æ ¹æ®çˆ¬å–æ–¹å‘åˆ¤æ–­æ˜¯å¦å·²çˆ¬å–
        if crawl_direction == 'desc':
            # å€’åºï¼šarticle_id è¶Šå°ï¼Œè¶Šæ—§
            if min_article_id and article_id <= min_article_id:
                logger.info(f"â­ï¸  è·³è¿‡å·²çˆ¬å–æ–‡ç« : {article_id} (<= {min_article_id})")
                continue
        else:
            # æ­£åºï¼šarticle_id è¶Šå¤§ï¼Œè¶Šæ–°
            if min_article_id and article_id >= min_article_id:
                logger.info(f"â­ï¸  è·³è¿‡å·²çˆ¬å–æ–‡ç« : {article_id} (>= {min_article_id})")
                continue
        
        # çˆ¬å–æ–°æ–‡ç« 
        await crawl_article(article)
        
        # æ›´æ–°æœ€å°/æœ€å¤§ article_id
        if crawl_direction == 'desc':
            if not min_article_id or article_id < min_article_id:
                min_article_id = article_id
        else:
            if not min_article_id or article_id > min_article_id:
                min_article_id = article_id
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        checkpoint.save_checkpoint(
            current_page=page + 1,
            min_article_id=min_article_id,
            ...
        )
```

---

## ğŸ“Š å½“å‰å®ç°çŠ¶æ€

### âœ… å·²å®ç°

1. **BBSSpider** - åŸºäºé¡µç çš„æ£€æŸ¥ç‚¹
   - é€‚ç”¨äºä¼ ç»Ÿåˆ†é¡µè®ºå›
   - é¡µç å’Œå†…å®¹é¡ºåºä¸€è‡´

2. **æ£€æŸ¥ç‚¹ç®¡ç†å™¨** - é€šç”¨æ£€æŸ¥ç‚¹åŠŸèƒ½
   - ä¿å­˜/åŠ è½½æ£€æŸ¥ç‚¹
   - æ”¯æŒå¤šç§çŠ¶æ€

### âŒ æœªå®ç°

1. **DynamicNewsCrawler** - æœªé›†æˆæ£€æŸ¥ç‚¹
   - `crawl-news` å‘½ä»¤ä¸æ”¯æŒæ–­ç‚¹ç»­ä¼ 
   - éœ€è¦æ·»åŠ æ£€æŸ¥ç‚¹æ”¯æŒ

2. **åŸºäº article_id çš„æ£€æŸ¥ç‚¹** - æœªå®ç°
   - å½“å‰åªæ”¯æŒåŸºäºé¡µç 
   - éœ€è¦æ·»åŠ  article_id å»é‡é€»è¾‘

---

## ğŸ¯ é’ˆå¯¹ sxd.xd.com çš„æ”¹è¿›å»ºè®®

### æ”¹è¿›1: ä¸º DynamicNewsCrawler æ·»åŠ æ£€æŸ¥ç‚¹æ”¯æŒ

```python
# åœ¨ DynamicNewsCrawler.crawl_dynamic_page_ajax() ä¸­
async def crawl_dynamic_page_ajax(self, base_url: str, max_pages: Optional[int] = None):
    # 1. åˆ›å»ºæ£€æŸ¥ç‚¹ç®¡ç†å™¨
    checkpoint = CheckpointManager(site=self.config.bbs.base_url, board="news")
    
    # 2. ä»æ£€æŸ¥ç‚¹æ¢å¤
    checkpoint_data = checkpoint.load_checkpoint()
    seen_article_ids = set()
    start_page = 1
    
    if checkpoint_data and checkpoint_data.get('status') != 'completed':
        seen_article_ids = set(checkpoint_data.get('seen_article_ids', []))
        start_page = checkpoint_data.get('current_page', 1)
        logger.info(f"ğŸ”„ ä»æ£€æŸ¥ç‚¹æ¢å¤: ç¬¬ {start_page} é¡µï¼Œå·²çˆ¬å– {len(seen_article_ids)} ç¯‡æ–‡ç« ")
    
    # 3. ä» start_page å¼€å§‹çˆ¬å–
    page = start_page
    all_articles = []
    
    while True:
        # ... çˆ¬å–é€»è¾‘ ...
        
        # 4. è¿‡æ»¤å·²çˆ¬å–çš„æ–‡ç« 
        new_articles = []
        for article in articles:
            article_id = article['article_id']
            if article_id not in seen_article_ids:
                seen_article_ids.add(article_id)
                new_articles.append(article)
        
        # 5. ä¿å­˜æ£€æŸ¥ç‚¹
        checkpoint.save_checkpoint(
            current_page=page + 1,
            seen_article_ids=list(seen_article_ids),
            last_thread_id=new_articles[-1]['article_id'] if new_articles else None,
            status="running",
            stats={
                "articles_found": len(all_articles),
                "articles_crawled": len(all_articles)
            }
        )
        
        page += 1
```

### æ”¹è¿›2: æ·»åŠ  article_id å»é‡é€»è¾‘

```python
# åœ¨çˆ¬å–è¿‡ç¨‹ä¸­
for article in articles:
    article_id = article['article_id']
    
    # æ£€æŸ¥æ˜¯å¦å·²çˆ¬å–ï¼ˆé€šè¿‡æ£€æŸ¥ç‚¹ï¼‰
    if article_id in seen_article_ids:
        logger.debug(f"â­ï¸  è·³è¿‡å·²çˆ¬å–æ–‡ç« : {article_id}")
        continue
    
    # æ£€æŸ¥æ˜¯å¦å·²ä¸‹è½½ï¼ˆé€šè¿‡æ–‡ä»¶ç³»ç»Ÿï¼‰
    # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œä¹Ÿè·³è¿‡
    if article_id in downloaded_article_ids:
        logger.debug(f"â­ï¸  è·³è¿‡å·²ä¸‹è½½æ–‡ç« : {article_id}")
        continue
    
    # çˆ¬å–æ–°æ–‡ç« 
    await crawl_article(article)
    seen_article_ids.add(article_id)
```

---

## ğŸ” å½“å‰æœºåˆ¶çš„å·¥ä½œæµç¨‹

### BBSè®ºå›ï¼ˆBBSSpiderï¼‰

```
1. å¼€å§‹çˆ¬å–ç¬¬1é¡µ
   â†’ çˆ¬å–å¸–å­1, 2, 3...
   â†’ ä¿å­˜æ£€æŸ¥ç‚¹: current_page = 2

2. ä¸­æ–­

3. æ¢å¤çˆ¬å–
   â†’ åŠ è½½æ£€æŸ¥ç‚¹: current_page = 2
   â†’ ä»ç¬¬2é¡µå¼€å§‹çˆ¬å–
   â†’ âœ… æ­£å¸¸å·¥ä½œï¼ˆå› ä¸ºé¡µç å’Œå†…å®¹é¡ºåºä¸€è‡´ï¼‰
```

### åŠ¨æ€æ–°é—»ï¼ˆDynamicNewsCrawlerï¼‰- å½“å‰æœªæ”¯æŒ

```
1. å¼€å§‹çˆ¬å–ç¬¬1é¡µ
   â†’ å‘ç°æ–‡ç« : 15503, 15502, 15501 (å€’åº)
   â†’ âŒ æœªä¿å­˜æ£€æŸ¥ç‚¹

2. ä¸­æ–­

3. æ¢å¤çˆ¬å–
   â†’ âŒ æ²¡æœ‰æ£€æŸ¥ç‚¹
   â†’ ä»å¤´å¼€å§‹çˆ¬å–
   â†’ âš ï¸ ä¼šé‡å¤çˆ¬å–ç¬¬1é¡µçš„å†…å®¹
```

---

## ğŸ’¡ æ€»ç»“

### å½“å‰æ£€æŸ¥ç‚¹æœºåˆ¶

- **è®°å½•æ–¹å¼**: åŸºäºé¡µç ï¼ˆcurrent_pageï¼‰
- **é€‚ç”¨åœºæ™¯**: ä¼ ç»Ÿåˆ†é¡µç½‘ç«™ï¼ˆé¡µç å’Œå†…å®¹é¡ºåºä¸€è‡´ï¼‰
- **å·²é›†æˆ**: BBSSpider âœ…
- **æœªé›†æˆ**: DynamicNewsCrawler âŒ

### sxd.xd.com çš„é—®é¢˜

- **æ–‡ç« æ’åˆ—**: å€’åºï¼ˆç¬¬ä¸€é¡µ article_id æœ€å¤§ï¼‰
- **å½“å‰æœºåˆ¶**: ä¸é€‚ç”¨ï¼ˆåŸºäºé¡µç ä¼šè·³è¿‡å†…å®¹ï¼‰
- **éœ€è¦æ”¹è¿›**: åŸºäº article_id å»é‡

### æ”¹è¿›æ–¹å‘

1. **ä¸º DynamicNewsCrawler æ·»åŠ æ£€æŸ¥ç‚¹æ”¯æŒ** â­
2. **å®ç°åŸºäº article_id çš„å»é‡é€»è¾‘** â­
3. **æ”¯æŒå€’åº/æ­£åºçˆ¬å–æ–¹å‘æ£€æµ‹**

---

## ğŸš€ ä¸‹ä¸€æ­¥

éœ€è¦æˆ‘å®ç°è¿™äº›æ”¹è¿›å—ï¼Ÿæˆ‘å¯ä»¥ï¼š
1. ä¸º `DynamicNewsCrawler` æ·»åŠ æ£€æŸ¥ç‚¹æ”¯æŒ
2. å®ç°åŸºäº `article_id` çš„å»é‡é€»è¾‘
3. æ”¯æŒå€’åºæ’åˆ—çš„ç½‘ç«™
