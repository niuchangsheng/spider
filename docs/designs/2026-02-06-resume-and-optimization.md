# è®¾è®¡æ–‡æ¡£ï¼šæ–­ç‚¹ç»­ä¼ ä¸æ€§èƒ½ä¼˜åŒ–æ¶æ„

**æ–‡æ¡£ç¼–å·**: DESIGN-2026-02-06-003  
**åˆ›å»ºæ—¥æœŸ**: 2026-02-06  
**ä½œè€…**: Chang (æ¶æ„å¸ˆ)  
**çŠ¶æ€**: ğŸ“‹ è®¾è®¡ææ¡ˆ

---

## 1. é—®é¢˜èƒŒæ™¯

é’ˆå¯¹å¤§è§„æ¨¡çˆ¬å–ä»»åŠ¡ï¼ˆå¦‚çˆ¬å– https://sxd.xd.com/ æ‰€æœ‰å¸–å­ï¼‰ï¼Œå­˜åœ¨ä¸‰ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ï¼š

1. **æ–­ç‚¹ç»­ä¼ ** - çˆ¬å–è¿‡ç¨‹ä¸­æ–­åå¦‚ä½•æ¢å¤
2. **æ€§èƒ½ä¼˜åŒ–** - å¦‚ä½•æé«˜çˆ¬å–é€Ÿåº¦
3. **åå°ç¦** - å¦‚ä½•é¿å…è¢«ç½‘ç«™ç®¡ç†å‘˜å°ç¦

---

## 2. é—®é¢˜1ï¼šæ–­ç‚¹ç»­ä¼ æœºåˆ¶

### 2.1 å½“å‰çŠ¶æ€

âœ… **å·²æœ‰åŸºç¡€**ï¼š
- Redis å­˜å‚¨ `visited_urls`ï¼ˆå·²è®¿é—®URLé›†åˆï¼‰
- MongoDB å­˜å‚¨ `threads`ï¼ˆå¸–å­æ•°æ®ï¼‰
- `storage.thread_exists()` æ£€æŸ¥å¸–å­æ˜¯å¦å·²çˆ¬å–

âŒ **ç¼ºå¤±åŠŸèƒ½**ï¼š
- æ²¡æœ‰æ£€æŸ¥ç‚¹ï¼ˆcheckpointï¼‰æœºåˆ¶
- æ²¡æœ‰çˆ¬å–è¿›åº¦æŒä¹…åŒ–
- æ— æ³•ä»æŒ‡å®šä½ç½®æ¢å¤

### 2.2 æ¶æ„è®¾è®¡

#### æ–¹æ¡ˆAï¼šåŸºäºRedisçš„æ£€æŸ¥ç‚¹æœºåˆ¶ï¼ˆæ¨èï¼‰

```python
# Redis æ•°æ®ç»“æ„è®¾è®¡
checkpoint:{site}:{board} = {
    "current_page": 15,           # å½“å‰çˆ¬å–é¡µæ•°
    "last_thread_id": "12345",    # æœ€åçˆ¬å–çš„å¸–å­ID
    "last_crawl_time": "2026-02-06T10:30:00",
    "status": "running",          # running/paused/completed
    "total_pages": 100,           # æ€»é¡µæ•°ï¼ˆå¦‚æœå·²çŸ¥ï¼‰
    "crawled_count": 1500,        # å·²çˆ¬å–å¸–å­æ•°
    "failed_count": 5             # å¤±è´¥æ•°
}

# ä½¿ç”¨ç¤ºä¾‹
checkpoint_key = f"checkpoint:sxd.xd.com:all"
```

**ä¼˜ç‚¹**ï¼š
- è½»é‡çº§ï¼ŒæŸ¥è¯¢å¿«é€Ÿ
- æ”¯æŒå¤šä»»åŠ¡å¹¶è¡Œ
- æ˜“äºå®ç°

**ç¼ºç‚¹**ï¼š
- Redis é‡å¯ä¼šä¸¢å¤±ï¼ˆå¯é…åˆæŒä¹…åŒ–ï¼‰

#### æ–¹æ¡ˆBï¼šåŸºäºMongoDBçš„æ£€æŸ¥ç‚¹æœºåˆ¶

```python
# MongoDB é›†åˆ: checkpoints
{
    "_id": ObjectId,
    "site": "sxd.xd.com",
    "board": "all",
    "current_page": 15,
    "last_thread_id": "12345",
    "last_crawl_time": ISODate,
    "status": "running",
    "metadata": {
        "total_pages": 100,
        "crawled_count": 1500,
        "failed_count": 5
    },
    "created_at": ISODate,
    "updated_at": ISODate
}
```

**ä¼˜ç‚¹**ï¼š
- æŒä¹…åŒ–å­˜å‚¨ï¼Œæ›´å¯é 
- æ”¯æŒå¤æ‚æŸ¥è¯¢å’Œç»Ÿè®¡
- å¯ä»¥è®°å½•å†å²è®°å½•

**ç¼ºç‚¹**ï¼š
- æŸ¥è¯¢é€Ÿåº¦ç•¥æ…¢äºRedis
- éœ€è¦é¢å¤–çš„é›†åˆç®¡ç†

#### æ–¹æ¡ˆCï¼šæ··åˆæ–¹æ¡ˆï¼ˆæœ€ä½³å®è·µï¼‰â­

**è®¾è®¡æ€è·¯**ï¼š
- **Redis**ï¼šå­˜å‚¨å®æ—¶æ£€æŸ¥ç‚¹ï¼ˆå¿«é€Ÿè¯»å†™ï¼‰
- **MongoDB**ï¼šå®šæœŸåŒæ­¥æ£€æŸ¥ç‚¹ï¼ˆæŒä¹…åŒ–å¤‡ä»½ï¼‰
- **æœ¬åœ°æ–‡ä»¶**ï¼šç´§æ€¥å¤‡ä»½ï¼ˆJSONæ ¼å¼ï¼‰

```python
class CheckpointManager:
    """æ£€æŸ¥ç‚¹ç®¡ç†å™¨"""
    
    def __init__(self, site: str, board: str):
        self.site = site
        self.board = board
        self.checkpoint_key = f"checkpoint:{site}:{board}"
        self.backup_file = Path(f"checkpoints/{site}_{board}.json")
    
    async def save_checkpoint(self, page: int, thread_id: str, stats: dict):
        """ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆä¸‰çº§å­˜å‚¨ï¼‰"""
        checkpoint = {
            "current_page": page,
            "last_thread_id": thread_id,
            "last_crawl_time": datetime.now().isoformat(),
            "status": "running",
            **stats
        }
        
        # 1. Redisï¼ˆå®æ—¶ï¼‰
        storage.redis_client.hset(self.checkpoint_key, mapping=checkpoint)
        
        # 2. MongoDBï¼ˆå®šæœŸåŒæ­¥ï¼Œæ¯10ä¸ªæ£€æŸ¥ç‚¹åŒæ­¥ä¸€æ¬¡ï¼‰
        if page % 10 == 0:
            storage.save_checkpoint(checkpoint)
        
        # 3. æœ¬åœ°æ–‡ä»¶ï¼ˆæ¯100ä¸ªæ£€æŸ¥ç‚¹å¤‡ä»½ä¸€æ¬¡ï¼‰
        if page % 100 == 0:
            self._save_to_file(checkpoint)
    
    async def load_checkpoint(self) -> Optional[dict]:
        """åŠ è½½æ£€æŸ¥ç‚¹ï¼ˆä¼˜å…ˆçº§ï¼šRedis > MongoDB > æœ¬åœ°æ–‡ä»¶ï¼‰"""
        # 1. å°è¯•ä»RedisåŠ è½½
        checkpoint = storage.redis_client.hgetall(self.checkpoint_key)
        if checkpoint:
            return checkpoint
        
        # 2. å°è¯•ä»MongoDBåŠ è½½
        checkpoint = storage.load_checkpoint(self.site, self.board)
        if checkpoint:
            # æ¢å¤åˆ°Redis
            storage.redis_client.hset(self.checkpoint_key, mapping=checkpoint)
            return checkpoint
        
        # 3. å°è¯•ä»æœ¬åœ°æ–‡ä»¶åŠ è½½
        if self.backup_file.exists():
            checkpoint = json.loads(self.backup_file.read_text())
            # æ¢å¤åˆ°Rediså’ŒMongoDB
            storage.redis_client.hset(self.checkpoint_key, mapping=checkpoint)
            storage.save_checkpoint(checkpoint)
            return checkpoint
        
        return None
    
    def mark_completed(self):
        """æ ‡è®°ä»»åŠ¡å®Œæˆ"""
        checkpoint = self.load_checkpoint()
        checkpoint['status'] = 'completed'
        checkpoint['completed_at'] = datetime.now().isoformat()
        self.save_checkpoint(**checkpoint)
```

### 2.3 å®ç°æ–¹æ¡ˆ

#### ä¿®æ”¹ `spiders/bbs_spider.py`

```python
class BBSSpider(BaseSpider):
    def __init__(self, ...):
        # ... ç°æœ‰ä»£ç  ...
        self.checkpoint_manager = CheckpointManager(
            site=self.config.bbs.base_url,
            board=board_name
        )
    
    async def crawl_board(self, board_url: str, board_name: str, max_pages: Optional[int] = None):
        """çˆ¬å–æ¿å—ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰"""
        # 1. åŠ è½½æ£€æŸ¥ç‚¹
        checkpoint = await self.checkpoint_manager.load_checkpoint()
        start_page = checkpoint['current_page'] if checkpoint else 1
        
        if checkpoint:
            logger.info(f"ğŸ”„ ä»æ£€æŸ¥ç‚¹æ¢å¤: ç¬¬ {start_page} é¡µ")
        
        # 2. ä»æ£€æŸ¥ç‚¹ä½ç½®ç»§ç»­çˆ¬å–
        page_count = 0
        current_url = board_url
        
        for page in range(start_page, max_pages or float('inf')):
            # ... çˆ¬å–é€»è¾‘ ...
            
            # 3. æ¯çˆ¬å–ä¸€é¡µï¼Œä¿å­˜æ£€æŸ¥ç‚¹
            await self.checkpoint_manager.save_checkpoint(
                page=page,
                thread_id=last_thread_id,
                stats={
                    "crawled_count": self.stats['threads_crawled'],
                    "failed_count": self.stats['images_failed']
                }
            )
            
            # 4. æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ
            next_url = self.parser.find_next_page(html, current_url)
            if not next_url:
                break
            
            current_url = next_url
            page_count += 1
        
        # 5. æ ‡è®°å®Œæˆ
        self.checkpoint_manager.mark_completed()
```

### 2.4 ä½¿ç”¨ç¤ºä¾‹

```bash
# ç¬¬ä¸€æ¬¡è¿è¡Œï¼ˆä»å¤´å¼€å§‹ï¼‰
python spider.py crawl-board "https://sxd.xd.com/" --config xindong

# ä¸­æ–­åï¼Œå†æ¬¡è¿è¡Œï¼ˆè‡ªåŠ¨ä»æ£€æŸ¥ç‚¹æ¢å¤ï¼‰
python spider.py crawl-board "https://sxd.xd.com/" --config xindong
# è¾“å‡º: ğŸ”„ ä»æ£€æŸ¥ç‚¹æ¢å¤: ç¬¬ 15 é¡µ

# æ‰‹åŠ¨æŒ‡å®šèµ·å§‹é¡µï¼ˆè¦†ç›–æ£€æŸ¥ç‚¹ï¼‰
python spider.py crawl-board "https://sxd.xd.com/" --config xindong --start-page 20
```

---

## 3. é—®é¢˜2ï¼šæ€§èƒ½ä¼˜åŒ–

### 3.1 å½“å‰æ€§èƒ½ç“¶é¢ˆ

| ç“¶é¢ˆ | å½“å‰å€¼ | ä¼˜åŒ–ç›®æ ‡ |
|------|--------|---------|
| å¹¶å‘æ•° | 5 | 10-20 |
| è¯·æ±‚å»¶è¿Ÿ | 1.0ç§’ | 0.3-0.5ç§’ï¼ˆæ™ºèƒ½å»¶è¿Ÿï¼‰ |
| å›¾ç‰‡ä¸‹è½½ | ä¸²è¡Œ | æ‰¹é‡å¹¶å‘ |
| æ•°æ®åº“å†™å…¥ | é€æ¡ | æ‰¹é‡å†™å…¥ |

### 3.2 ä¼˜åŒ–ç­–ç•¥

#### ç­–ç•¥1ï¼šæ™ºèƒ½å¹¶å‘æ§åˆ¶

```python
class AdaptiveConcurrencyController:
    """è‡ªé€‚åº”å¹¶å‘æ§åˆ¶å™¨"""
    
    def __init__(self, initial_concurrent=5, max_concurrent=20):
        self.current_concurrent = initial_concurrent
        self.max_concurrent = max_concurrent
        self.min_concurrent = 1
        self.error_rate = 0.0
        self.success_count = 0
        self.error_count = 0
    
    def adjust_concurrency(self):
        """æ ¹æ®é”™è¯¯ç‡è°ƒæ•´å¹¶å‘æ•°"""
        total = self.success_count + self.error_count
        if total == 0:
            return
        
        self.error_rate = self.error_count / total
        
        if self.error_rate > 0.1:  # é”™è¯¯ç‡>10%ï¼Œé™ä½å¹¶å‘
            self.current_concurrent = max(
                self.min_concurrent,
                int(self.current_concurrent * 0.8)
            )
        elif self.error_rate < 0.01:  # é”™è¯¯ç‡<1%ï¼Œæé«˜å¹¶å‘
            self.current_concurrent = min(
                self.max_concurrent,
                int(self.current_concurrent * 1.2)
            )
        
        logger.info(f"ğŸ“Š å¹¶å‘æ•°è°ƒæ•´: {self.current_concurrent} (é”™è¯¯ç‡: {self.error_rate:.2%})")
```

#### ç­–ç•¥2ï¼šæ‰¹é‡æ•°æ®åº“å†™å…¥

```python
class BatchStorage:
    """æ‰¹é‡å­˜å‚¨ç®¡ç†å™¨"""
    
    def __init__(self, batch_size=100):
        self.batch_size = batch_size
        self.thread_buffer = []
        self.image_buffer = []
    
    async def save_thread_batch(self, thread_data: dict):
        """æ‰¹é‡ä¿å­˜å¸–å­"""
        self.thread_buffer.append(thread_data)
        
        if len(self.thread_buffer) >= self.batch_size:
            await self._flush_threads()
    
    async def _flush_threads(self):
        """åˆ·æ–°å¸–å­ç¼“å†²åŒº"""
        if not self.thread_buffer:
            return
        
        try:
            collection = storage.mongo_db['threads']
            # æ‰¹é‡æ’å…¥ï¼ˆä½¿ç”¨ insert_many + upsertï¼‰
            operations = [
                UpdateOne(
                    {'thread_id': t['thread_id']},
                    {'$set': t},
                    upsert=True
                )
                for t in self.thread_buffer
            ]
            collection.bulk_write(operations)
            
            logger.info(f"ğŸ’¾ æ‰¹é‡ä¿å­˜ {len(self.thread_buffer)} æ¡å¸–å­")
            self.thread_buffer.clear()
        except Exception as e:
            logger.error(f"æ‰¹é‡ä¿å­˜å¤±è´¥: {e}")
    
    async def flush_all(self):
        """åˆ·æ–°æ‰€æœ‰ç¼“å†²åŒº"""
        await self._flush_threads()
        await self._flush_images()
```

#### ç­–ç•¥3ï¼šè¿æ¥æ± ä¼˜åŒ–

```python
# åœ¨ BaseSpider ä¸­ä¼˜åŒ– HTTP è¿æ¥
class BaseSpider:
    def __init__(self, config):
        # ä½¿ç”¨è¿æ¥æ± ï¼Œå¤ç”¨è¿æ¥
        self.session = aiohttp.ClientSession(
            connector=aiohttp.TCPConnector(
                limit=100,  # è¿æ¥æ± å¤§å°
                limit_per_host=20,  # æ¯ä¸ªä¸»æœºæœ€å¤§è¿æ¥æ•°
                ttl_dns_cache=300,  # DNSç¼“å­˜æ—¶é—´
                force_close=False,  # ä¿æŒè¿æ¥
            ),
            timeout=aiohttp.ClientTimeout(total=30),
            headers={'User-Agent': self._get_user_agent()}
        )
```

#### ç­–ç•¥4ï¼šå¼‚æ­¥ä»»åŠ¡é˜Ÿåˆ—

```python
# ä½¿ç”¨ asyncio.Queue å®ç°ç”Ÿäº§è€…-æ¶ˆè´¹è€…æ¨¡å¼
class CrawlQueue:
    """çˆ¬å–ä»»åŠ¡é˜Ÿåˆ—"""
    
    def __init__(self, max_workers=10):
        self.queue = asyncio.Queue(maxsize=1000)
        self.max_workers = max_workers
    
    async def producer(self, urls: List[str]):
        """ç”Ÿäº§è€…ï¼šæ·»åŠ URLåˆ°é˜Ÿåˆ—"""
        for url in urls:
            await self.queue.put(url)
    
    async def consumer(self, spider: BBSSpider):
        """æ¶ˆè´¹è€…ï¼šä»é˜Ÿåˆ—å–URLå¹¶çˆ¬å–"""
        while True:
            try:
                url = await asyncio.wait_for(self.queue.get(), timeout=1.0)
                await spider.crawl_thread(url)
                self.queue.task_done()
            except asyncio.TimeoutError:
                break
    
    async def run(self, urls: List[str], spider: BBSSpider):
        """è¿è¡Œçˆ¬å–ä»»åŠ¡"""
        # å¯åŠ¨ç”Ÿäº§è€…
        producer_task = asyncio.create_task(self.producer(urls))
        
        # å¯åŠ¨å¤šä¸ªæ¶ˆè´¹è€…ï¼ˆå¹¶å‘ï¼‰
        consumer_tasks = [
            asyncio.create_task(self.consumer(spider))
            for _ in range(self.max_workers)
        ]
        
        await producer_task
        await self.queue.join()  # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        
        # å–æ¶ˆæ¶ˆè´¹è€…
        for task in consumer_tasks:
            task.cancel()
```

### 3.3 æ€§èƒ½ä¼˜åŒ–æ•ˆæœé¢„ä¼°

| ä¼˜åŒ–é¡¹ | ä¼˜åŒ–å‰ | ä¼˜åŒ–å | æå‡ |
|--------|--------|--------|------|
| å¹¶å‘æ•° | 5 | 15 | 3x |
| è¯·æ±‚å»¶è¿Ÿ | 1.0s | 0.4s | 2.5x |
| æ•°æ®åº“å†™å…¥ | é€æ¡ | æ‰¹é‡100æ¡ | 10x |
| **æ€»ä½“é€Ÿåº¦** | **150å›¾/åˆ†** | **500+å›¾/åˆ†** | **3.3x** |

---

## 4. é—®é¢˜3ï¼šåå°ç¦ç­–ç•¥

### 4.1 å°ç¦é£é™©åˆ†æ

| é£é™© | è§¦å‘æ¡ä»¶ | å½±å“ |
|------|----------|------|
| IPå°ç¦ | è¯·æ±‚é¢‘ç‡è¿‡é«˜ | ğŸ”´ ä¸¥é‡ |
| è´¦å·å°ç¦ | å¼‚å¸¸è¡Œä¸ºæ¨¡å¼ | ğŸ”´ ä¸¥é‡ |
| éªŒè¯ç  | é¢‘ç¹è¯·æ±‚ | ğŸŸ¡ ä¸­ç­‰ |
| é™æµ | è¶…è¿‡QPSé™åˆ¶ | ğŸŸ¡ ä¸­ç­‰ |

### 4.2 åå°ç¦æ¶æ„è®¾è®¡

#### ç­–ç•¥1ï¼šæ™ºèƒ½å»¶è¿Ÿï¼ˆHuman-like Behaviorï¼‰

```python
class HumanLikeDelay:
    """äººç±»è¡Œä¸ºæ¨¡æ‹Ÿå»¶è¿Ÿ"""
    
    def __init__(self, base_delay=1.0):
        self.base_delay = base_delay
        self.last_request_time = 0
    
    async def wait(self):
        """æ™ºèƒ½å»¶è¿Ÿï¼ˆæ¨¡æ‹Ÿäººç±»è¡Œä¸ºï¼‰"""
        # åŸºç¡€å»¶è¿Ÿ + éšæœºæŠ–åŠ¨ï¼ˆÂ±30%ï¼‰
        delay = self.base_delay * (1 + random.uniform(-0.3, 0.3))
        
        # è€ƒè™‘æ—¶é—´é—´éš”ï¼ˆå¦‚æœè·ç¦»ä¸Šæ¬¡è¯·æ±‚>5ç§’ï¼Œå‡å°‘å»¶è¿Ÿï¼‰
        time_since_last = time.time() - self.last_request_time
        if time_since_last > 5:
            delay *= 0.5
        
        # å¤œé—´é™ä½å»¶è¿Ÿï¼ˆå‡è®¾æœåŠ¡å™¨å‹åŠ›å°ï¼‰
        hour = datetime.now().hour
        if 2 <= hour <= 6:
            delay *= 0.7
        
        await asyncio.sleep(delay)
        self.last_request_time = time.time()
```

#### ç­–ç•¥2ï¼šUser-Agentè½®æ¢

```python
class UserAgentRotator:
    """User-Agentè½®æ¢å™¨"""
    
    def __init__(self):
        self.ua_generator = UserAgent()
        self.ua_list = [
            # Chrome
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
            # Firefox
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101',
            # Safari
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) Version/17.0 Safari/605.1.15',
        ]
        self.current_index = 0
    
    def get_ua(self) -> str:
        """è·å–ä¸‹ä¸€ä¸ªUA"""
        ua = self.ua_list[self.current_index]
        self.current_index = (self.current_index + 1) % len(self.ua_list)
        return ua
    
    def get_random_ua(self) -> str:
        """è·å–éšæœºUA"""
        return random.choice(self.ua_list)
```

#### ç­–ç•¥3ï¼šè¯·æ±‚é¢‘ç‡æ§åˆ¶

```python
class RateLimiter:
    """è¯·æ±‚é¢‘ç‡é™åˆ¶å™¨"""
    
    def __init__(self, max_requests_per_minute=60):
        self.max_rpm = max_requests_per_minute
        self.request_times = deque(maxlen=max_requests_per_minute)
    
    async def acquire(self):
        """è·å–è¯·æ±‚è®¸å¯"""
        now = time.time()
        
        # ç§»é™¤1åˆ†é’Ÿå‰çš„è®°å½•
        while self.request_times and now - self.request_times[0] > 60:
            self.request_times.popleft()
        
        # å¦‚æœè¶…è¿‡é™åˆ¶ï¼Œç­‰å¾…
        if len(self.request_times) >= self.max_rpm:
            wait_time = 60 - (now - self.request_times[0])
            logger.warning(f"â³ é¢‘ç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time:.1f} ç§’")
            await asyncio.sleep(wait_time)
            return await self.acquire()
        
        self.request_times.append(now)
```

#### ç­–ç•¥4ï¼šä»£ç†æ± ï¼ˆå¯é€‰ï¼‰

```python
class ProxyPool:
    """ä»£ç†æ± ç®¡ç†å™¨"""
    
    def __init__(self, proxy_list: List[str]):
        self.proxies = proxy_list
        self.current_index = 0
        self.failed_proxies = set()
    
    def get_proxy(self) -> Optional[str]:
        """è·å–å¯ç”¨ä»£ç†"""
        if not self.proxies:
            return None
        
        # è½®è¯¢ä»£ç†
        for _ in range(len(self.proxies)):
            proxy = self.proxies[self.current_index]
            self.current_index = (self.current_index + 1) % len(self.proxies)
            
            if proxy not in self.failed_proxies:
                return proxy
        
        # æ‰€æœ‰ä»£ç†éƒ½å¤±è´¥ï¼Œé‡ç½®
        logger.warning("âš ï¸  æ‰€æœ‰ä»£ç†éƒ½å¤±è´¥ï¼Œé‡ç½®ä»£ç†æ± ")
        self.failed_proxies.clear()
        return self.proxies[0]
    
    def mark_failed(self, proxy: str):
        """æ ‡è®°ä»£ç†å¤±è´¥"""
        self.failed_proxies.add(proxy)
        logger.warning(f"âŒ ä»£ç†å¤±è´¥: {proxy}")
```

#### ç­–ç•¥5ï¼šé”™è¯¯å¤„ç†ä¸é™çº§

```python
class AntiBanManager:
    """åå°ç¦ç®¡ç†å™¨"""
    
    def __init__(self):
        self.consecutive_errors = 0
        self.last_error_time = 0
    
    async def handle_error(self, error: Exception):
        """å¤„ç†é”™è¯¯ï¼Œè‡ªåŠ¨é™çº§"""
        self.consecutive_errors += 1
        self.last_error_time = time.time()
        
        # è¿ç»­é”™è¯¯>5æ¬¡ï¼Œè§¦å‘é™çº§
        if self.consecutive_errors > 5:
            logger.error("ğŸš¨ è¿ç»­é”™è¯¯è¿‡å¤šï¼Œè§¦å‘é™çº§ç­–ç•¥")
            await self._degrade()
    
    async def _degrade(self):
        """é™çº§ç­–ç•¥"""
        # 1. å¢åŠ å»¶è¿Ÿ
        config.crawler.download_delay *= 2
        
        # 2. å‡å°‘å¹¶å‘
        config.crawler.max_concurrent_requests = max(1, config.crawler.max_concurrent_requests // 2)
        
        # 3. ç­‰å¾…ä¸€æ®µæ—¶é—´
        wait_time = min(300, self.consecutive_errors * 60)  # æœ€å¤šç­‰å¾…5åˆ†é’Ÿ
        logger.info(f"â¸ï¸  é™çº§ç­‰å¾… {wait_time} ç§’")
        await asyncio.sleep(wait_time)
        
        # 4. é‡ç½®é”™è¯¯è®¡æ•°
        self.consecutive_errors = 0
```

### 4.3 åå°ç¦é…ç½®å»ºè®®

é’ˆå¯¹ https://sxd.xd.com/ çš„æ¨èé…ç½®ï¼š

```python
# configs/sxd.json
{
    "crawler": {
        "max_concurrent_requests": 3,      # ä¿å®ˆå¹¶å‘æ•°
        "download_delay": 2.0,             # 2ç§’å»¶è¿Ÿï¼ˆå®‰å…¨ï¼‰
        "request_timeout": 30,
        "max_retries": 3,
        "rotate_user_agent": true,
        "use_proxy": false                 # å¦‚æœè¢«å°ï¼Œå¯ç”¨ä»£ç†
    },
    "anti_ban": {
        "max_requests_per_minute": 30,     # æ¯åˆ†é’Ÿæœ€å¤š30è¯·æ±‚
        "human_like_delay": true,          # å¯ç”¨äººç±»è¡Œä¸ºæ¨¡æ‹Ÿ
        "error_threshold": 5,              # é”™è¯¯é˜ˆå€¼
        "degrade_on_error": true           # é”™è¯¯æ—¶è‡ªåŠ¨é™çº§
    }
}
```

---

## 5. ç»¼åˆå®æ–½æ–¹æ¡ˆ

### 5.1 ä¼˜å…ˆçº§æ’åº

| ä¼˜å…ˆçº§ | åŠŸèƒ½ | é¢„è®¡å·¥ä½œé‡ | å½±å“ |
|--------|------|-----------|------|
| P0 | æ–­ç‚¹ç»­ä¼ ï¼ˆæ£€æŸ¥ç‚¹ï¼‰ | 2å¤© | ğŸ”´ å…³é”® |
| P0 | æ‰¹é‡æ•°æ®åº“å†™å…¥ | 1å¤© | ğŸ”´ å…³é”® |
| P1 | æ™ºèƒ½å»¶è¿Ÿ | 1å¤© | ğŸŸ¡ é‡è¦ |
| P1 | è‡ªé€‚åº”å¹¶å‘ | 2å¤© | ğŸŸ¡ é‡è¦ |
| P2 | ä»£ç†æ±  | 3å¤© | ğŸŸ¢ å¯é€‰ |
| P2 | é”™è¯¯é™çº§ | 1å¤© | ğŸŸ¢ å¯é€‰ |

### 5.2 å®æ–½æ­¥éª¤

**é˜¶æ®µ1ï¼šæ–­ç‚¹ç»­ä¼ ï¼ˆ1å‘¨ï¼‰**
1. å®ç° `CheckpointManager` ç±»
2. ä¿®æ”¹ `crawl_board()` æ”¯æŒæ£€æŸ¥ç‚¹
3. æ·»åŠ  CLI å‚æ•° `--resume` / `--start-page`
4. æµ‹è¯•éªŒè¯

**é˜¶æ®µ2ï¼šæ€§èƒ½ä¼˜åŒ–ï¼ˆ1å‘¨ï¼‰**
1. å®ç°æ‰¹é‡æ•°æ®åº“å†™å…¥
2. ä¼˜åŒ–è¿æ¥æ± é…ç½®
3. å®ç°è‡ªé€‚åº”å¹¶å‘æ§åˆ¶
4. æ€§èƒ½æµ‹è¯•

**é˜¶æ®µ3ï¼šåå°ç¦ï¼ˆ1å‘¨ï¼‰**
1. å®ç°æ™ºèƒ½å»¶è¿Ÿ
2. å®ç°é¢‘ç‡é™åˆ¶å™¨
3. å®ç°é”™è¯¯é™çº§
4. å‹åŠ›æµ‹è¯•

### 5.3 ä½¿ç”¨ç¤ºä¾‹

```bash
# 1. é¦–æ¬¡è¿è¡Œï¼ˆè‡ªåŠ¨ä¿å­˜æ£€æŸ¥ç‚¹ï¼‰
python spider.py crawl-board "https://sxd.xd.com/" --config sxd

# 2. ä¸­æ–­åæ¢å¤ï¼ˆè‡ªåŠ¨ä»æ£€æŸ¥ç‚¹ç»§ç»­ï¼‰
python spider.py crawl-board "https://sxd.xd.com/" --config sxd --resume

# 3. æŒ‡å®šèµ·å§‹é¡µï¼ˆè¦†ç›–æ£€æŸ¥ç‚¹ï¼‰
python spider.py crawl-board "https://sxd.xd.com/" --config sxd --start-page 50

# 4. æŸ¥çœ‹æ£€æŸ¥ç‚¹çŠ¶æ€
python spider.py checkpoint-status --site sxd.xd.com
```

---

## 6. é£é™©è¯„ä¼°

| é£é™© | æ¦‚ç‡ | å½±å“ | åº”å¯¹æªæ–½ |
|------|------|------|---------|
| æ£€æŸ¥ç‚¹ä¸¢å¤± | ä½ | ä¸­ | ä¸‰çº§å­˜å‚¨ï¼ˆRedis+MongoDB+æ–‡ä»¶ï¼‰ |
| æ€§èƒ½ä¼˜åŒ–è¿‡åº¦ | ä¸­ | é«˜ | è‡ªé€‚åº”å¹¶å‘ï¼Œè‡ªåŠ¨é™çº§ |
| è¢«å°ç¦ | ä¸­ | é«˜ | æ™ºèƒ½å»¶è¿Ÿï¼Œé¢‘ç‡é™åˆ¶ï¼Œä»£ç†æ±  |
| æ•°æ®ä¸ä¸€è‡´ | ä½ | ä¸­ | äº‹åŠ¡å¤„ç†ï¼Œå¹‚ç­‰æ€§è®¾è®¡ |

---

## 7. å®¡æ‰¹

| è§’è‰² | å§“å | æ„è§ | æ—¥æœŸ |
|------|------|------|------|
| æ¶æ„å¸ˆ | Chang | å¾…å®¡æ‰¹ | - |

---

## 8. å‚è€ƒèµ„æ–™

- [Redis æŒä¹…åŒ–](https://redis.io/docs/management/persistence/)
- [MongoDB æ‰¹é‡æ“ä½œ](https://www.mongodb.com/docs/manual/core/bulk-write-operations/)
- [aiohttp è¿æ¥æ± ](https://docs.aiohttp.org/en/stable/client_reference.html#aiohttp.ClientSession)
