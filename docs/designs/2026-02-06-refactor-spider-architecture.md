# è®¾è®¡æ–‡æ¡£ï¼šçˆ¬è™«æ¶æ„é‡æ„ - ç»Ÿä¸€ç»§æ‰¿ä½“ç³»

**æ–‡æ¡£ç¼–å·**: DESIGN-2026-02-06-001  
**åˆ›å»ºæ—¥æœŸ**: 2026-02-06  
**ä½œè€…**: Chang (æ¶æ„å¸ˆ)  
**çŠ¶æ€**: âœ… å·²å®ç°

---

## 1. èƒŒæ™¯ä¸é—®é¢˜

### 1.1 å½“å‰æ¶æ„é—®é¢˜

åœ¨å®ç°åŠ¨æ€æ–°é—»é¡µé¢çˆ¬è™«ï¼ˆ`DynamicNewsCrawler`ï¼‰åï¼Œå‘ç°å½“å‰æ¶æ„å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        å½“å‰ç±»å…³ç³»å›¾                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Parser å±‚:                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                               â”‚
â”‚  â”‚  BBSParser   â”‚ â—„â”€â”€â”€ DynamicPageParser (ç»§æ‰¿)                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      âŒ è¯­ä¹‰ä¸å¯¹ï¼Dynamicä¸æ˜¯ä¸€ç§BBS          â”‚
â”‚                                                                 â”‚
â”‚  Crawler å±‚:                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                               â”‚
â”‚  â”‚  BBSSpider   â”‚ â—„â”€â”€â”€ DiscuzSpider (ç»§æ‰¿)                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â—„â”€â”€â”€ PhpBBSpider (ç»§æ‰¿)                       â”‚
â”‚                   â—„â”€â”€â”€ VBulletinSpider (ç»§æ‰¿)                   â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                         â”‚
â”‚  â”‚ DynamicNewsCrawler â”‚  âŒ ç‹¬ç«‹å­˜åœ¨ï¼Œæœªçº³å…¥ç»§æ‰¿ä½“ç³»            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                         â”‚
â”‚                                                                 â”‚
â”‚  Factory å±‚:                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                             â”‚
â”‚  â”‚ SpiderFactory  â”‚ â”€â”€â”€ åªç®¡ç† BBSSpider åŠå…¶å­ç±»               â”‚
â”‚  â”‚  _registry:    â”‚     âŒ ä¸ç®¡ç† DynamicNewsCrawler            â”‚
â”‚  â”‚  - discuz      â”‚ â†’ DiscuzSpider                              â”‚
â”‚  â”‚  - phpbb       â”‚ â†’ PhpBBSpider                               â”‚
â”‚  â”‚  - vbulletin   â”‚ â†’ VBulletinSpider                           â”‚
â”‚  â”‚  - generic     â”‚ â†’ BBSSpider  âŒ ä¸å­ç±»å¹¶åˆ—ï¼Œå±‚çº§æ··ä¹±        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                             â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| é—®é¢˜ | æè¿° | å½±å“ |
|------|------|------|
| **Parserç»§æ‰¿è¯­ä¹‰é”™è¯¯** | `DynamicPageParser` ç»§æ‰¿ `BBSParser`ï¼Œä½†åŠ¨æ€é¡µé¢ä¸æ˜¯BBS | æ¦‚å¿µæ··æ·† |
| **ç¼ºå°‘ParseråŸºç±»** | `BBSParser` å’Œ `DynamicPageParser` åº”è¯¥å…±äº«åŸºç±» | ä»£ç é‡å¤ |
| **Crawlerç»§æ‰¿ä¸ä¸€è‡´** | `DynamicNewsCrawler` ä¸ç»§æ‰¿ `BBSSpider` | ä»£ç ç»“æ„æ··ä¹± |
| **å·¥å‚æ¨¡å¼ä¸å®Œæ•´** | `SpiderFactory` åªç®¡ç† BBS çˆ¬è™« | æ— æ³•ç»Ÿä¸€åˆ›å»ºçˆ¬è™« |
| **ä»£ç é‡å¤** | `DynamicNewsCrawler` æœ‰ç‹¬ç«‹çš„ `fetch_page`ã€`stats`ã€`session` | ç»´æŠ¤æˆæœ¬é«˜ |
| **Factoryå±‚çº§æ··ä¹±** | `genericâ†’BBSSpider` ä¸ `discuzâ†’DiscuzSpider` å¹¶åˆ—ï¼Œä½†å®é™…æ˜¯çˆ¶å­å…³ç³» | éš¾ä»¥ç†è§£ |

### 1.2 é‡å¤ä»£ç åˆ†æ

| åŠŸèƒ½ | BBSSpider | DynamicNewsCrawler | é‡å¤ï¼Ÿ |
|------|-----------|-------------------|--------|
| HTTP Session ç®¡ç† | âœ… `self.session` | âœ… `self.session` | ğŸ”´ é‡å¤ |
| é¡µé¢è·å– | âœ… `fetch_page()` | âœ… `fetch_page()` | ğŸ”´ é‡å¤ |
| ç»Ÿè®¡ä¿¡æ¯ | âœ… `self.stats` | âœ… `self.stats` | ğŸ”´ é‡å¤ |
| å¼‚æ­¥ä¸Šä¸‹æ–‡ | âœ… `__aenter__/__aexit__` | âœ… `__aenter__/__aexit__` | ğŸ”´ é‡å¤ |
| è¯·æ±‚å¤´ç®¡ç† | âœ… `get_headers()` | âœ… `get_headers()` | ğŸ”´ é‡å¤ |
| é…ç½®ç®¡ç† | âœ… `self.config` | âœ… `self.config` | ğŸ”´ é‡å¤ |

---

## 2. è®¾è®¡ç›®æ ‡

1. **ç»Ÿä¸€ç»§æ‰¿ä½“ç³»** - æ‰€æœ‰çˆ¬è™«ç±»å…±äº«åŸºç±»
2. **æ¶ˆé™¤ä»£ç é‡å¤** - å…¬å…±åŠŸèƒ½æŠ½å–åˆ°åŸºç±»
3. **å·¥å‚æ¨¡å¼å®Œæ•´** - `SpiderFactory` ç®¡ç†æ‰€æœ‰çˆ¬è™«ç±»å‹
4. **æ˜“äºæ‰©å±•** - æ–°å¢çˆ¬è™«ç±»å‹åªéœ€ç»§æ‰¿å’Œæ³¨å†Œ
5. **å‘åå…¼å®¹** - ç°æœ‰ API ä¿æŒä¸å˜

---

## 3. æ¶æ„è®¾è®¡

### 3.1 æ¨èæ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        æ¨èç±»å…³ç³»å›¾                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Parser å±‚ (é‡æ„):                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
â”‚  â”‚   BaseParser     â”‚  â—„â”€â”€ æ–°å¢ï¼æŠ½å–å…¬å…±è§£æåŠŸèƒ½               â”‚
â”‚  â”‚  - config        â”‚      (HTMLè§£æã€URLå¤„ç†ã€å›¾ç‰‡æå–)        â”‚
â”‚  â”‚  - parse_images()â”‚                                           â”‚
â”‚  â”‚  - extract_id()  â”‚                                           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
â”‚           â”‚                                                     â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                  â”‚
â”‚    â–¼                         â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚  â”‚  BBSParser   â”‚    â”‚ DynamicPageParser  â”‚                     â”‚
â”‚  â”‚  (è®ºå›è§£æ)   â”‚    â”‚ (åŠ¨æ€é¡µé¢è§£æ)      â”‚                     â”‚
â”‚  â”‚  - thread_*  â”‚    â”‚ - article_*        â”‚                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                                                                 â”‚
â”‚  Crawler å±‚ (é‡æ„):                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
â”‚  â”‚   BaseSpider     â”‚  â—„â”€â”€ æ–°å¢ï¼æŠ½å–å…¬å…±åŠŸèƒ½                   â”‚
â”‚  â”‚  - config        â”‚      (HTTPè¯·æ±‚ã€ç»Ÿè®¡ã€sessionç®¡ç†)        â”‚
â”‚  â”‚  - session       â”‚                                           â”‚
â”‚  â”‚  - stats         â”‚                                           â”‚
â”‚  â”‚  - fetch_page()  â”‚                                           â”‚
â”‚  â”‚  - get_headers() â”‚                                           â”‚
â”‚  â”‚  - __aenter__()  â”‚                                           â”‚
â”‚  â”‚  - __aexit__()   â”‚                                           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
â”‚           â”‚                                                     â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                  â”‚
â”‚    â–¼                         â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚  â”‚  BBSSpider   â”‚    â”‚ DynamicNewsCrawler â”‚                     â”‚
â”‚  â”‚  (è®ºå›çˆ¬è™«)   â”‚    â”‚ (åŠ¨æ€é¡µé¢çˆ¬è™«)      â”‚                     â”‚
â”‚  â”‚  - parser    â”‚    â”‚ - parser           â”‚                     â”‚
â”‚  â”‚  - crawl_*   â”‚    â”‚ - crawl_*          â”‚                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚         â”‚                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚
â”‚  â–¼      â–¼              â–¼                                        â”‚
â”‚ Discuz PhpBB      VBulletin                                     â”‚
â”‚ Spider Spider      Spider                                       â”‚
â”‚                                                                 â”‚
â”‚  Factory å±‚ (æ‰©å±•):                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ SpiderFactory  â”€â”€â”€ ç»Ÿä¸€ç®¡ç†æ‰€æœ‰çˆ¬è™«ç±»å‹             â”‚         â”‚
â”‚  â”‚                                                    â”‚         â”‚
â”‚  â”‚  _registry (ç±»å‹ â†’ ç±»):                            â”‚         â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚         â”‚
â”‚  â”‚  â”‚ BBSç±»å‹:                                     â”‚  â”‚         â”‚
â”‚  â”‚  â”‚   'generic'   â†’ BBSSpider (é€šç”¨BBS)         â”‚  â”‚         â”‚
â”‚  â”‚  â”‚   'discuz'    â†’ DiscuzSpider (ç»§æ‰¿BBSSpider)â”‚  â”‚         â”‚
â”‚  â”‚  â”‚   'phpbb'     â†’ PhpBBSpider (ç»§æ‰¿BBSSpider) â”‚  â”‚         â”‚
â”‚  â”‚  â”‚   'vbulletin' â†’ VBulletinSpider (ç»§æ‰¿BBS)   â”‚  â”‚         â”‚
â”‚  â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚         â”‚
â”‚  â”‚  â”‚ åŠ¨æ€é¡µé¢ç±»å‹:                                â”‚  â”‚         â”‚
â”‚  â”‚  â”‚   'dynamic'   â†’ DynamicNewsCrawler  ğŸ†•      â”‚  â”‚         â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 ç»§æ‰¿å…³ç³»è¯´æ˜

```
ç»§æ‰¿å±‚çº§å›¾:

BaseParser (æŠ½è±¡åŸºç±»)
â”œâ”€â”€ BBSParser (è®ºå›è§£æå™¨)
â””â”€â”€ DynamicPageParser (åŠ¨æ€é¡µé¢è§£æå™¨)

BaseSpider (æŠ½è±¡åŸºç±»)
â”œâ”€â”€ BBSSpider (é€šç”¨BBSçˆ¬è™«)
â”‚   â”œâ”€â”€ DiscuzSpider (Discuzä¸“ç”¨)
â”‚   â”œâ”€â”€ PhpBBSpider (phpBBä¸“ç”¨)
â”‚   â””â”€â”€ VBulletinSpider (vBulletinä¸“ç”¨)
â””â”€â”€ DynamicNewsCrawler (åŠ¨æ€é¡µé¢çˆ¬è™«)

æ³¨æ„:
- BBSSpider æ˜¯ DiscuzSpider ç­‰çš„çˆ¶ç±»ï¼Œä¸æ˜¯å¹¶åˆ—å…³ç³»
- 'generic' ç±»å‹ä½¿ç”¨ BBSSpiderï¼Œé€‚ç”¨äºæœªçŸ¥è®ºå›ç±»å‹
- 'discuz' ç­‰ç±»å‹ä½¿ç”¨ä¸“ç”¨å­ç±»ï¼Œæœ‰ç‰¹å®šå¤„ç†é€»è¾‘
```

### 3.3 ç±»è®¾è®¡

#### 3.3.1 BaseParser (æ–°å¢)

```python
class BaseParser(ABC):
    """
    è§£æå™¨åŸºç±»
    
    æ‰€æœ‰è§£æå™¨çš„å…¬å…±åŸºç±»ï¼Œæä¾›ï¼š
    - åŸºç¡€HTMLè§£æ
    - URLå¤„ç†
    - å›¾ç‰‡æå–
    - IDæå–
    """
    
    def __init__(self, config: Optional[Config] = None):
        self.config = config
    
    def _extract_id(self, url: str, patterns: List[str]) -> str:
        """
        ä»URLä¸­æå–ID
        
        Args:
            url: é¡µé¢URL
            patterns: æ­£åˆ™è¡¨è¾¾å¼åˆ—è¡¨
        
        Returns:
            æå–çš„IDï¼Œå¤±è´¥è¿”å›MD5å“ˆå¸Œ
        """
        import re
        import hashlib
        
        for pattern in patterns:
            match = re.search(pattern, url)
            if match:
                return match.group(1)
        
        # å›é€€ï¼šä½¿ç”¨URLçš„MD5
        return hashlib.md5(url.encode()).hexdigest()[:16]
    
    def _extract_images(self, soup: BeautifulSoup, selectors: List[str], base_url: str) -> List[str]:
        """
        ä»HTMLä¸­æå–å›¾ç‰‡URL
        
        Args:
            soup: BeautifulSoupå¯¹è±¡
            selectors: CSSé€‰æ‹©å™¨åˆ—è¡¨
            base_url: åŸºç¡€URLï¼ˆç”¨äºå¤„ç†ç›¸å¯¹è·¯å¾„ï¼‰
        
        Returns:
            å›¾ç‰‡URLåˆ—è¡¨
        """
        images = []
        for selector in selectors:
            for img in soup.select(selector):
                src = self._get_image_url(img)
                if src:
                    if not src.startswith('http'):
                        src = urljoin(base_url, src)
                    if src not in images:
                        images.append(src)
        return images
    
    def _get_image_url(self, img_tag) -> Optional[str]:
        """
        ä»imgæ ‡ç­¾è·å–æœ€ä½³å›¾ç‰‡URL
        
        ä¼˜å…ˆçº§: srcset(æœ€å¤§) > data-src > src
        """
        # å­ç±»å¯é‡å†™æ­¤æ–¹æ³•
        return img_tag.get('src')
    
    @abstractmethod
    def parse(self, html: str, url: str) -> Dict[str, Any]:
        """è§£æé¡µé¢ï¼ˆå­ç±»å®ç°ï¼‰"""
        pass
```

#### 3.3.2 BBSParser (ä¿®æ”¹)

```python
class BBSParser(BaseParser):
    """
    BBSè®ºå›è§£æå™¨
    
    ç»§æ‰¿ BaseParserï¼Œæ·»åŠ è®ºå›ç‰¹æœ‰åŠŸèƒ½ï¼š
    - å¸–å­åˆ—è¡¨è§£æ
    - å¸–å­è¯¦æƒ…è§£æ
    - åˆ†é¡µæ£€æµ‹
    """
    
    def __init__(self, config: Optional[Config] = None):
        super().__init__(config)
        # ä½¿ç”¨å…¨å±€configæˆ–ä¼ å…¥çš„config
        self.bbs_config = (config or global_config).bbs
    
    def parse_thread_list(self, html: str, base_url: str) -> List[Dict]:
        """è§£æå¸–å­åˆ—è¡¨"""
        # ä½¿ç”¨åŸºç±»çš„ _extract_images ç­‰æ–¹æ³•
        ...
    
    def parse_thread_page(self, html: str, thread_url: str) -> Dict:
        """è§£æå¸–å­è¯¦æƒ…é¡µ"""
        ...
    
    def _extract_thread_id(self, url: str) -> str:
        """æå–å¸–å­ID"""
        patterns = [
            r'tid[=_](\d+)',
            r'thread[/-](\d+)',
            r'/(\d+)\.html?$',
        ]
        return self._extract_id(url, patterns)
```

#### 3.3.3 DynamicPageParser (ä¿®æ”¹)

```python
class DynamicPageParser(BaseParser):
    """
    åŠ¨æ€é¡µé¢è§£æå™¨
    
    ç»§æ‰¿ BaseParserï¼Œæ·»åŠ åŠ¨æ€é¡µé¢ç‰¹æœ‰åŠŸèƒ½ï¼š
    - æ–‡ç« åˆ—è¡¨è§£æ
    - æ–‡ç« è¯¦æƒ…è§£æ
    - Ajaxåˆ†é¡µæ£€æµ‹
    - åŸå›¾URLæå–
    """
    
    def __init__(self, config: Config):
        super().__init__(config)
        # åŠ¨æ€é¡µé¢ç‰¹æœ‰é…ç½®
        self.article_selector = getattr(config.bbs, 'article_selector', '.article')
    
    def parse_articles(self, html: str) -> List[Dict]:
        """è§£ææ–‡ç« åˆ—è¡¨"""
        ...
    
    async def parse_article_detail(self, url: str, html: str) -> Dict:
        """è§£ææ–‡ç« è¯¦æƒ…é¡µ"""
        ...
    
    def _get_image_url(self, img_tag) -> Optional[str]:
        """
        é‡å†™ï¼šè·å–åŸå›¾URL
        
        ä¼˜å…ˆçº§: srcset(æœ€å¤§å°ºå¯¸) > data-src > src(å»é™¤å°ºå¯¸åç¼€)
        """
        # æ–¹æ³•1: srcset
        srcset = img_tag.get('srcset', '')
        if srcset:
            max_url = self._parse_srcset_max(srcset)
            if max_url:
                return max_url
        
        # æ–¹æ³•2: data-src
        if img_tag.get('data-src'):
            return img_tag['data-src']
        
        # æ–¹æ³•3: src (å»é™¤å°ºå¯¸åç¼€)
        src = img_tag.get('src', '')
        return re.sub(r'-\d+x\d+(\.[a-zA-Z]+)$', r'\1', src) if src else None
    
    def _extract_article_id(self, url: str) -> str:
        """æå–æ–‡ç« ID"""
        patterns = [
            r'/(\d+)/?$',
            r'id[=_](\d+)',
            r'article[/-](\d+)',
        ]
        return self._extract_id(url, patterns)
```

#### 3.3.4 BaseSpider (æ–°å¢)

```python
class BaseSpider(ABC):
    """
    çˆ¬è™«åŸºç±»
    
    æ‰€æœ‰çˆ¬è™«çš„å…¬å…±åŸºç±»ï¼Œæä¾›ï¼š
    - HTTP Session ç®¡ç†
    - é¡µé¢è·å–
    - ç»Ÿè®¡ä¿¡æ¯
    - å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†
    """
    
    def __init__(self, config: Config):
        self.config = config
        self.session: Optional[aiohttp.ClientSession] = None
        self.stats = {
            'pages_fetched': 0,
            'requests_failed': 0,
        }
    
    async def __aenter__(self) -> 'BaseSpider':
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        await self.init()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        await self.close()
    
    async def init(self):
        """åˆå§‹åŒ–çˆ¬è™«ï¼ˆåˆ›å»ºsessionç­‰ï¼‰"""
        timeout = aiohttp.ClientTimeout(total=self.config.crawler.request_timeout)
        self.session = aiohttp.ClientSession(timeout=timeout)
    
    async def close(self):
        """å…³é—­çˆ¬è™«ï¼ˆæ¸…ç†èµ„æºï¼‰"""
        if self.session:
            await self.session.close()
    
    def get_headers(self) -> Dict[str, str]:
        """è·å–è¯·æ±‚å¤´"""
        return {
            "User-Agent": self.config.crawler.user_agent or UserAgent().random,
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
        }
    
    async def fetch_page(self, url: str, headers: Optional[Dict] = None) -> Optional[str]:
        """
        è·å–é¡µé¢å†…å®¹
        
        Args:
            url: é¡µé¢URL
            headers: å¯é€‰çš„é¢å¤–è¯·æ±‚å¤´
        
        Returns:
            HTMLå†…å®¹ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            request_headers = self.get_headers()
            if headers:
                request_headers.update(headers)
            
            async with self.session.get(url, headers=request_headers) as response:
                if response.status == 200:
                    self.stats['pages_fetched'] += 1
                    return await response.text()
                else:
                    logger.warning(f"HTTP {response.status}: {url}")
                    return None
        except Exception as e:
            self.stats['requests_failed'] += 1
            logger.error(f"è·å–é¡µé¢å¤±è´¥ {url}: {e}")
            return None
    
    @abstractmethod
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯ï¼ˆå­ç±»å®ç°ï¼‰"""
        pass
```

#### 3.3.5 BBSSpider (ä¿®æ”¹)

```python
class BBSSpider(BaseSpider):
    """
    BBSè®ºå›çˆ¬è™«
    
    ç»§æ‰¿ BaseSpiderï¼Œæ·»åŠ è®ºå›ç‰¹æœ‰åŠŸèƒ½ï¼š
    - å¸–å­åˆ—è¡¨è§£æ
    - å¸–å­è¯¦æƒ…çˆ¬å–
    - å›¾ç‰‡ä¸‹è½½
    """
    
    def __init__(self, config: Config):
        super().__init__(config)
        self.parser = BBSParser()
        self.downloader = None
        self.deduplicator = None
        self.storage = None
        
        # BBSç‰¹æœ‰ç»Ÿè®¡
        self.stats.update({
            'threads_crawled': 0,
            'images_found': 0,
            'images_downloaded': 0,
            'images_failed': 0,
            'duplicates_skipped': 0,
        })
    
    async def init(self):
        """åˆå§‹åŒ–BBSçˆ¬è™«"""
        await super().init()  # è°ƒç”¨åŸºç±»åˆå§‹åŒ–
        # BBSç‰¹æœ‰åˆå§‹åŒ–
        self.downloader = ImageDownloader(...)
        self.deduplicator = ImageDeduplicator(...)
        self.storage = Storage(...)
    
    async def close(self):
        """å…³é—­BBSçˆ¬è™«"""
        # BBSç‰¹æœ‰æ¸…ç†
        if self.downloader:
            await self.downloader.close()
        await super().close()  # è°ƒç”¨åŸºç±»å…³é—­
    
    # ... å…¶ä»–BBSç‰¹æœ‰æ–¹æ³•
```

#### 3.3.6 DynamicNewsCrawler (ä¿®æ”¹)

```python
class DynamicNewsCrawler(BaseSpider):
    """
    åŠ¨æ€æ–°é—»é¡µé¢çˆ¬è™«
    
    ç»§æ‰¿ BaseSpiderï¼Œæ·»åŠ åŠ¨æ€é¡µé¢ç‰¹æœ‰åŠŸèƒ½ï¼š
    - Ajaxåˆ†é¡µå¤„ç†
    - æ–‡ç« è¯¦æƒ…çˆ¬å–
    - å›¾ç‰‡æå–
    """
    
    def __init__(self, config: Config):
        super().__init__(config)
        self.parser = DynamicPageParser(config)
        
        # åŠ¨æ€é¡µé¢ç‰¹æœ‰ç»Ÿè®¡
        self.stats.update({
            'articles_found': 0,
            'articles_crawled': 0,
            'articles_failed': 0,
            'images_downloaded': 0,
            'images_failed': 0,
        })
    
    async def fetch_page(self, url: str, headers: Optional[Dict] = None, is_ajax: bool = False) -> Optional[str]:
        """
        é‡å†™ fetch_pageï¼Œæ”¯æŒ Ajax è¯·æ±‚
        """
        request_headers = headers or {}
        if is_ajax:
            request_headers["X-Requested-With"] = "XMLHttpRequest"
        return await super().fetch_page(url, request_headers)
    
    # ... å…¶ä»–åŠ¨æ€é¡µé¢ç‰¹æœ‰æ–¹æ³•
```

#### 3.3.7 SpiderFactory (æ‰©å±•)

```python
class SpiderFactory:
    """
    çˆ¬è™«å·¥å‚ç±»
    
    ç»Ÿä¸€ç®¡ç†æ‰€æœ‰çˆ¬è™«ç±»å‹çš„åˆ›å»º
    """
    
    _registry: Dict[str, Type[BaseSpider]] = {
        # BBSç±»å‹
        'discuz': DiscuzSpider,
        'phpbb': PhpBBSpider,
        'vbulletin': VBulletinSpider,
        'generic': BBSSpider,
        # åŠ¨æ€é¡µé¢ç±»å‹
        'dynamic': DynamicNewsCrawler,  # ğŸ†• æ–°å¢
    }
    
    @classmethod
    def create(cls, 
               config: Optional[Config] = None, 
               url: Optional[str] = None, 
               preset: Optional[str] = None,
               spider_type: Optional[str] = None  # ğŸ†• æ–°å¢å‚æ•°
    ) -> BaseSpider:
        """
        åˆ›å»ºçˆ¬è™«å®ä¾‹
        
        Args:
            config: é…ç½®å¯¹è±¡
            url: è®ºå›URLï¼ˆç”¨äºè‡ªåŠ¨æ£€æµ‹ï¼‰
            preset: è®ºå›ç±»å‹é¢„è®¾
            spider_type: çˆ¬è™«ç±»å‹ (bbs/dynamic)
        
        Returns:
            çˆ¬è™«å®ä¾‹
        """
        # ç¡®å®šçˆ¬è™«ç±»å‹
        if spider_type == 'dynamic':
            return DynamicNewsCrawler(config or Config())
        
        # åŸæœ‰BBSé€»è¾‘...
```

### 3.3 CLI æ›´æ–°

```bash
# ç°æœ‰å‘½ä»¤ï¼ˆä¿æŒä¸å˜ï¼‰
spider.py crawl-url "https://bbs.com/thread/123" --auto-detect
spider.py crawl-urls --config xindong
spider.py crawl-board "https://bbs.com/forum/1" --config xindong
spider.py crawl-boards --config xindong

# åŠ¨æ€é¡µé¢å‘½ä»¤ï¼ˆå·²å®ç°ï¼‰
spider.py crawl-news "https://sxd.xd.com/" --download-images --max-pages 5
```

---

## 4. å®ç°è®¡åˆ’

### 4.1 é˜¶æ®µåˆ’åˆ†

| é˜¶æ®µ | å†…å®¹ | é¢„è®¡æ—¶é—´ | é£é™© |
|------|------|---------|------|
| **é˜¶æ®µ1** | åˆ›å»º `BaseParser` åŸºç±» | 30åˆ†é’Ÿ | ä½ |
| **é˜¶æ®µ2** | ä¿®æ”¹ `BBSParser` ç»§æ‰¿ `BaseParser` | 30åˆ†é’Ÿ | ä¸­ |
| **é˜¶æ®µ3** | ä¿®æ”¹ `DynamicPageParser` ç»§æ‰¿ `BaseParser` | 30åˆ†é’Ÿ | ä¸­ |
| **é˜¶æ®µ4** | åˆ›å»º `BaseSpider` åŸºç±» | 30åˆ†é’Ÿ | ä½ |
| **é˜¶æ®µ5** | ä¿®æ”¹ `BBSSpider` ç»§æ‰¿ `BaseSpider` | 30åˆ†é’Ÿ | ä¸­ |
| **é˜¶æ®µ6** | ä¿®æ”¹ `DynamicNewsCrawler` ç»§æ‰¿ `BaseSpider` | 30åˆ†é’Ÿ | ä¸­ |
| **é˜¶æ®µ7** | æ‰©å±• `SpiderFactory` | 15åˆ†é’Ÿ | ä½ |
| **é˜¶æ®µ8** | æ›´æ–°æ–‡æ¡£å’Œæµ‹è¯• | 30åˆ†é’Ÿ | ä½ |

### 4.2 æ–‡ä»¶å˜æ›´

| æ–‡ä»¶ | å˜æ›´ç±»å‹ | å†…å®¹ | çŠ¶æ€ |
|------|---------|------|------|
| `core/parser.py` | ä¿®æ”¹ | æ·»åŠ  `BaseParser`ï¼Œä¿®æ”¹ `BBSParser` ç»§æ‰¿ | âœ… å®Œæˆ |
| `core/dynamic_parser.py` | ä¿®æ”¹ | ç»§æ‰¿ `BaseParser`ï¼Œåˆ é™¤é‡å¤ä»£ç  | âœ… å®Œæˆ |
| `spider.py` | ä¿®æ”¹ | æ·»åŠ  `BaseSpider`ï¼Œä¿®æ”¹ `BBSSpider` ç»§æ‰¿ | âœ… å®Œæˆ |
| `core/dynamic_crawler.py` | ä¿®æ”¹ | ä¸ `BaseSpider` æ¥å£ä¸€è‡´ | âœ… å®Œæˆ |
| `ARCHITECTURE.md` | ä¿®æ”¹ | æ›´æ–°æ¶æ„å›¾ | âœ… å®Œæˆ |
| `README.md` | ä¿®æ”¹ | æ·»åŠ åŠ¨æ€é¡µé¢çˆ¬è™«æ–‡æ¡£ | âœ… å®Œæˆ |
| `run_spider.sh` | ä¿®æ”¹ | æ·»åŠ  `crawl-news` ç¤ºä¾‹ | âœ… å®Œæˆ |

### 4.3 å‘åå…¼å®¹

| API | å…¼å®¹æ€§ | è¯´æ˜ |
|-----|--------|------|
| `SpiderFactory.create()` | âœ… å®Œå…¨å…¼å®¹ | ç°æœ‰å‚æ•°ä¿æŒä¸å˜ |
| `BBSSpider` | âœ… å®Œå…¨å…¼å®¹ | å…¬å…±æ¥å£ä¸å˜ |
| `DynamicNewsCrawler` | âœ… å®Œå…¨å…¼å®¹ | å…¬å…±æ¥å£ä¸å˜ |
| CLI å‘½ä»¤ | âœ… å®Œå…¨å…¼å®¹ | ç°æœ‰å‘½ä»¤ä¿æŒä¸å˜ |

---

## 5. é£é™©è¯„ä¼°

| é£é™© | å¯èƒ½æ€§ | å½±å“ | ç¼“è§£æªæ–½ |
|------|--------|------|---------|
| ç»§æ‰¿é“¾è¿‡æ·± | ä½ | ä¸­ | åªæœ‰2å±‚ç»§æ‰¿ |
| æ¥å£ä¸å…¼å®¹ | ä¸­ | é«˜ | å……åˆ†æµ‹è¯•ï¼Œä¿æŒå…¬å…±æ¥å£ |
| æ€§èƒ½ä¸‹é™ | ä½ | ä½ | åŸºç±»æ–¹æ³•ç®€å•ï¼Œæ— æ€§èƒ½å½±å“ |

---

## 6. æµ‹è¯•è®¡åˆ’

### 6.1 å•å…ƒæµ‹è¯•

```python
# æµ‹è¯• BaseSpider
def test_base_spider_init():
    spider = ConcreteSpider(config)
    assert spider.session is None
    
async def test_base_spider_fetch_page():
    async with ConcreteSpider(config) as spider:
        html = await spider.fetch_page("https://example.com")
        assert html is not None
```

### 6.2 é›†æˆæµ‹è¯•

```bash
# æµ‹è¯• BBS çˆ¬è™«
python spider.py crawl-url "https://bbs.xd.com/..." --config xindong

# æµ‹è¯•åŠ¨æ€é¡µé¢çˆ¬è™«
python spider.py crawl-news "https://sxd.xd.com/" --download-images --max-pages 2
```

---

## 7. å®¡æ‰¹

| è§’è‰² | å§“å | æ„è§ | æ—¥æœŸ |
|------|------|------|------|
| æ¶æ„å¸ˆ | Chang | âœ… å·²æ‰¹å‡† | 2026-02-06 |
| å¼€å‘è€… | AI Assistant | âœ… å·²å®ç° | 2026-02-06 |

---

## 8. é™„å½•

### 8.1 å‚è€ƒèµ„æ–™

- [Python ABC æ¨¡å—æ–‡æ¡£](https://docs.python.org/3/library/abc.html)
- [è®¾è®¡æ¨¡å¼ï¼šæ¨¡æ¿æ–¹æ³•æ¨¡å¼](https://refactoring.guru/design-patterns/template-method)

### 8.2 ç›¸å…³è®¾è®¡æ–‡æ¡£

- `docs/designs/2026-02-05-dynamic-news-page-crawler.md` - åŠ¨æ€æ–°é—»é¡µé¢çˆ¬è™«è®¾è®¡
- `docs/designs/2026-02-04-implement-subcommand-mode.md` - å­å‘½ä»¤æ¨¡å¼å®ç°
