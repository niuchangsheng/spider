"""
å¿ƒåŠ¨è®ºå›çˆ¬è™« - ä¸“ç”¨å¯åŠ¨è„šæœ¬
é’ˆå¯¹ https://bbs.xd.com è¿›è¡Œä¼˜åŒ–
"""
import asyncio
import sys
from pathlib import Path
from loguru import logger

# å¯¼å…¥é…ç½®
from config_xindong import xindong_config, XINDONG_BOARDS, EXAMPLE_THREADS
from bbs_spider import BBSSpider

# åº”ç”¨å¿ƒåŠ¨è®ºå›é…ç½®
import config as config_module
config_module.config = xindong_config


class XindongSpider(BBSSpider):
    """å¿ƒåŠ¨è®ºå›ä¸“ç”¨çˆ¬è™«"""
    
    def __init__(self):
        super().__init__()
        logger.info("å¿ƒåŠ¨è®ºå›çˆ¬è™«åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"ç›®æ ‡ç«™ç‚¹: {self.config.bbs.base_url}")
    
    async def process_discuz_images(self, images: list) -> list:
        """
        å¤„ç†Discuzè®ºå›çš„ç‰¹æ®Šå›¾ç‰‡é“¾æ¥
        Discuzçš„é™„ä»¶é“¾æ¥æ ¼å¼ï¼šforum.php?mod=attachment&aid=xxx
        """
        processed_images = []
        
        for img_url in images:
            # å¤„ç†ç›¸å¯¹è·¯å¾„
            if img_url.startswith('forum.php') or img_url.startswith('/forum.php'):
                img_url = f"{self.config.bbs.base_url}/{img_url.lstrip('/')}"
            
            # Discuzé™„ä»¶é“¾æ¥é€šå¸¸éœ€è¦æ·»åŠ  &nothumb=yes è·å–åŸå›¾
            if 'mod=attachment' in img_url and 'nothumb' not in img_url:
                img_url += '&nothumb=yes'
            
            processed_images.append(img_url)
        
        return processed_images
    
    async def crawl_thread(self, thread_info: dict):
        """é‡å†™çˆ¬å–æ–¹æ³•ï¼Œæ·»åŠ Discuzç‰¹æ®Šå¤„ç†"""
        thread_url = thread_info['url']
        thread_id = thread_info['thread_id']
        
        # æ£€æŸ¥æ˜¯å¦å·²çˆ¬å–
        if self.config.database and hasattr(self, 'storage'):
            from core.storage import storage
            if storage.thread_exists(thread_id):
                logger.info(f"å¸–å­ {thread_id} å·²çˆ¬å–ï¼Œè·³è¿‡")
                return
        
        logger.info(f"æ­£åœ¨çˆ¬å–å¸–å­: {thread_info.get('title', thread_id)}")
        
        # è·å–å¸–å­é¡µé¢
        html = await self.fetch_page(thread_url)
        if not html:
            return
        
        # è§£æå¸–å­å†…å®¹
        thread_data = self.parser.parse_thread_page(html, thread_url)
        thread_data['board'] = thread_info.get('board')
        thread_data['title'] = thread_info.get('title')
        
        # å¤„ç†Discuzç‰¹æ®Šå›¾ç‰‡é“¾æ¥
        thread_data['images'] = await self.process_discuz_images(thread_data['images'])
        
        # æ›´æ–°ç»Ÿè®¡
        self.stats['threads_crawled'] += 1
        self.stats['images_found'] += len(thread_data['images'])
        
        logger.info(f"å‘ç° {len(thread_data['images'])} å¼ å›¾ç‰‡")
        
        # ä¸‹è½½å›¾ç‰‡
        if thread_data['images']:
            await self.download_thread_images(thread_data)
        
        # ä¿å­˜å¸–å­æ•°æ®ï¼ˆå¦‚æœé…ç½®äº†æ•°æ®åº“ï¼‰
        try:
            from core.storage import storage
            if storage.mongo_client:
                storage.save_thread(thread_data)
        except:
            logger.debug("æ•°æ®åº“æœªé…ç½®ï¼Œè·³è¿‡ä¿å­˜å…ƒæ•°æ®")


async def crawl_single_thread():
    """ç¤ºä¾‹1ï¼šçˆ¬å–å•ä¸ªå¸–å­"""
    logger.info("=" * 60)
    logger.info("çˆ¬å–å¿ƒåŠ¨è®ºå›å•ä¸ªå¸–å­")
    logger.info("=" * 60)
    
    async with XindongSpider() as spider:
        # çˆ¬å–ç¤ºä¾‹å¸–å­
        thread_url = EXAMPLE_THREADS[0]
        
        thread_info = {
            'url': thread_url,
            'thread_id': spider.parser._extract_thread_id(thread_url),
            'title': 'ç¥ä»™é“æ€€æ—§æœå…¬æµ‹',
            'board': 'ç¥ä»™é“'
        }
        
        await spider.crawl_thread(thread_info)
        
        # æ˜¾ç¤ºç»Ÿè®¡
        stats = spider.get_statistics()
        logger.success("=" * 60)
        logger.success(f"çˆ¬å–å®Œæˆï¼")
        logger.success(f"å¸–å­æ•°: {stats['threads_crawled']}")
        logger.success(f"å‘ç°å›¾ç‰‡: {stats['images_found']}")
        logger.success(f"ä¸‹è½½æˆåŠŸ: {stats['images_downloaded']}")
        logger.success(f"ä¸‹è½½å¤±è´¥: {stats['images_failed']}")
        logger.success(f"å»é‡è·³è¿‡: {stats['duplicates_skipped']}")
        logger.success("=" * 60)


async def crawl_board():
    """ç¤ºä¾‹2ï¼šçˆ¬å–æ¿å—"""
    logger.info("=" * 60)
    logger.info("çˆ¬å–å¿ƒåŠ¨è®ºå›æ¿å—")
    logger.info("=" * 60)
    
    board_info = XINDONG_BOARDS["ç¥ä»™é“"]
    
    async with XindongSpider() as spider:
        await spider.crawl_board(
            board_url=board_info["url"],
            board_name=board_info["board_name"],
            max_pages=3  # åªçˆ¬å–å‰3é¡µ
        )


async def crawl_multiple_threads():
    """ç¤ºä¾‹3ï¼šæ‰¹é‡çˆ¬å–å¤šä¸ªå¸–å­"""
    logger.info("=" * 60)
    logger.info("æ‰¹é‡çˆ¬å–å¿ƒåŠ¨è®ºå›å¸–å­")
    logger.info("=" * 60)
    
    # æ·»åŠ æ›´å¤šå¸–å­URL
    thread_urls = [
        "https://bbs.xd.com/forum.php?mod=viewthread&tid=3479145&extra=page%3D1",
        # å¯ä»¥æ·»åŠ æ›´å¤šå¸–å­URL
    ]
    
    async with XindongSpider() as spider:
        await spider.crawl_threads_from_list(thread_urls)


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    # å‘½ä»¤è¡Œå‚æ•°è§£æ
    parser = argparse.ArgumentParser(description='å¿ƒåŠ¨è®ºå›å›¾ç‰‡çˆ¬è™«')
    parser.add_argument(
        '--mode', 
        type=int, 
        default=1, 
        choices=[1, 2, 3],
        help='è¿è¡Œæ¨¡å¼: 1=å•ä¸ªå¸–å­(é»˜è®¤), 2=çˆ¬å–æ¿å—, 3=æ‰¹é‡å¸–å­'
    )
    args = parser.parse_args()
    
    # é…ç½®æ—¥å¿—
    logger.remove()  # ç§»é™¤é»˜è®¤å¤„ç†å™¨
    logger.add(
        sys.stderr,
        format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <level>{message}</level>",
        level="INFO",
        colorize=True
    )
    
    # åŒæ—¶è¾“å‡ºåˆ°æ–‡ä»¶
    log_file = Path(__file__).parent / "logs" / "xindong_spider.log"
    log_file.parent.mkdir(parents=True, exist_ok=True)
    logger.add(
        log_file,
        rotation="100 MB",
        retention="30 days",
        encoding="utf-8",
        level="DEBUG"
    )
    
    print("\n" + "=" * 60)
    print("å¿ƒåŠ¨è®ºå›å›¾ç‰‡çˆ¬è™«")
    print("=" * 60)
    
    # æ ¹æ®å‚æ•°é€‰æ‹©åŠŸèƒ½
    choice = args.mode
    
    if choice == 1:
        print("\nğŸ“Œ æ¨¡å¼: çˆ¬å–ç¤ºä¾‹å¸–å­")
        asyncio.run(crawl_single_thread())
    elif choice == 2:
        print("\nğŸ“Œ æ¨¡å¼: çˆ¬å–ç¥ä»™é“æ¿å—ï¼ˆå‰3é¡µï¼‰")
        asyncio.run(crawl_board())
    elif choice == 3:
        print("\nğŸ“Œ æ¨¡å¼: æ‰¹é‡çˆ¬å–å¤šä¸ªå¸–å­")
        asyncio.run(crawl_multiple_threads())


if __name__ == "__main__":
    main()
