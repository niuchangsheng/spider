"""
åŠ¨æ€æ–°é—»é¡µé¢çˆ¬è™«
ç”¨äºçˆ¬å–ä½¿ç”¨Ajaxå¼‚æ­¥åŠ è½½å†…å®¹çš„æ–°é—»/å…¬å‘Šé¡µé¢
"""
import asyncio
import aiohttp
from typing import List, Dict, Optional
from loguru import logger
from pathlib import Path
from fake_useragent import UserAgent

from config import Config
from parsers.dynamic_parser import DynamicPageParser
from core.checkpoint import CheckpointManager


class DynamicNewsCrawler:
    """
    åŠ¨æ€æ–°é—»é¡µé¢çˆ¬è™«
    
    ä¸ BBSSpider å¹¶åˆ—ï¼Œç»§æ‰¿ç›¸åŒçš„è®¾è®¡ç†å¿µï¼š
    - å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†
    - ç»Ÿä¸€çš„ç»Ÿè®¡ä¿¡æ¯æ¥å£
    - å¯é…ç½®çš„è¯·æ±‚å¤´
    
    ç‰¹ç‚¹ï¼š
    - æ”¯æŒAjaxæ–¹å¼å¿«é€Ÿçˆ¬å–
    - æ”¯æŒSeleniumæ–¹å¼å¤‡ç”¨ï¼ˆå¯é æ€§é«˜ï¼‰
    - è‡ªåŠ¨å¤„ç†"æŸ¥çœ‹æ›´å¤š"åˆ†é¡µ
    - æ”¯æŒæ‰¹é‡ä¸‹è½½æ–‡ç« è¯¦æƒ…å’Œå›¾ç‰‡
    
    Example:
        config = Config(bbs={"base_url": "https://example.com"})
        crawler = DynamicNewsCrawler(config)
        
        async with crawler:
            articles = await crawler.crawl_dynamic_page_ajax(
                "https://example.com/news",
                max_pages=5
            )
    """
    
    def __init__(self, config: Config):
        """
        åˆå§‹åŒ–åŠ¨æ€æ–°é—»çˆ¬è™«
        
        Args:
            config: é…ç½®å¯¹è±¡
        """
        self.config = config
        self.parser = DynamicPageParser(config)
        self.session = None
        self.ua = None
        
        # ç»Ÿè®¡ä¿¡æ¯ï¼ˆä¸ BaseSpider ä¿æŒä¸€è‡´çš„ç»“æ„ï¼‰
        self.stats = {
            'pages_fetched': 0,       # åŸºç¡€ç»Ÿè®¡
            'requests_failed': 0,     # åŸºç¡€ç»Ÿè®¡
            'articles_found': 0,      # å‘ç°çš„æ–‡ç« æ•°
            'articles_crawled': 0,    # æˆåŠŸçˆ¬å–çš„æ–‡ç« æ•°
            'articles_failed': 0,     # å¤±è´¥çš„æ–‡ç« æ•°
            'images_downloaded': 0,   # ä¸‹è½½çš„å›¾ç‰‡æ•°
            'images_failed': 0,       # å¤±è´¥çš„å›¾ç‰‡æ•°
        }
        
        logger.info(f"ğŸš€ åˆå§‹åŒ–åŠ¨æ€æ–°é—»çˆ¬è™«: {config.bbs.name}")
    
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        await self.init()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        await self.close()
    
    async def init(self):
        """åˆå§‹åŒ–çˆ¬è™«"""
        logger.info("âš™ï¸  åˆå§‹åŒ–çˆ¬è™«ç»„ä»¶...")
        
        self.ua = UserAgent()
        
        # åˆ›å»ºHTTPä¼šè¯
        timeout = aiohttp.ClientTimeout(total=self.config.crawler.request_timeout)
        self.session = aiohttp.ClientSession(timeout=timeout)
        
        logger.debug("âœ“ HTTPä¼šè¯å·²åˆ›å»º")
    
    async def close(self):
        """å…³é—­çˆ¬è™«"""
        logger.info("ğŸ”’ å…³é—­çˆ¬è™«...")
        
        if self.session:
            await self.session.close()
            logger.debug("âœ“ HTTPä¼šè¯å·²å…³é—­")
        
        logger.info(f"ğŸ“Š çˆ¬è™«ç»Ÿè®¡: {self.get_statistics()}")
    
    def get_headers(self) -> Dict[str, str]:
        """è·å–è¯·æ±‚å¤´"""
        return {
            "User-Agent": self.ua.random if self.config.crawler.rotate_user_agent else self.ua.chrome,
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Accept-Encoding": "gzip, deflate",
            "Connection": "keep-alive",
        }
    
    async def fetch_page(self, url: str, headers: Optional[Dict] = None, is_ajax: bool = False) -> Optional[str]:
        """
        è·å–é¡µé¢å†…å®¹
        
        Args:
            url: é¡µé¢URL
            headers: å¯é€‰çš„HTTPå¤´
            is_ajax: æ˜¯å¦ä¸ºAjaxè¯·æ±‚ï¼ˆä¼šæ·»åŠ X-Requested-Withå¤´ï¼‰
        
        Returns:
            HTMLå†…å®¹ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            logger.debug(f"ğŸ“„ è·å–é¡µé¢: {url} (Ajax: {is_ajax})")
            
            # è·å–åŸºç¡€è¯·æ±‚å¤´
            request_headers = self.get_headers()
            
            # åˆå¹¶è‡ªå®šä¹‰ headers
            if headers:
                request_headers.update(headers)
            
            # Ajaxè¯·æ±‚éœ€è¦ç‰¹æ®Šå¤´
            if is_ajax:
                request_headers["X-Requested-With"] = "XMLHttpRequest"
            
            async with self.session.get(url, headers=request_headers) as response:
                if response.status == 200:
                    self.stats['pages_fetched'] += 1
                    html = await response.text()
                    await asyncio.sleep(self.config.crawler.download_delay)
                    return html
                else:
                    logger.warning(f"âš ï¸  HTTP {response.status}: {url}")
                    return None
        
        except asyncio.TimeoutError:
            self.stats['requests_failed'] += 1
            logger.error(f"âŒ è¶…æ—¶: {url}")
            return None
        except Exception as e:
            self.stats['requests_failed'] += 1
            logger.error(f"âŒ è·å–å¤±è´¥ {url}: {e}")
            return None
    
    async def crawl_dynamic_page_ajax(
        self, 
        base_url: str, 
        max_pages: Optional[int] = None,
        resume: bool = True,
        start_page: Optional[int] = None
    ) -> List[Dict]:
        """
        ä½¿ç”¨Ajaxæ–¹å¼çˆ¬å–åŠ¨æ€é¡µé¢ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰
        
        é€šè¿‡ç›´æ¥è¯·æ±‚åˆ†é¡µURLï¼ˆå¦‚ ?page=2ï¼‰æ¥è·å–æ›´å¤šå†…å®¹ã€‚
        è¿™ç§æ–¹å¼é€Ÿåº¦å¿«ã€èµ„æºå ç”¨å°‘ã€‚
        
        Args:
            base_url: åŸºç¡€URL
            max_pages: æœ€å¤§é¡µæ•°ï¼ŒNoneè¡¨ç¤ºä¸é™åˆ¶
            resume: æ˜¯å¦ä»æ£€æŸ¥ç‚¹æ¢å¤ï¼ˆé»˜è®¤Trueï¼‰
            start_page: èµ·å§‹é¡µç ï¼ˆå¦‚æœæŒ‡å®šï¼Œä¼šè¦†ç›–æ£€æŸ¥ç‚¹ï¼‰
        
        Returns:
            æ–‡ç« åˆ—è¡¨
        """
        logger.info(f"ğŸš€ å¼€å§‹çˆ¬å–åŠ¨æ€é¡µé¢ï¼ˆAjaxæ–¹å¼ï¼‰")
        logger.info(f"   URL: {base_url}")
        logger.info(f"   æœ€å¤§é¡µæ•°: {max_pages if max_pages else 'ä¸é™åˆ¶'}")
        
        # 1. åˆ›å»ºæ£€æŸ¥ç‚¹ç®¡ç†å™¨
        from urllib.parse import urlparse
        parsed = urlparse(base_url)
        site = parsed.netloc or self.config.bbs.base_url
        checkpoint = CheckpointManager(site=site, board="news")
        
        # 2. ä»æ£€æŸ¥ç‚¹æ¢å¤
        seen_article_ids = set()
        start_page_num = 1
        min_article_id = None
        max_article_id = None
        
        if start_page is not None:
            # æ‰‹åŠ¨æŒ‡å®šèµ·å§‹é¡µï¼ˆè¦†ç›–æ£€æŸ¥ç‚¹ï¼‰
            start_page_num = start_page
            logger.info(f"ğŸ“Œ æ‰‹åŠ¨æŒ‡å®šèµ·å§‹é¡µ: {start_page_num}")
            if checkpoint.exists():
                logger.info("âš ï¸  å°†è¦†ç›–ç°æœ‰æ£€æŸ¥ç‚¹")
        elif resume and checkpoint.exists():
            # ä»æ£€æŸ¥ç‚¹æ¢å¤
            checkpoint_data = checkpoint.load_checkpoint()
            if checkpoint_data:
                status = checkpoint_data.get('status', 'running')
                if status == 'completed':
                    logger.info("âœ… è¯¥ä»»åŠ¡å·²å®Œæˆçˆ¬å–ï¼Œè·³è¿‡")
                    return []
                
                start_page_num = checkpoint_data.get('current_page', 1)
                seen_article_ids = checkpoint.get_seen_article_ids()
                min_article_id = checkpoint.get_min_article_id()
                max_article_id = checkpoint.get_max_article_id()
                
                logger.info(f"ğŸ”„ ä»æ£€æŸ¥ç‚¹æ¢å¤: ç¬¬ {start_page_num} é¡µ")
                logger.info(f"   å·²çˆ¬å–æ–‡ç« æ•°: {len(seen_article_ids)}")
                if min_article_id:
                    logger.info(f"   æœ€å°æ–‡ç« ID: {min_article_id} (å·²çˆ¬å–çš„æœ€æ—§æ–‡ç« )")
                if max_article_id:
                    logger.info(f"   æœ€å¤§æ–‡ç« ID: {max_article_id} (å·²çˆ¬å–çš„æœ€æ–°æ–‡ç« )")
                logger.info(f"   ç­–ç•¥: è·³è¿‡ {min_article_id}~{max_article_id} ä¹‹é—´çš„æ–‡ç« ")
                logger.info(f"   ç­–ç•¥: çˆ¬å– > {max_article_id} çš„æ–°æ–‡ç« ï¼ˆå¦‚æœç½‘ç«™æœ‰æ›´æ–°ï¼‰")
                logger.info(f"   ç­–ç•¥: ç»§ç»­çˆ¬å– < {min_article_id} çš„æ—§æ–‡ç« ")
        
        page = start_page_num
        all_articles = []
        
        try:
            while True:
                # æ„é€ åˆ†é¡µURL
                if page == 1:
                    page_url = base_url
                else:
                    # å°è¯•å¤šç§åˆ†é¡µURLæ ¼å¼
                    separator = '&' if '?' in base_url else '?'
                    page_url = f"{base_url}{separator}page={page}"
                
                logger.info(f"\nğŸ“„ çˆ¬å–ç¬¬ {page} é¡µ: {page_url}")
                
                # è·å–é¡µé¢å†…å®¹ï¼ˆåˆ†é¡µè¯·æ±‚éœ€è¦Ajaxå¤´ï¼‰
                html = await self.fetch_page(page_url, is_ajax=(page > 1))
                
                if not html:
                    logger.warning(f"âš ï¸  ç¬¬{page}é¡µè·å–å¤±è´¥ï¼Œåœæ­¢çˆ¬å–")
                    break
                
                # è§£ææ–‡ç« åˆ—è¡¨
                articles = self.parser.parse_articles(html)
                
                if not articles:
                    logger.info(f"âœ… ç¬¬{page}é¡µæ²¡æœ‰æ–‡ç« ï¼Œåœæ­¢çˆ¬å–")
                    break
            
                # è¿‡æ»¤é‡å¤æ–‡ç« ï¼ˆåŸºäº article_id å»é‡ï¼‰
                # ç­–ç•¥ï¼š
                # 1. article_id > max_article_id: æ–°æ–‡ç« ï¼ˆç½‘ç«™æœ‰æ›´æ–°ï¼‰ï¼Œåº”è¯¥çˆ¬å–
                # 2. min_article_id <= article_id <= max_article_id: ä¸­é—´èŒƒå›´ï¼ˆå·²çˆ¬å–ï¼‰ï¼Œåº”è¯¥è·³è¿‡
                # 3. article_id < min_article_id: æ›´æ—§çš„æ–‡ç« ï¼ˆæœªçˆ¬å–ï¼‰ï¼Œåº”è¯¥ç»§ç»­çˆ¬å–
                new_articles = []
                has_new_articles_beyond_max = False  # æ ‡è®°æ˜¯å¦æœ‰è¶…è¿‡ max_article_id çš„æ–°æ–‡ç« 
                
                for article in articles:
                    article_id = article['article_id']
                    
                    # æ–¹å¼1: é€šè¿‡é›†åˆå»é‡ï¼ˆç²¾ç¡®ï¼‰
                    if article_id in seen_article_ids:
                        logger.debug(f"â­ï¸  è·³è¿‡å·²çˆ¬å–æ–‡ç« : {article_id} (åœ¨é›†åˆä¸­)")
                        continue
                    
                    # æ–¹å¼2: é€šè¿‡æœ€å°/æœ€å¤§IDåˆ¤æ–­ï¼ˆå¿«é€Ÿï¼Œé€‚ç”¨äºå€’åº/æ­£åºï¼‰
                    should_skip = False
                    try:
                        article_id_int = int(article_id)
                        
                        if max_article_id and min_article_id:
                            max_id_int = int(max_article_id)
                            min_id_int = int(min_article_id)
                            
                            # æƒ…å†µ1: æ–°æ–‡ç« ï¼ˆID > max_article_idï¼‰- ç½‘ç«™æœ‰æ›´æ–°
                            if article_id_int > max_id_int:
                                logger.info(f"ğŸ†• å‘ç°æ–°æ–‡ç« : {article_id} (>{max_article_id})ï¼Œç½‘ç«™æœ‰æ›´æ–°ï¼")
                                has_new_articles_beyond_max = True
                                # æ–°æ–‡ç« åº”è¯¥çˆ¬å–ï¼Œä¸è·³è¿‡
                            
                            # æƒ…å†µ2: ä¸­é—´èŒƒå›´ï¼ˆmin_article_id <= ID <= max_article_idï¼‰- å·²çˆ¬å–ï¼Œè·³è¿‡
                            elif min_id_int <= article_id_int <= max_id_int:
                                logger.debug(f"â­ï¸  è·³è¿‡ä¸­é—´èŒƒå›´æ–‡ç« : {article_id} ({min_article_id} <= {article_id} <= {max_article_id})")
                                seen_article_ids.add(article_id)  # æ·»åŠ åˆ°é›†åˆï¼Œé¿å…é‡å¤åˆ¤æ–­
                                should_skip = True
                            
                            # æƒ…å†µ3: æ›´æ—§çš„æ–‡ç« ï¼ˆID < min_article_idï¼‰- æœªçˆ¬å–ï¼Œåº”è¯¥ç»§ç»­çˆ¬å–
                            # ä¸è·³è¿‡ï¼Œç»§ç»­å¤„ç†
                        
                        elif max_article_id:
                            # åªæœ‰ max_article_idï¼Œæ²¡æœ‰ min_article_id
                            max_id_int = int(max_article_id)
                            if article_id_int > max_id_int:
                                logger.info(f"ğŸ†• å‘ç°æ–°æ–‡ç« : {article_id} (>{max_article_id})ï¼Œç½‘ç«™æœ‰æ›´æ–°ï¼")
                                has_new_articles_beyond_max = True
                        
                        elif min_article_id:
                            # åªæœ‰ min_article_idï¼Œæ²¡æœ‰ max_article_id
                            min_id_int = int(min_article_id)
                            # å¯¹äºå€’åºæ’åˆ—ï¼Œå¦‚æœ ID < min_article_idï¼Œè¯´æ˜æ˜¯æ›´æ—§çš„æ–‡ç« ï¼Œåº”è¯¥ç»§ç»­çˆ¬å–
                            # ä¸è·³è¿‡ï¼Œç»§ç»­å¤„ç†
                    
                    except (ValueError, TypeError):
                        # article_id ä¸æ˜¯æ•°å­—ï¼Œåªèƒ½é€šè¿‡é›†åˆå»é‡
                        pass
                    
                    if should_skip:
                        continue
                    
                    # æ–°æ–‡ç« ï¼Œæ·»åŠ åˆ°åˆ—è¡¨
                    seen_article_ids.add(article_id)
                    new_articles.append(article)
                    
                    # æ›´æ–°æœ€å°/æœ€å¤§ article_id
                    try:
                        article_id_int = int(article_id)
                        if not min_article_id or article_id_int < int(min_article_id):
                            min_article_id = article_id
                        if not max_article_id or article_id_int > int(max_article_id):
                            old_max = max_article_id
                            max_article_id = article_id
                            if old_max:
                                logger.info(f"ğŸ“ˆ æ›´æ–°æœ€å¤§æ–‡ç« ID: {old_max} -> {max_article_id}")
                    except (ValueError, TypeError):
                        pass
                
                # å¦‚æœå‘ç°æ–°æ–‡ç« ï¼Œè®°å½•æ—¥å¿—
                if has_new_articles_beyond_max:
                    logger.info(f"âœ¨ æ£€æµ‹åˆ°ç½‘ç«™æœ‰æ–°æ–‡ç« å‘å¸ƒï¼Œå·²å¼€å§‹çˆ¬å–æ–°å†…å®¹")
                
                if not new_articles:
                    logger.info(f"âœ… ç¬¬{page}é¡µæ²¡æœ‰æ–°æ–‡ç« ï¼ˆå…¨éƒ¨é‡å¤ï¼‰ï¼Œåœæ­¢çˆ¬å–")
                    break
                
                logger.info(f"   âœ“ å‘ç° {len(new_articles)} ç¯‡æ–°æ–‡ç«  (æœ¬é¡µå…± {len(articles)} ç¯‡)")
                all_articles.extend(new_articles)
                self.stats['articles_found'] += len(new_articles)
                
                # 3. ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆæ¯é¡µä¿å­˜ä¸€æ¬¡ï¼‰
                checkpoint.save_checkpoint(
                    current_page=page + 1,
                    last_thread_id=new_articles[-1]['article_id'] if new_articles else None,
                    last_thread_url=new_articles[-1].get('url') if new_articles else None,
                    status="running",
                    stats={
                        "articles_found": len(all_articles),
                        "articles_crawled": self.stats.get('articles_crawled', 0),
                        "images_downloaded": self.stats.get('images_downloaded', 0)
                    },
                    seen_article_ids=list(seen_article_ids),
                    min_article_id=min_article_id,
                    max_article_id=max_article_id
                )
                
                # æ£€æŸ¥é¡µæ•°é™åˆ¶
                if max_pages and page >= max_pages:
                    logger.info(f"âœ… è¾¾åˆ°æœ€å¤§é¡µæ•°é™åˆ¶: {max_pages}")
                    break
                
                # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰"æŸ¥çœ‹æ›´å¤š"æŒ‰é’® (è¾…åŠ©åˆ¤æ–­)
                has_more = self.parser.has_load_more_button(html)
                if not has_more:
                    logger.info("âœ… æ²¡æœ‰æ›´å¤šå†…å®¹æ ‡è¯†ï¼Œåœæ­¢çˆ¬å–")
                    break
                
                page += 1
            
            # 4. æ ‡è®°å®Œæˆ
            checkpoint.mark_completed(final_stats={
                "total_articles": len(all_articles),
                "total_images": self.stats.get('images_downloaded', 0)
            })
            
            logger.success(f"ğŸ‰ å®Œæˆçˆ¬å–ï¼æ€»å…±å‘ç° {len(all_articles)} ç¯‡æ–°æ–‡ç« ")
            
            return all_articles
            
        except Exception as e:
            # å‘ç”Ÿé”™è¯¯æ—¶ä¿å­˜æ£€æŸ¥ç‚¹
            logger.error(f"âŒ çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            checkpoint.mark_error(str(e))
            raise
    
    async def crawl_dynamic_page_selenium(
        self, 
        url: str, 
        max_clicks: Optional[int] = None
    ) -> List[Dict]:
        """
        ä½¿ç”¨Seleniumæ–¹å¼çˆ¬å–åŠ¨æ€é¡µé¢ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
        
        é€šè¿‡æ¨¡æ‹Ÿæµè§ˆå™¨ç‚¹å‡»"æŸ¥çœ‹æ›´å¤š"æŒ‰é’®æ¥åŠ è½½å†…å®¹ã€‚
        è¿™ç§æ–¹å¼æ›´å¯é ï¼Œä½†é€Ÿåº¦è¾ƒæ…¢ã€èµ„æºå ç”¨å¤§ã€‚
        
        Args:
            url: é¡µé¢URL
            max_clicks: æœ€å¤§ç‚¹å‡»æ¬¡æ•°ï¼ŒNoneè¡¨ç¤ºä¸é™åˆ¶
        
        Returns:
            æ–‡ç« åˆ—è¡¨
        """
        logger.info(f"ğŸš€ å¼€å§‹çˆ¬å–åŠ¨æ€é¡µé¢ï¼ˆSeleniumæ–¹å¼ï¼‰")
        logger.info(f"   URL: {url}")
        logger.info(f"   æœ€å¤§ç‚¹å‡»æ¬¡æ•°: {max_clicks if max_clicks else 'ä¸é™åˆ¶'}")
        
        try:
            from selenium import webdriver
            from selenium.webdriver.common.by import By
            from selenium.webdriver.support.ui import WebDriverWait
            from selenium.webdriver.support import expected_conditions as EC
            from selenium.common.exceptions import TimeoutException
        except ImportError:
            logger.error("âŒ ç¼ºå°‘Seleniumä¾èµ–ï¼Œè¯·å®‰è£…: pip install selenium")
            return []
        
        # é…ç½®Selenium
        options = webdriver.ChromeOptions()
        options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        
        driver = None
        
        try:
            driver = webdriver.Chrome(options=options)
            driver.get(url)
            
            logger.info("âœ“ æµè§ˆå™¨å·²å¯åŠ¨")
            
            clicks = 0
            last_article_count = 0
            
            while True:
                html = driver.page_source
                current_articles = self.parser.parse_articles(html)
                current_count = len(current_articles)
                logger.debug(f"å½“å‰æ–‡ç« æ•°: {current_count}")
                
                if clicks == 0:
                    last_article_count = current_count
                
                if max_clicks and clicks >= max_clicks:
                    logger.info(f"âœ… è¾¾åˆ°æœ€å¤§ç‚¹å‡»æ¬¡æ•°: {max_clicks}")
                    break
                
                try:
                    load_more = WebDriverWait(driver, 10).until(
                        EC.presence_of_element_located((
                            By.CSS_SELECTOR, 
                            'a.more, .load-more, .btn-more'
                        ))
                    )
                    
                    if not load_more.is_displayed():
                        logger.info("âš ï¸  'æŸ¥çœ‹æ›´å¤š'æŒ‰é’®ä¸å¯è§ï¼Œå¯èƒ½å·²åŠ è½½å®Œæ¯•")
                        break
                    
                    driver.execute_script("arguments[0].scrollIntoView({behavior: 'smooth', block: 'center'});", load_more)
                    await asyncio.sleep(1)
                    
                    driver.execute_script("arguments[0].click();", load_more)
                    clicks += 1
                    logger.info(f"ğŸ”„ ç‚¹å‡»'æŸ¥çœ‹æ›´å¤š' ç¬¬{clicks}æ¬¡")
                    
                    wait_time = 0
                    loaded = False
                    while wait_time < 10:
                        await asyncio.sleep(1)
                        wait_time += 1
                        
                        new_html = driver.page_source
                        new_articles = self.parser.parse_articles(new_html)
                        new_count = len(new_articles)
                        
                        if new_count > last_article_count:
                            logger.info(f"   âœ“ åŠ è½½æˆåŠŸï¼æ–°å¢ {new_count - last_article_count} ç¯‡æ–‡ç« ")
                            last_article_count = new_count
                            loaded = True
                            break
                    
                    if not loaded:
                        logger.warning(f"âš ï¸  ç­‰å¾…10ç§’åæ–‡ç« æ•°é‡æœªå¢åŠ ")
                        
                except TimeoutException:
                    logger.info("âœ… æ²¡æœ‰æ‰¾åˆ°'æŸ¥çœ‹æ›´å¤š'æŒ‰é’®ï¼Œåœæ­¢åŠ è½½")
                    break
                except Exception as e:
                    logger.error(f"âŒ ç‚¹å‡»è¿‡ç¨‹å‡ºé”™: {e}")
                    break
            
            html = driver.page_source
            articles = self.parser.parse_articles(html)
            self.stats['articles_found'] = len(articles)
            
            logger.success(f"ğŸ‰ å®Œæˆçˆ¬å–ï¼æ€»å…±å‘ç° {len(articles)} ç¯‡æ–‡ç« ")
            
            return articles
            
        except Exception as e:
            logger.error(f"âŒ Seleniumçˆ¬å–å¤±è´¥: {e}")
            return []
        
        finally:
            if driver:
                driver.quit()
                logger.debug("âœ“ æµè§ˆå™¨å·²å…³é—­")
    
    async def crawl_article_detail(self, article: Dict) -> Optional[Dict]:
        """çˆ¬å–å•ç¯‡æ–‡ç« è¯¦æƒ…"""
        url = article.get('url')
        if not url:
            logger.warning("âš ï¸  æ–‡ç« ç¼ºå°‘URLï¼Œè·³è¿‡")
            return None
        
        logger.info(f"ğŸ“ çˆ¬å–æ–‡ç« è¯¦æƒ…: {article.get('title', 'N/A')[:50]}")
        
        try:
            html = await self.fetch_page(url)
            
            if not html:
                logger.error(f"âŒ æ— æ³•è·å–æ–‡ç« è¯¦æƒ…: {url}")
                self.stats['articles_failed'] += 1
                return None
            
            detail = self.parser.parse_article_detail(html, url)
            full_article = {**article, **detail}
            
            self.stats['articles_crawled'] += 1
            logger.success(f"   âœ“ æˆåŠŸï¼Œå‘ç° {len(detail.get('images', []))} å¼ å›¾ç‰‡")
            
            return full_article
            
        except Exception as e:
            logger.error(f"âŒ çˆ¬å–æ–‡ç« è¯¦æƒ…å¤±è´¥: {e}")
            self.stats['articles_failed'] += 1
            return None
    
    async def crawl_articles_batch(self, articles: List[Dict]) -> List[Dict]:
        """æ‰¹é‡çˆ¬å–æ–‡ç« è¯¦æƒ…"""
        logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡çˆ¬å– {len(articles)} ç¯‡æ–‡ç« è¯¦æƒ…")
        
        tasks = [self.crawl_article_detail(article) for article in articles]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        full_articles = [r for r in results if r and not isinstance(r, Exception)]
        
        logger.success(f"âœ… æˆåŠŸçˆ¬å– {len(full_articles)}/{len(articles)} ç¯‡æ–‡ç« è¯¦æƒ…")
        
        return full_articles
    
    def get_statistics(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return self.stats.copy()
