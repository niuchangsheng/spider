"""
åŠ¨æ€æ–°é—»é¡µé¢çˆ¬è™«
ç”¨äºçˆ¬å–ä½¿ç”¨Ajaxå¼‚æ­¥åŠ è½½å†…å®¹çš„æ–°é—»/å…¬å‘Šé¡µé¢
"""
import asyncio
import aiohttp
from typing import List, Dict, Optional
from loguru import logger
from pathlib import Path
from fake_useragent import UserAgent

from config import Config
from parsers.dynamic_parser import DynamicPageParser
from core.storage import storage
from core.checkpoint import CheckpointManager
from core.crawl_queue import CrawlQueue, AdaptiveCrawlQueue


class DynamicNewsCrawler:
    """
    åŠ¨æ€æ–°é—»é¡µé¢çˆ¬è™«
    
    ä¸ BBSSpider å¹¶åˆ—ï¼Œç»§æ‰¿ç›¸åŒçš„è®¾è®¡ç†å¿µï¼š
    - å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†
    - ç»Ÿä¸€çš„ç»Ÿè®¡ä¿¡æ¯æ¥å£
    - å¯é…ç½®çš„è¯·æ±‚å¤´
    
    ç‰¹ç‚¹ï¼š
    - æ”¯æŒAjaxæ–¹å¼å¿«é€Ÿçˆ¬å–
    - æ”¯æŒSeleniumæ–¹å¼å¤‡ç”¨ï¼ˆå¯é æ€§é«˜ï¼‰
    - è‡ªåŠ¨å¤„ç†"æŸ¥çœ‹æ›´å¤š"åˆ†é¡µ
    - æ”¯æŒæ‰¹é‡ä¸‹è½½æ–‡ç« è¯¦æƒ…å’Œå›¾ç‰‡
    
    Example:
        config = Config(bbs={"base_url": "https://example.com"})
        crawler = DynamicNewsCrawler(config)
        
        async with crawler:
            articles = await crawler.crawl_dynamic_page_ajax(
                "https://example.com/news",
                max_pages=5
            )
    """
    
    def __init__(self, config: Config):
        """
        åˆå§‹åŒ–åŠ¨æ€æ–°é—»çˆ¬è™«
        
        Args:
            config: é…ç½®å¯¹è±¡
        """
        self.config = config
        self.parser = DynamicPageParser(config)
        self.session = None
        self.ua = None
        
        # ç»Ÿè®¡ä¿¡æ¯ï¼ˆä¸ BaseSpider ä¿æŒä¸€è‡´çš„ç»“æ„ï¼‰
        self.stats = {
            'pages_fetched': 0,       # åŸºç¡€ç»Ÿè®¡
            'requests_failed': 0,     # åŸºç¡€ç»Ÿè®¡
            'articles_found': 0,      # å‘ç°çš„æ–‡ç« æ•°
            'articles_crawled': 0,    # æˆåŠŸçˆ¬å–çš„æ–‡ç« æ•°
            'articles_failed': 0,     # å¤±è´¥çš„æ–‡ç« æ•°
            'images_downloaded': 0,   # ä¸‹è½½çš„å›¾ç‰‡æ•°
            'images_failed': 0,       # å¤±è´¥çš„å›¾ç‰‡æ•°
        }
        
        logger.info(f"ğŸš€ åˆå§‹åŒ–åŠ¨æ€æ–°é—»çˆ¬è™«: {config.bbs.name}")
    
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        await self.init()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        await self.close()
    
    async def init(self):
        """åˆå§‹åŒ–çˆ¬è™«ï¼ˆæ¥å…¥ Storageï¼Œä½¿ CheckpointManager è–„å°è£…å¯è¯»å†™ checkpoints è¡¨ï¼‰"""
        logger.info("âš™ï¸  åˆå§‹åŒ–çˆ¬è™«ç»„ä»¶...")
        storage.connect()
        self.ua = UserAgent()
        timeout = aiohttp.ClientTimeout(total=self.config.crawler.request_timeout)
        self.session = aiohttp.ClientSession(timeout=timeout)
        logger.debug("âœ“ HTTPä¼šè¯å·²åˆ›å»º")
    
    async def close(self):
        """å…³é—­çˆ¬è™«"""
        logger.info("ğŸ”’ å…³é—­çˆ¬è™«...")
        if self.session:
            await self.session.close()
            logger.debug("âœ“ HTTPä¼šè¯å·²å…³é—­")
        storage.close()
        logger.info(f"ğŸ“Š çˆ¬è™«ç»Ÿè®¡: {self.get_statistics()}")
    
    def get_headers(self) -> Dict[str, str]:
        """è·å–è¯·æ±‚å¤´"""
        return {
            "User-Agent": self.ua.random if self.config.crawler.rotate_user_agent else self.ua.chrome,
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Accept-Encoding": "gzip, deflate",
            "Connection": "keep-alive",
        }
    
    async def fetch_page(self, url: str, headers: Optional[Dict] = None, is_ajax: bool = False) -> Optional[str]:
        """
        è·å–é¡µé¢å†…å®¹
        
        Args:
            url: é¡µé¢URL
            headers: å¯é€‰çš„HTTPå¤´
            is_ajax: æ˜¯å¦ä¸ºAjaxè¯·æ±‚ï¼ˆä¼šæ·»åŠ X-Requested-Withå¤´ï¼‰
        
        Returns:
            HTMLå†…å®¹ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            logger.debug(f"ğŸ“„ è·å–é¡µé¢: {url} (Ajax: {is_ajax})")
            
            # è·å–åŸºç¡€è¯·æ±‚å¤´
            request_headers = self.get_headers()
            
            # åˆå¹¶è‡ªå®šä¹‰ headers
            if headers:
                request_headers.update(headers)
            
            # Ajaxè¯·æ±‚éœ€è¦ç‰¹æ®Šå¤´
            if is_ajax:
                request_headers["X-Requested-With"] = "XMLHttpRequest"
            
            async with self.session.get(url, headers=request_headers) as response:
                if response.status == 200:
                    self.stats['pages_fetched'] += 1
                    html = await response.text()
                    await asyncio.sleep(self.config.crawler.download_delay)
                    return html
                else:
                    logger.warning(f"âš ï¸  HTTP {response.status}: {url}")
                    return None
        
        except asyncio.TimeoutError:
            self.stats['requests_failed'] += 1
            logger.error(f"âŒ è¶…æ—¶: {url}")
            return None
        except Exception as e:
            self.stats['requests_failed'] += 1
            logger.error(f"âŒ è·å–å¤±è´¥ {url}: {e}")
            return None
    
    async def crawl_dynamic_page_ajax(
        self, 
        base_url: str, 
        max_pages: Optional[int] = None,
        resume: bool = True,
        start_page: Optional[int] = None
    ) -> List[Dict]:
        """
        ä½¿ç”¨Ajaxæ–¹å¼çˆ¬å–åŠ¨æ€é¡µé¢ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰
        
        é€šè¿‡ç›´æ¥è¯·æ±‚åˆ†é¡µURLï¼ˆå¦‚ ?page=2ï¼‰æ¥è·å–æ›´å¤šå†…å®¹ã€‚
        è¿™ç§æ–¹å¼é€Ÿåº¦å¿«ã€èµ„æºå ç”¨å°‘ã€‚
        
        Args:
            base_url: åŸºç¡€URL
            max_pages: æœ€å¤§é¡µæ•°ï¼ŒNoneè¡¨ç¤ºä¸é™åˆ¶
            resume: æ˜¯å¦ä»æ£€æŸ¥ç‚¹æ¢å¤ï¼ˆé»˜è®¤Trueï¼‰
            start_page: èµ·å§‹é¡µç ï¼ˆå¦‚æœæŒ‡å®šï¼Œä¼šè¦†ç›–æ£€æŸ¥ç‚¹ï¼‰
        
        Returns:
            æ–‡ç« åˆ—è¡¨
        """
        logger.info(f"ğŸš€ å¼€å§‹çˆ¬å–åŠ¨æ€é¡µé¢ï¼ˆAjaxæ–¹å¼ï¼‰")
        logger.info(f"   URL: {base_url}")
        logger.info(f"   æœ€å¤§é¡µæ•°: {max_pages if max_pages else 'ä¸é™åˆ¶'}")
        
        # 1. åˆ›å»ºæ£€æŸ¥ç‚¹ç®¡ç†å™¨
        from urllib.parse import urlparse
        parsed = urlparse(base_url)
        site = parsed.netloc or self.config.bbs.base_url
        checkpoint = CheckpointManager(site=site, board="news")
        
        # 2. ä»æ£€æŸ¥ç‚¹æ¢å¤
        seen_article_ids = set()
        start_page_num = 1
        min_article_id = None
        max_article_id = None
        
        if start_page is not None:
            # æ‰‹åŠ¨æŒ‡å®šèµ·å§‹é¡µï¼ˆè¦†ç›–æ£€æŸ¥ç‚¹ï¼‰
            start_page_num = start_page
            logger.info(f"ğŸ“Œ æ‰‹åŠ¨æŒ‡å®šèµ·å§‹é¡µ: {start_page_num}")
            if checkpoint.exists():
                logger.info("âš ï¸  å°†è¦†ç›–ç°æœ‰æ£€æŸ¥ç‚¹")
        elif resume and checkpoint.exists():
            # ä»æ£€æŸ¥ç‚¹æ¢å¤
            checkpoint_data = checkpoint.load_checkpoint()
            if checkpoint_data:
                status = checkpoint_data.get('status', 'running')
                if status == 'completed':
                    logger.info("âœ… è¯¥ä»»åŠ¡å·²å®Œæˆçˆ¬å–ï¼Œè·³è¿‡")
                    return []
                
                start_page_num = checkpoint_data.get('current_page', 1)
                seen_article_ids = checkpoint.get_seen_article_ids()
                min_article_id = checkpoint.get_min_article_id()
                max_article_id = checkpoint.get_max_article_id()
                
                logger.info(f"ğŸ”„ ä»æ£€æŸ¥ç‚¹æ¢å¤: ç¬¬ {start_page_num} é¡µ")
                logger.info(f"   å·²çˆ¬å–æ–‡ç« æ•°: {len(seen_article_ids)}")
                if min_article_id:
                    logger.info(f"   æœ€å°æ–‡ç« ID: {min_article_id} (å·²çˆ¬å–çš„æœ€æ—§æ–‡ç« )")
                if max_article_id:
                    logger.info(f"   æœ€å¤§æ–‡ç« ID: {max_article_id} (å·²çˆ¬å–çš„æœ€æ–°æ–‡ç« )")
                logger.info(f"   ç­–ç•¥: ä½¿ç”¨ç²¾ç¡®é›†åˆå»é‡ï¼ˆseen_article_idsï¼‰ï¼Œæ”¯æŒä¹±åºåˆ—è¡¨")
                logger.info(f"   ç­–ç•¥: çˆ¬å– > {max_article_id} çš„æ–°æ–‡ç« ï¼ˆå¦‚æœç½‘ç«™æœ‰æ›´æ–°ï¼‰")
                logger.info(f"   ç­–ç•¥: ç»§ç»­çˆ¬å–æ‰€æœ‰æœªåœ¨é›†åˆä¸­çš„æ–‡ç« ï¼ˆä¸ä¾èµ–IDèŒƒå›´ï¼‰")
        
        page = start_page_num
        all_articles = []
        consecutive_no_new = 0  # è¿ç»­æ— æ–°æ–‡ç« çš„é¡µæ•°ï¼ˆç”¨äºå®‰å…¨åœæ­¢ï¼Œé¿å…åˆ†é¡µå¾ªç¯æ—¶æ— é™è¯·æ±‚ï¼‰
        max_consecutive_no_new = 10  # è¿ç»­ N é¡µæ— æ–°æ–‡ç« åˆ™åœæ­¢
        stopped_early = False  # æ˜¯å¦å› è¿ç»­æ— æ–°æ–‡ç« è€Œæå‰åœæ­¢ï¼ˆä¸æ ‡è®°ä¸º completedï¼Œä¾¿äºä¸‹æ¬¡ç»§ç»­ï¼‰
        
        try:
            while True:
                # æ„é€ åˆ†é¡µURL
                if page == 1:
                    page_url = base_url
                else:
                    # å°è¯•å¤šç§åˆ†é¡µURLæ ¼å¼
                    separator = '&' if '?' in base_url else '?'
                    page_url = f"{base_url}{separator}page={page}"
                
                logger.info(f"\nğŸ“„ çˆ¬å–ç¬¬ {page} é¡µ: {page_url}")
                
                # è·å–é¡µé¢å†…å®¹ï¼ˆåˆ†é¡µè¯·æ±‚éœ€è¦Ajaxå¤´ï¼‰
                html = await self.fetch_page(page_url, is_ajax=(page > 1))
                
                if not html:
                    logger.warning(f"âš ï¸  ç¬¬{page}é¡µè·å–å¤±è´¥ï¼Œåœæ­¢çˆ¬å–")
                    break
                
                # è§£ææ–‡ç« åˆ—è¡¨
                articles = self.parser.parse_articles(html)
                
                if not articles:
                    logger.info(f"âœ… ç¬¬{page}é¡µæ²¡æœ‰æ–‡ç« ï¼Œåœæ­¢çˆ¬å–")
                    break
            
                # è¿‡æ»¤é‡å¤æ–‡ç« ï¼ˆä¸ BBS ç»Ÿä¸€ï¼šStorage ä¸ºæƒå¨ï¼ŒCheckpoint ä¸ºæœ¬è½®+æ–­ç‚¹æ¢å¤ï¼‰
                # 1) å…ˆæŸ¥ Storage.article_existsï¼ˆè·¨ä»»åŠ¡æƒå¨ï¼Œä¸ thread_exists å¯¹ç§°ï¼‰
                # 2) å†æŸ¥ seen_article_idsï¼ˆæœ¬è½® + æ–­ç‚¹æ¢å¤çš„é›†åˆï¼‰
                new_articles = []
                has_new_articles_beyond_max = False
                
                for article in articles:
                    article_id = article['article_id']
                    
                    if storage.article_exists(article_id):
                        logger.debug(f"â­ï¸  è·³è¿‡å·²çˆ¬å–æ–‡ç« : {article_id} (Storage å·²å­˜åœ¨)")
                        seen_article_ids.add(article_id)  # åŒæ­¥åˆ°æœ¬è½®é›†åˆï¼Œé¿å…é‡å¤æŸ¥åº“
                        continue
                    if article_id in seen_article_ids:
                        logger.debug(f"â­ï¸  è·³è¿‡å·²çˆ¬å–æ–‡ç« : {article_id} (æœ¬è½®å·²è§)")
                        continue
                    
                    seen_article_ids.add(article_id)
                    new_articles.append(article)
                    # æŒä¹…åŒ–åˆ° Storageï¼Œä¸‹æ¬¡è¿è¡Œå¯ç›´æ¥ article_exists è·³è¿‡ï¼ˆä¸ save_thread å¯¹ç§°ï¼‰
                    storage.save_article({**article, "site": site, "board": "news"})
                    
                    # æ›´æ–°æœ€å°/æœ€å¤§ article_idï¼ˆç”¨äºç»Ÿè®¡å’Œæ—¥å¿—ï¼Œä¸ç”¨äºå»é‡ï¼‰
                    try:
                        article_id_int = int(article_id)
                        
                        # æ£€æµ‹æ˜¯å¦æœ‰è¶…è¿‡å½“å‰æœ€å¤§IDçš„æ–°æ–‡ç« ï¼ˆç½‘ç«™æœ‰æ›´æ–°ï¼‰
                        if max_article_id and article_id_int > int(max_article_id):
                            logger.info(f"ğŸ†• å‘ç°æ–°æ–‡ç« : {article_id} (>{max_article_id})ï¼Œç½‘ç«™æœ‰æ›´æ–°ï¼")
                            has_new_articles_beyond_max = True
                        
                        # æ›´æ–°æœ€å°/æœ€å¤§ID
                        if not min_article_id or article_id_int < int(min_article_id):
                            min_article_id = article_id
                        if not max_article_id or article_id_int > int(max_article_id):
                            old_max = max_article_id
                            max_article_id = article_id
                            if old_max:
                                logger.info(f"ğŸ“ˆ æ›´æ–°æœ€å¤§æ–‡ç« ID: {old_max} -> {max_article_id}")
                    except (ValueError, TypeError):
                        # article_id ä¸æ˜¯æ•°å­—ï¼Œè·³è¿‡IDèŒƒå›´æ›´æ–°
                        pass
                
                # å¦‚æœå‘ç°æ–°æ–‡ç« ï¼Œè®°å½•æ—¥å¿—
                if has_new_articles_beyond_max:
                    logger.info(f"âœ¨ æ£€æµ‹åˆ°ç½‘ç«™æœ‰æ–°æ–‡ç« å‘å¸ƒï¼Œå·²å¼€å§‹çˆ¬å–æ–°å†…å®¹")
                
                if not new_articles:
                    # æœ¬é¡µå…¨éƒ¨é‡å¤ï¼šä¸ç«‹å³åœæ­¢ï¼Œç»§ç»­è¯·æ±‚ä¸‹ä¸€é¡µï¼ˆæ”¯æŒä¸Šåƒé¡µçš„é•¿åˆ—è¡¨ï¼‰
                    # ä»…å½“æ•´é¡µæ— æ–‡ç« ï¼ˆç©ºé¡µï¼‰æ—¶æ‰åœæ­¢ï¼Œè§ä¸Šæ–¹ if not articles: break
                    consecutive_no_new += 1
                    logger.info(f"   ç¬¬{page}é¡µæ— æ–°æ–‡ç« ï¼ˆæœ¬é¡µ {len(articles)} ç¯‡å‡é‡å¤ï¼‰ï¼Œç»§ç»­ä¸‹ä¸€é¡µ")
                    if consecutive_no_new >= max_consecutive_no_new:
                        logger.info(f"âœ… è¿ç»­ {max_consecutive_no_new} é¡µæ— æ–°æ–‡ç« ï¼Œåœæ­¢çˆ¬å–ï¼ˆå¯èƒ½å·²åˆ°æœ«å°¾æˆ–åˆ†é¡µå¾ªç¯ï¼‰")
                        stopped_early = True
                        # ä¿å­˜æ£€æŸ¥ç‚¹ä»¥ä¾¿ä¸‹æ¬¡ä»å½“å‰é¡µç»§ç»­ï¼ˆä¿æŒ status=runningï¼Œä¸æ ‡è®° completedï¼‰
                        checkpoint.save_checkpoint(
                            current_page=page + 1,
                            last_thread_id=all_articles[-1]['article_id'] if all_articles else None,
                            last_thread_url=all_articles[-1].get('url') if all_articles else None,
                            status="running",
                            stats={"articles_found": len(all_articles), "articles_crawled": self.stats.get('articles_crawled', 0), "images_downloaded": self.stats.get('images_downloaded', 0)},
                            seen_article_ids=list(seen_article_ids),
                            min_article_id=min_article_id,
                            max_article_id=max_article_id
                        )
                        break
                else:
                    consecutive_no_new = 0  # æœ‰æ–°æ–‡ç« åˆ™é‡ç½®è®¡æ•°
                    logger.info(f"   âœ“ å‘ç° {len(new_articles)} ç¯‡æ–°æ–‡ç«  (æœ¬é¡µå…± {len(articles)} ç¯‡)")
                    all_articles.extend(new_articles)
                    self.stats['articles_found'] += len(new_articles)
                
                # 3. ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆæ¯é¡µä¿å­˜ä¸€æ¬¡ï¼Œæ— æ–°æ–‡ç« æ—¶ä¹Ÿæ¨è¿›é¡µç ï¼‰
                checkpoint.save_checkpoint(
                    current_page=page + 1,
                    last_thread_id=(new_articles[-1]['article_id'] if new_articles else None) or (all_articles[-1]['article_id'] if all_articles else None),
                    last_thread_url=(new_articles[-1].get('url') if new_articles else None) or (all_articles[-1].get('url') if all_articles else None),
                    status="running",
                    stats={
                        "articles_found": len(all_articles),
                        "articles_crawled": self.stats.get('articles_crawled', 0),
                        "images_downloaded": self.stats.get('images_downloaded', 0)
                    },
                    seen_article_ids=list(seen_article_ids),
                    min_article_id=min_article_id,
                    max_article_id=max_article_id
                )
                
                # æ£€æŸ¥é¡µæ•°é™åˆ¶
                if max_pages and page >= max_pages:
                    logger.info(f"âœ… è¾¾åˆ°æœ€å¤§é¡µæ•°é™åˆ¶: {max_pages}")
                    break
                
                # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰"æŸ¥çœ‹æ›´å¤š"æŒ‰é’® (è¾…åŠ©åˆ¤æ–­)
                has_more = self.parser.has_load_more_button(html)
                if not has_more:
                    logger.info("âœ… æ²¡æœ‰æ›´å¤šå†…å®¹æ ‡è¯†ï¼Œåœæ­¢çˆ¬å–")
                    break
                
                page += 1
            
            # 4. ä»…åœ¨æ­£å¸¸è·‘å®Œæ—¶æ ‡è®°å®Œæˆï¼ˆæå‰åœæ­¢æ—¶ä¿æŒ runningï¼Œä¾¿äºä¸‹æ¬¡ç»§ç»­ï¼‰
            if not stopped_early:
                checkpoint.mark_completed(final_stats={
                    "total_articles": len(all_articles),
                    "total_images": self.stats.get('images_downloaded', 0)
                })
            
            logger.success(f"ğŸ‰ å®Œæˆçˆ¬å–ï¼æ€»å…±å‘ç° {len(all_articles)} ç¯‡æ–°æ–‡ç« ")
            
            return all_articles
            
        except Exception as e:
            # å‘ç”Ÿé”™è¯¯æ—¶ä¿å­˜æ£€æŸ¥ç‚¹
            logger.error(f"âŒ çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            checkpoint.mark_error(str(e))
            raise
    
    async def crawl_dynamic_page_selenium(
        self, 
        url: str, 
        max_clicks: Optional[int] = None
    ) -> List[Dict]:
        """
        ä½¿ç”¨Seleniumæ–¹å¼çˆ¬å–åŠ¨æ€é¡µé¢ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
        
        é€šè¿‡æ¨¡æ‹Ÿæµè§ˆå™¨ç‚¹å‡»"æŸ¥çœ‹æ›´å¤š"æŒ‰é’®æ¥åŠ è½½å†…å®¹ã€‚
        è¿™ç§æ–¹å¼æ›´å¯é ï¼Œä½†é€Ÿåº¦è¾ƒæ…¢ã€èµ„æºå ç”¨å¤§ã€‚
        
        Args:
            url: é¡µé¢URL
            max_clicks: æœ€å¤§ç‚¹å‡»æ¬¡æ•°ï¼ŒNoneè¡¨ç¤ºä¸é™åˆ¶
        
        Returns:
            æ–‡ç« åˆ—è¡¨
        """
        logger.info(f"ğŸš€ å¼€å§‹çˆ¬å–åŠ¨æ€é¡µé¢ï¼ˆSeleniumæ–¹å¼ï¼‰")
        logger.info(f"   URL: {url}")
        logger.info(f"   æœ€å¤§ç‚¹å‡»æ¬¡æ•°: {max_clicks if max_clicks else 'ä¸é™åˆ¶'}")
        
        try:
            from selenium import webdriver
            from selenium.webdriver.common.by import By
            from selenium.webdriver.support.ui import WebDriverWait
            from selenium.webdriver.support import expected_conditions as EC
            from selenium.common.exceptions import TimeoutException
        except ImportError:
            logger.error("âŒ ç¼ºå°‘Seleniumä¾èµ–ï¼Œè¯·å®‰è£…: pip install selenium")
            return []
        
        # é…ç½®Selenium
        options = webdriver.ChromeOptions()
        options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        
        driver = None
        
        try:
            driver = webdriver.Chrome(options=options)
            driver.get(url)
            
            logger.info("âœ“ æµè§ˆå™¨å·²å¯åŠ¨")
            
            clicks = 0
            last_article_count = 0
            
            while True:
                html = driver.page_source
                current_articles = self.parser.parse_articles(html)
                current_count = len(current_articles)
                logger.debug(f"å½“å‰æ–‡ç« æ•°: {current_count}")
                
                if clicks == 0:
                    last_article_count = current_count
                
                if max_clicks and clicks >= max_clicks:
                    logger.info(f"âœ… è¾¾åˆ°æœ€å¤§ç‚¹å‡»æ¬¡æ•°: {max_clicks}")
                    break
                
                try:
                    load_more = WebDriverWait(driver, 10).until(
                        EC.presence_of_element_located((
                            By.CSS_SELECTOR, 
                            'a.more, .load-more, .btn-more'
                        ))
                    )
                    
                    if not load_more.is_displayed():
                        logger.info("âš ï¸  'æŸ¥çœ‹æ›´å¤š'æŒ‰é’®ä¸å¯è§ï¼Œå¯èƒ½å·²åŠ è½½å®Œæ¯•")
                        break
                    
                    driver.execute_script("arguments[0].scrollIntoView({behavior: 'smooth', block: 'center'});", load_more)
                    await asyncio.sleep(1)
                    
                    driver.execute_script("arguments[0].click();", load_more)
                    clicks += 1
                    logger.info(f"ğŸ”„ ç‚¹å‡»'æŸ¥çœ‹æ›´å¤š' ç¬¬{clicks}æ¬¡")
                    
                    wait_time = 0
                    loaded = False
                    while wait_time < 10:
                        await asyncio.sleep(1)
                        wait_time += 1
                        
                        new_html = driver.page_source
                        new_articles = self.parser.parse_articles(new_html)
                        new_count = len(new_articles)
                        
                        if new_count > last_article_count:
                            logger.info(f"   âœ“ åŠ è½½æˆåŠŸï¼æ–°å¢ {new_count - last_article_count} ç¯‡æ–‡ç« ")
                            last_article_count = new_count
                            loaded = True
                            break
                    
                    if not loaded:
                        logger.warning(f"âš ï¸  ç­‰å¾…10ç§’åæ–‡ç« æ•°é‡æœªå¢åŠ ")
                        
                except TimeoutException:
                    logger.info("âœ… æ²¡æœ‰æ‰¾åˆ°'æŸ¥çœ‹æ›´å¤š'æŒ‰é’®ï¼Œåœæ­¢åŠ è½½")
                    break
                except Exception as e:
                    logger.error(f"âŒ ç‚¹å‡»è¿‡ç¨‹å‡ºé”™: {e}")
                    break
            
            html = driver.page_source
            articles = self.parser.parse_articles(html)
            self.stats['articles_found'] = len(articles)
            
            logger.success(f"ğŸ‰ å®Œæˆçˆ¬å–ï¼æ€»å…±å‘ç° {len(articles)} ç¯‡æ–‡ç« ")
            
            return articles
            
        except Exception as e:
            logger.error(f"âŒ Seleniumçˆ¬å–å¤±è´¥: {e}")
            return []
        
        finally:
            if driver:
                driver.quit()
                logger.debug("âœ“ æµè§ˆå™¨å·²å…³é—­")
    
    async def crawl_article_detail(self, article: Dict) -> Optional[Dict]:
        """çˆ¬å–å•ç¯‡æ–‡ç« è¯¦æƒ…"""
        url = article.get('url')
        if not url:
            logger.warning("âš ï¸  æ–‡ç« ç¼ºå°‘URLï¼Œè·³è¿‡")
            return None
        
        logger.info(f"ğŸ“ çˆ¬å–æ–‡ç« è¯¦æƒ…: {article.get('title', 'N/A')[:50]}")
        
        try:
            html = await self.fetch_page(url)
            
            if not html:
                logger.error(f"âŒ æ— æ³•è·å–æ–‡ç« è¯¦æƒ…: {url}")
                self.stats['articles_failed'] += 1
                return None
            
            detail = self.parser.parse_article_detail(html, url)
            full_article = {**article, **detail}
            
            self.stats['articles_crawled'] += 1
            logger.success(f"   âœ“ æˆåŠŸï¼Œå‘ç° {len(detail.get('images', []))} å¼ å›¾ç‰‡")
            
            return full_article
            
        except Exception as e:
            logger.error(f"âŒ çˆ¬å–æ–‡ç« è¯¦æƒ…å¤±è´¥: {e}")
            self.stats['articles_failed'] += 1
            return None
    
    async def crawl_articles_batch(
        self, 
        articles: List[Dict],
        use_queue: bool = True,
        max_workers: Optional[int] = None,
        use_adaptive: bool = False
    ) -> List[Dict]:
        """
        æ‰¹é‡çˆ¬å–æ–‡ç« è¯¦æƒ…ï¼ˆæ”¯æŒå¼‚æ­¥é˜Ÿåˆ—ï¼‰
        
        Args:
            articles: æ–‡ç« åˆ—è¡¨
            use_queue: æ˜¯å¦ä½¿ç”¨å¼‚æ­¥é˜Ÿåˆ—ï¼ˆé»˜è®¤Trueï¼‰
            max_workers: æ¶ˆè´¹è€…ï¼ˆworkerï¼‰çš„æ•°é‡ï¼Œå³å¹¶å‘çˆ¬å–æ–‡ç« çš„çº¿ç¨‹æ•°
                        æ³¨æ„ï¼šç”Ÿäº§è€…åªæœ‰ä¸€ä¸ªï¼Œæ¶ˆè´¹è€…æœ‰ max_workers ä¸ª
            use_adaptive: æ˜¯å¦ä½¿ç”¨è‡ªé€‚åº”é˜Ÿåˆ—ï¼ˆé»˜è®¤Falseï¼‰
        
        Returns:
            æ–‡ç« è¯¦æƒ…åˆ—è¡¨
        """
        logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡çˆ¬å– {len(articles)} ç¯‡æ–‡ç« è¯¦æƒ…")
        
        if not use_queue:
            # ä½¿ç”¨ä¼ ç»Ÿçš„ asyncio.gatherï¼ˆå…¼å®¹æ¨¡å¼ï¼‰
            tasks = [self.crawl_article_detail(article) for article in articles]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            full_articles = [r for r in results if r and not isinstance(r, Exception)]
        else:
            # ä½¿ç”¨å¼‚æ­¥é˜Ÿåˆ—
            workers = max_workers or self.config.crawler.max_concurrent_requests or 5
            queue_size = self.config.crawler.queue_size or 1000
            
            if use_adaptive:
                queue = AdaptiveCrawlQueue(
                    initial_workers=workers,
                    max_workers=workers * 2,
                    min_workers=1,
                    queue_size=queue_size
                )
                logger.info(f"ğŸ¯ ä½¿ç”¨è‡ªé€‚åº”é˜Ÿåˆ—çˆ¬å–æ–‡ç« è¯¦æƒ…: åˆå§‹å¹¶å‘={workers}")
            else:
                queue = CrawlQueue(max_workers=workers, queue_size=queue_size)
                logger.info(f"ğŸš€ ä½¿ç”¨å¼‚æ­¥é˜Ÿåˆ—çˆ¬å–æ–‡ç« è¯¦æƒ…: å¹¶å‘æ•°={workers}")
            
            # ä½¿ç”¨å…±äº«åˆ—è¡¨æ”¶é›†ç»“æœ
            results_container = []
            
            async def crawl_article_task_with_result(article: Dict):
                result = await self.crawl_article_detail(article)
                if result:
                    results_container.append(result)
                return result
            
            # è¿è¡Œé˜Ÿåˆ—
            await queue.run(articles, crawl_article_task_with_result)
            full_articles = results_container
        
        logger.success(f"âœ… æˆåŠŸçˆ¬å– {len(full_articles)}/{len(articles)} ç¯‡æ–‡ç« è¯¦æƒ…")
        
        return full_articles
    
    def get_statistics(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return self.stats.copy()
