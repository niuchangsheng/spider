"""
BBSè®ºå›çˆ¬è™«æ¨¡å—

åŒ…å«:
- BBSSpider: BBSè®ºå›çˆ¬è™«åŸºç±»
- DiscuzSpider: Discuzè®ºå›çˆ¬è™«
- PhpBBSpider: phpBBè®ºå›çˆ¬è™«
- VBulletinSpider: vBulletinè®ºå›çˆ¬è™«
"""
from typing import List, Dict, Any, Optional
from pathlib import Path
from loguru import logger
from tqdm import tqdm

from spiders.base import BaseSpider
from core.downloader import ImageDownloader
from core.storage import storage
from core.deduplicator import ImageDeduplicator
from core.checkpoint import CheckpointManager
from core.crawl_queue import CrawlQueue, AdaptiveCrawlQueue
from parsers.bbs_parser import BBSParser
from config import Config, ConfigLoader


class BBSSpider(BaseSpider):
    """
    BBSè®ºå›å›¾ç‰‡çˆ¬è™«
    
    ç»§æ‰¿ BaseSpiderï¼Œæ·»åŠ è®ºå›ç‰¹æœ‰åŠŸèƒ½ï¼š
    - å¸–å­åˆ—è¡¨çˆ¬å–
    - å¸–å­è¯¦æƒ…çˆ¬å–
    - å›¾ç‰‡ä¸‹è½½å’Œå»é‡
    
    å­ç±»ï¼ˆå¦‚ DiscuzSpiderï¼‰å¯é‡å†™ process_images() å®ç°è®ºå›ç‰¹å®šå¤„ç†
    """
    
    def __init__(self, config: Optional[Config] = None, url: Optional[str] = None, preset: Optional[str] = None):
        """
        åˆå§‹åŒ–BBSçˆ¬è™«
        
        Args:
            config: æ‰‹åŠ¨é…ç½®ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
            url: è®ºå›URLï¼Œè‡ªåŠ¨æ£€æµ‹é…ç½®
            preset: è®ºå›ç±»å‹é¢„è®¾ (discuz/phpbb/vbulletin)
        
        Note:
            æ¨èä½¿ç”¨ SpiderFactory.create() è€Œä¸æ˜¯ç›´æ¥å®ä¾‹åŒ–
        """
        # é…ç½®ä¼˜å…ˆçº§: config > preset > url
        if config:
            final_config = config
        elif preset:
            final_config = ConfigLoader.load(preset)
        elif url:
            final_config = ConfigLoader.auto_detect(url)
        else:
            raise ValueError("å¿…é¡»æä¾› configã€preset æˆ– url å‚æ•°ä¹‹ä¸€")
        
        # è°ƒç”¨åŸºç±»åˆå§‹åŒ–
        super().__init__(final_config)
        
        # BBSç‰¹æœ‰ç»„ä»¶
        self.parser = BBSParser()
        self.deduplicator = ImageDeduplicator(use_perceptual_hash=True)
        
        # BBSç‰¹æœ‰ç»Ÿè®¡ä¿¡æ¯ï¼ˆæ‰©å±•åŸºç±»statsï¼‰
        self.stats.update({
            "threads_crawled": 0,
            "images_found": 0,
            "images_downloaded": 0,
            "images_failed": 0,
            "duplicates_skipped": 0
        })
        
        logger.info(f"ğŸš€ åˆå§‹åŒ–çˆ¬è™«: {self.config.bbs.name} ({self.config.bbs.forum_type})")
    
    async def init(self):
        """åˆå§‹åŒ–BBSçˆ¬è™«"""
        # è°ƒç”¨åŸºç±»åˆå§‹åŒ–
        await super().init()
        
        # BBSç‰¹æœ‰åˆå§‹åŒ–
        storage.connect()
        
        # åŠ è½½å·²å­˜åœ¨çš„æ–‡ä»¶å“ˆå¸Œ
        if self.config.image.enable_deduplication:
            self.deduplicator.load_existing_hashes(self.config.image.download_dir)
        
        logger.success("âœ… çˆ¬è™«åˆå§‹åŒ–å®Œæˆ")
    
    async def close(self):
        """å…³é—­BBSçˆ¬è™«"""
        # BBSç‰¹æœ‰æ¸…ç†
        storage.close()
        
        # è¾“å‡ºå»é‡ç»Ÿè®¡
        logger.info(f"ğŸ”„ å»é‡ç»Ÿè®¡: {self.deduplicator.get_stats()}")
        
        # è°ƒç”¨åŸºç±»å…³é—­
        await super().close()
    
    async def process_images(self, images: List[str]) -> List[str]:
        """
        å¤„ç†å›¾ç‰‡URLï¼ˆé’©å­æ–¹æ³•ï¼‰
        
        å­ç±»å¯é‡å†™æ­¤æ–¹æ³•å®ç°è®ºå›ç‰¹å®šçš„å›¾ç‰‡å¤„ç†é€»è¾‘
        
        Args:
            images: åŸå§‹å›¾ç‰‡URLåˆ—è¡¨
        
        Returns:
            å¤„ç†åçš„å›¾ç‰‡URLåˆ—è¡¨
        """
        return images
    
    async def crawl_board(
        self, 
        board_url: str, 
        board_name: str, 
        max_pages: Optional[int] = None,
        resume: bool = True,
        start_page: Optional[int] = None
    ):
        """
        çˆ¬å–æ¿å—ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰
        
        Args:
            board_url: æ¿å—URL
            board_name: æ¿å—åç§°
            max_pages: æœ€å¤§é¡µæ•°
            resume: æ˜¯å¦ä»æ£€æŸ¥ç‚¹æ¢å¤ï¼ˆé»˜è®¤Trueï¼‰
            start_page: èµ·å§‹é¡µç ï¼ˆå¦‚æœæŒ‡å®šï¼Œä¼šè¦†ç›–æ£€æŸ¥ç‚¹ï¼‰
        """
        logger.info(f"ğŸ“š å¼€å§‹çˆ¬å–æ¿å—: {board_name}")
        
        # 1. åˆ›å»ºæ£€æŸ¥ç‚¹ç®¡ç†å™¨
        checkpoint = CheckpointManager(
            site=self.config.bbs.base_url,
            board=board_name
        )
        
        # 2. ç¡®å®šèµ·å§‹é¡µ
        start_from_checkpoint = False
        if start_page is not None:
            # æ‰‹åŠ¨æŒ‡å®šèµ·å§‹é¡µï¼ˆè¦†ç›–æ£€æŸ¥ç‚¹ï¼‰
            start_page_num = start_page
            logger.info(f"ğŸ“Œ æ‰‹åŠ¨æŒ‡å®šèµ·å§‹é¡µ: {start_page_num}")
            if checkpoint.exists():
                logger.info("âš ï¸  å°†è¦†ç›–ç°æœ‰æ£€æŸ¥ç‚¹")
        elif resume and checkpoint.exists():
            # ä»æ£€æŸ¥ç‚¹æ¢å¤
            checkpoint_data = checkpoint.load_checkpoint()
            if checkpoint_data:
                status = checkpoint_data.get('status', 'running')
                if status == 'completed':
                    logger.info("âœ… è¯¥æ¿å—å·²å®Œæˆçˆ¬å–ï¼Œè·³è¿‡")
                    return
                
                # æ£€æŸ¥ç‚¹ä¿å­˜çš„æ˜¯"å½“å‰é¡µ"ï¼ˆåˆšçˆ¬å®Œçš„é¡µï¼‰ï¼Œæ¢å¤æ—¶ä»ä¸‹ä¸€é¡µå¼€å§‹
                checkpoint_page = checkpoint_data.get('current_page', 1)
                start_page_num = checkpoint_page
                last_thread_id = checkpoint_data.get('last_thread_id')
                logger.info(f"ğŸ”„ ä»æ£€æŸ¥ç‚¹æ¢å¤: ç¬¬ {start_page_num} é¡µ")
                if last_thread_id:
                    logger.info(f"   æœ€åçˆ¬å–çš„å¸–å­ID: {last_thread_id}")
                start_from_checkpoint = True
            else:
                start_page_num = 1
        else:
            # ä»å¤´å¼€å§‹
            start_page_num = 1
        
        # 3. ä»èµ·å§‹é¡µå¼€å§‹çˆ¬å–
        current_url = board_url
        page_count = 0
        last_thread_id = None
        last_thread_url = None
        
        # å¦‚æœä»æ£€æŸ¥ç‚¹æ¢å¤ä¸”èµ·å§‹é¡µ>1ï¼Œéœ€è¦è·³è½¬åˆ°æŒ‡å®šé¡µ
        # æ³¨æ„ï¼šä¸åŒè®ºå›çš„åˆ†é¡µæ–¹å¼ä¸åŒï¼Œè¿™é‡Œé‡‡ç”¨ç®€å•ç­–ç•¥ï¼š
        # ä»ç¬¬ä¸€é¡µå¼€å§‹ï¼Œé€šè¿‡"ä¸‹ä¸€é¡µ"é“¾æ¥åˆ°è¾¾æŒ‡å®šé¡µï¼ˆä¼šè·³è¿‡å·²çˆ¬çš„å¸–å­ï¼‰
        if start_from_checkpoint and start_page_num > 1:
            logger.info(f"â© ä»æ£€æŸ¥ç‚¹æ¢å¤ï¼Œéœ€è¦è·³è½¬åˆ°ç¬¬ {start_page_num} é¡µ")
            logger.info(f"   æç¤ºï¼šå°†ä»ç¬¬ä¸€é¡µå¼€å§‹ï¼Œé€šè¿‡'ä¸‹ä¸€é¡µ'é“¾æ¥åˆ°è¾¾æŒ‡å®šé¡µ")
            logger.info(f"   å·²çˆ¬å–çš„å¸–å­ä¼šè‡ªåŠ¨è·³è¿‡ï¼ˆé€šè¿‡å»é‡æœºåˆ¶ï¼‰")
        
        try:
            while current_url and (max_pages is None or page_count < max_pages):
                page_count += 1
                
                # è®¡ç®—å®é™…é¡µç 
                if start_from_checkpoint and start_page_num > 1:
                    # ä»æ£€æŸ¥ç‚¹æ¢å¤ï¼šå‰å‡ é¡µè·³è¿‡ï¼Œåˆ°è¾¾æŒ‡å®šé¡µåå¼€å§‹çˆ¬å–
                    actual_page = page_count
                    if actual_page < start_page_num:
                        # è·³è¿‡å·²çˆ¬é¡µï¼ŒåªæŸ¥æ‰¾ä¸‹ä¸€é¡µ
                        logger.debug(f"â­ï¸  è·³è¿‡ç¬¬ {actual_page} é¡µï¼ˆå·²çˆ¬å–ï¼‰")
                        html = await self.fetch_page(current_url)
                        if html:
                            current_url = self.parser.find_next_page(html, current_url)
                            if not current_url:
                                logger.warning("âš ï¸  æ— æ³•æ‰¾åˆ°ä¸‹ä¸€é¡µï¼Œå¯èƒ½å·²åˆ°è¾¾æœ€åä¸€é¡µ")
                                break
                        continue
                    actual_page = page_count
                else:
                    # æ­£å¸¸æƒ…å†µï¼šä»æŒ‡å®šé¡µæˆ–ç¬¬1é¡µå¼€å§‹
                    actual_page = start_page_num + page_count - 1 if start_page_num > 1 else page_count
                
                logger.info(f"ğŸ“„ çˆ¬å–ç¬¬ {actual_page} é¡µ: {current_url}")
                
                # è·å–åˆ—è¡¨é¡µ
                html = await self.fetch_page(current_url)
                if not html:
                    checkpoint.mark_error("æ— æ³•è·å–é¡µé¢")
                    logger.error(f"âŒ æ— æ³•è·å–ç¬¬ {actual_page} é¡µ")
                    break
                
                # è§£æå¸–å­åˆ—è¡¨
                threads = self.parser.parse_thread_list(html, current_url)
                logger.info(f"âœ… å‘ç° {len(threads)} ä¸ªå¸–å­")
                
                if not threads:
                    logger.warning(f"âš ï¸  ç¬¬ {actual_page} é¡µæ²¡æœ‰æ‰¾åˆ°å¸–å­")
                    # ä¿å­˜æ£€æŸ¥ç‚¹åç»§ç»­ä¸‹ä¸€é¡µ
                    checkpoint.save_checkpoint(
                        current_page=actual_page + 1,
                        last_thread_id=last_thread_id,
                        last_thread_url=last_thread_url,
                        status="running",
                        stats={
                            "crawled_count": self.stats['threads_crawled'],
                            "failed_count": self.stats['images_failed'],
                            "images_downloaded": self.stats['images_downloaded']
                        }
                    )
                    # æŸ¥æ‰¾ä¸‹ä¸€é¡µ
                    current_url = self.parser.find_next_page(html, current_url)
                    if not current_url:
                        break
                    continue
                
                # ä½¿ç”¨å¼‚æ­¥ä»»åŠ¡é˜Ÿåˆ—å¹¶å‘çˆ¬å–å¸–å­ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if self.config.crawler.use_async_queue:
                    # å‡†å¤‡ä»»åŠ¡åˆ—è¡¨
                    thread_tasks = []
                    for thread in threads:
                        thread['board'] = board_name
                        thread_tasks.append(thread)
                    
                    # åˆ›å»ºé˜Ÿåˆ—å¹¶è¿è¡Œ
                    max_workers = self.config.crawler.max_concurrent_requests or 5
                    use_adaptive = self.config.crawler.use_adaptive_queue
                    queue_size = self.config.crawler.queue_size or 1000
                    
                    if use_adaptive:
                        queue = AdaptiveCrawlQueue(
                            initial_workers=max_workers,
                            max_workers=max_workers * 2,
                            min_workers=1,
                            queue_size=queue_size
                        )
                        logger.info(f"ğŸ¯ ä½¿ç”¨è‡ªé€‚åº”é˜Ÿåˆ—: åˆå§‹å¹¶å‘={max_workers}")
                    else:
                        queue = CrawlQueue(max_workers=max_workers, queue_size=queue_size)
                        logger.info(f"ğŸš€ ä½¿ç”¨å¼‚æ­¥é˜Ÿåˆ—: å¹¶å‘æ•°={max_workers}")
                    
                    # å®šä¹‰å·¥ä½œå‡½æ•°
                    async def crawl_thread_task(thread_info: Dict[str, Any]):
                        """é˜Ÿåˆ—å·¥ä½œå‡½æ•°"""
                        await self.crawl_thread(thread_info)
                        return thread_info
                    
                    # è¿è¡Œé˜Ÿåˆ—
                    queue_stats = await queue.run(thread_tasks, crawl_thread_task)
                    logger.info(f"ğŸ“Š é˜Ÿåˆ—ç»Ÿè®¡: {queue_stats}")
                else:
                    # ä¸²è¡Œçˆ¬å–ï¼ˆå…¼å®¹æ¨¡å¼ï¼‰
                    logger.debug("ğŸ“ ä½¿ç”¨ä¸²è¡Œæ¨¡å¼çˆ¬å–å¸–å­")
                    for thread in threads:
                        thread['board'] = board_name
                        await self.crawl_thread(thread)
                
                # æ›´æ–°æœ€åçˆ¬å–çš„å¸–å­ä¿¡æ¯
                if threads:
                    last_thread_id = threads[-1].get('thread_id')
                    last_thread_url = threads[-1].get('url')
                
                # 4. ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆæ¯é¡µä¿å­˜ä¸€æ¬¡ï¼‰
                checkpoint.save_checkpoint(
                    current_page=actual_page + 1,  # ä¸‹ä¸€é¡µ
                    last_thread_id=last_thread_id,
                    last_thread_url=last_thread_url,
                    status="running",
                    stats={
                        "crawled_count": self.stats['threads_crawled'],
                        "failed_count": self.stats['images_failed'],
                        "images_downloaded": self.stats['images_downloaded']
                    }
                )
                
                # æŸ¥æ‰¾ä¸‹ä¸€é¡µ
                current_url = self.parser.find_next_page(html, current_url)
                if not current_url:
                    logger.info("ğŸ“Œ æ²¡æœ‰æ›´å¤šé¡µé¢")
                    break
            
            # 5. æ ‡è®°å®Œæˆ
            checkpoint.mark_completed(final_stats={
                "total_crawled": self.stats['threads_crawled'],
                "total_images": self.stats['images_downloaded'],
                "total_failed": self.stats['images_failed']
            })
            
            logger.success(f"ğŸ‰ æ¿å—çˆ¬å–å®Œæˆ: {board_name}, æ€»é¡µæ•°: {page_count}")
            
        except Exception as e:
            # å‘ç”Ÿé”™è¯¯æ—¶ä¿å­˜æ£€æŸ¥ç‚¹
            logger.error(f"âŒ çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            checkpoint.mark_error(str(e))
            raise
    
    async def crawl_thread(self, thread_info: Dict[str, Any]):
        """
        çˆ¬å–å•ä¸ªå¸–å­
        
        Args:
            thread_info: å¸–å­ä¿¡æ¯å­—å…¸
        """
        thread_url = thread_info['url']
        thread_id = thread_info['thread_id']
        
        # æ£€æŸ¥æ˜¯å¦å·²çˆ¬å–
        if storage.thread_exists(thread_id):
            logger.info(f"â­ï¸  å¸–å­ {thread_id} å·²çˆ¬å–ï¼Œè·³è¿‡")
            return
        
        logger.info(f"ğŸ“ çˆ¬å–å¸–å­: {thread_info.get('title', thread_id)}")
        
        # è·å–å¸–å­é¡µé¢
        html = await self.fetch_page(thread_url)
        if not html:
            return
        
        # è§£æå¸–å­å†…å®¹
        thread_data = self.parser.parse_thread_page(html, thread_url)
        thread_data['board'] = thread_info.get('board')
        thread_data['title'] = thread_info.get('title')
        
        # è®ºå›ç‰¹å®šå¤„ç†ï¼ˆç­–ç•¥æ¨¡å¼ - å­ç±»å¯é‡å†™ï¼‰
        thread_data['images'] = await self.process_images(thread_data['images'])
        
        # æ›´æ–°ç»Ÿè®¡
        self.stats['threads_crawled'] += 1
        self.stats['images_found'] += len(thread_data['images'])
        
        logger.info(f"ğŸ–¼ï¸  å‘ç° {len(thread_data['images'])} å¼ å›¾ç‰‡")
        
        # ä¸‹è½½å›¾ç‰‡
        if thread_data['images']:
            await self.download_thread_images(thread_data)
        
        # ä¿å­˜å¸–å­æ•°æ®
        storage.save_thread(thread_data)
    
    async def download_thread_images(self, thread_data: Dict[str, Any]):
        """ä¸‹è½½å¸–å­ä¸­çš„å›¾ç‰‡"""
        images = thread_data['images']
        thread_id = thread_data['thread_id']
        board = thread_data.get('board', 'unknown')
        
        # è¿‡æ»¤é‡å¤URL
        unique_images = []
        for img_url in images:
            if self.config.image.enable_deduplication:
                if not self.deduplicator.is_duplicate_url(img_url):
                    unique_images.append(img_url)
                else:
                    self.stats['duplicates_skipped'] += 1
            else:
                unique_images.append(img_url)
        
        if not unique_images:
            logger.info(f"â­ï¸  æ²¡æœ‰æ–°å›¾ç‰‡éœ€è¦ä¸‹è½½")
            return
        
        logger.info(f"â¬‡ï¸  ä¸‹è½½ {len(unique_images)} å¼ å›¾ç‰‡...")
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        save_dir = self.config.image.download_dir / board / thread_id
        save_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¸‹è½½å›¾ç‰‡
        async with ImageDownloader() as downloader:
            metadata = {
                'board': board,
                'thread_id': thread_id,
                'thread_url': thread_data['url']
            }
            
            results = await downloader.download_batch(
                unique_images,
                save_dir,
                metadata
            )
            
            # ç»Ÿè®¡ç»“æœ
            for result in results:
                if result.get('success'):
                    self.stats['images_downloaded'] += 1
                    
                    # æ£€æŸ¥æ–‡ä»¶å»é‡
                    if self.config.image.enable_deduplication:
                        file_path = Path(result['save_path'])
                        if self.deduplicator.is_duplicate_file(file_path):
                            self.deduplicator.remove_duplicate_file(file_path)
                            self.stats['duplicates_skipped'] += 1
                            self.stats['images_downloaded'] -= 1
                            continue
                    
                    # ä¿å­˜å›¾ç‰‡è®°å½•
                    storage.save_image_record(result)
                elif result.get('skipped'):
                    # è¢«è·³è¿‡çš„å›¾ç‰‡ï¼ˆå·²å­˜åœ¨/å°ºå¯¸ä¸ç¬¦ç­‰ï¼‰
                    self.stats['duplicates_skipped'] += 1
                else:
                    # çœŸæ­£ä¸‹è½½å¤±è´¥çš„å›¾ç‰‡
                    self.stats['images_failed'] += 1
    
    async def crawl_threads_from_list(self, thread_urls: List[str]):
        """
        ä»URLåˆ—è¡¨çˆ¬å–å¸–å­
        
        Args:
            thread_urls: å¸–å­URLåˆ—è¡¨
        """
        logger.info(f"ğŸ“‹ æ‰¹é‡çˆ¬å– {len(thread_urls)} ä¸ªå¸–å­")
        
        for url in tqdm(thread_urls, desc="çˆ¬å–è¿›åº¦"):
            thread_info = {
                'url': url,
                'thread_id': self.parser._extract_thread_id(url)
            }
            await self.crawl_thread(thread_info)
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.stats.copy()
        stats['deduplication'] = self.deduplicator.get_stats()
        stats['storage'] = storage.get_statistics()
        return stats


# ============================================================================
# è®ºå›ç‰¹å®šçˆ¬è™«
# ============================================================================

class DiscuzSpider(BBSSpider):
    """
    Discuzè®ºå›ä¸“ç”¨çˆ¬è™«
    
    å¤„ç†Discuzç‰¹æœ‰çš„å›¾ç‰‡é“¾æ¥æ ¼å¼å’Œé™„ä»¶ç³»ç»Ÿ
    """
    
    async def process_images(self, images: List[str]) -> List[str]:
        """
        å¤„ç†Discuzç‰¹æ®Šå›¾ç‰‡é“¾æ¥
        
        Discuzçš„é™„ä»¶é“¾æ¥æ ¼å¼: forum.php?mod=attachment&aid=xxx
        éœ€è¦æ·»åŠ  &nothumb=yes å‚æ•°è·å–åŸå›¾
        """
        processed_images = []
        
        for img_url in images:
            # å¤„ç†ç›¸å¯¹è·¯å¾„
            if img_url.startswith('forum.php') or img_url.startswith('/forum.php'):
                img_url = f"{self.config.bbs.base_url}/{img_url.lstrip('/')}"
            
            # Discuzé™„ä»¶é“¾æ¥éœ€è¦æ·»åŠ åŸå›¾å‚æ•°
            if 'mod=attachment' in img_url and 'nothumb' not in img_url:
                img_url += '&nothumb=yes'
            
            processed_images.append(img_url)
        
        return processed_images


class PhpBBSpider(BBSSpider):
    """
    phpBBè®ºå›ä¸“ç”¨çˆ¬è™«
    
    å¤„ç†phpBBç‰¹æœ‰çš„ç»“æ„
    """
    pass  # æš‚æ—¶ä½¿ç”¨åŸºç±»é€»è¾‘ï¼Œå¯æ ¹æ®éœ€è¦æ‰©å±•


class VBulletinSpider(BBSSpider):
    """
    vBulletinè®ºå›ä¸“ç”¨çˆ¬è™«
    
    å¤„ç†vBulletinç‰¹æœ‰çš„ç»“æ„
    """
    pass  # æš‚æ—¶ä½¿ç”¨åŸºç±»é€»è¾‘ï¼Œå¯æ ¹æ®éœ€è¦æ‰©å±•
