"""
BBSè®ºå›çˆ¬è™«æ¨¡å—

åŒ…å«:
- BBSSpider: BBSè®ºå›çˆ¬è™«åŸºç±»
- DiscuzSpider: Discuzè®ºå›çˆ¬è™«
- PhpBBSpider: phpBBè®ºå›çˆ¬è™«
- VBulletinSpider: vBulletinè®ºå›çˆ¬è™«
"""
from typing import List, Dict, Any, Optional
from pathlib import Path
from loguru import logger
from tqdm import tqdm

from core.base import BaseSpider
from core.downloader import ImageDownloader
from core.storage import storage
from core.deduplicator import ImageDeduplicator
from parsers.bbs_parser import BBSParser
from config import Config, ConfigLoader


class BBSSpider(BaseSpider):
    """
    BBSè®ºå›å›¾ç‰‡çˆ¬è™«
    
    ç»§æ‰¿ BaseSpiderï¼Œæ·»åŠ è®ºå›ç‰¹æœ‰åŠŸèƒ½ï¼š
    - å¸–å­åˆ—è¡¨çˆ¬å–
    - å¸–å­è¯¦æƒ…çˆ¬å–
    - å›¾ç‰‡ä¸‹è½½å’Œå»é‡
    
    å­ç±»ï¼ˆå¦‚ DiscuzSpiderï¼‰å¯é‡å†™ process_images() å®ç°è®ºå›ç‰¹å®šå¤„ç†
    """
    
    def __init__(self, config: Optional[Config] = None, url: Optional[str] = None, preset: Optional[str] = None):
        """
        åˆå§‹åŒ–BBSçˆ¬è™«
        
        Args:
            config: æ‰‹åŠ¨é…ç½®ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
            url: è®ºå›URLï¼Œè‡ªåŠ¨æ£€æµ‹é…ç½®
            preset: è®ºå›ç±»å‹é¢„è®¾ (discuz/phpbb/vbulletin)
        
        Note:
            æ¨èä½¿ç”¨ SpiderFactory.create() è€Œä¸æ˜¯ç›´æ¥å®ä¾‹åŒ–
        """
        # é…ç½®ä¼˜å…ˆçº§: config > preset > url
        if config:
            final_config = config
        elif preset:
            final_config = ConfigLoader.load(preset)
        elif url:
            final_config = ConfigLoader.auto_detect(url)
        else:
            raise ValueError("å¿…é¡»æä¾› configã€preset æˆ– url å‚æ•°ä¹‹ä¸€")
        
        # è°ƒç”¨åŸºç±»åˆå§‹åŒ–
        super().__init__(final_config)
        
        # BBSç‰¹æœ‰ç»„ä»¶
        self.parser = BBSParser()
        self.deduplicator = ImageDeduplicator(use_perceptual_hash=True)
        
        # BBSç‰¹æœ‰ç»Ÿè®¡ä¿¡æ¯ï¼ˆæ‰©å±•åŸºç±»statsï¼‰
        self.stats.update({
            "threads_crawled": 0,
            "images_found": 0,
            "images_downloaded": 0,
            "images_failed": 0,
            "duplicates_skipped": 0
        })
        
        logger.info(f"ğŸš€ åˆå§‹åŒ–çˆ¬è™«: {self.config.bbs.name} ({self.config.bbs.forum_type})")
    
    async def init(self):
        """åˆå§‹åŒ–BBSçˆ¬è™«"""
        # è°ƒç”¨åŸºç±»åˆå§‹åŒ–
        await super().init()
        
        # BBSç‰¹æœ‰åˆå§‹åŒ–
        storage.connect()
        
        # åŠ è½½å·²å­˜åœ¨çš„æ–‡ä»¶å“ˆå¸Œ
        if self.config.image.enable_deduplication:
            self.deduplicator.load_existing_hashes(self.config.image.download_dir)
        
        logger.success("âœ… çˆ¬è™«åˆå§‹åŒ–å®Œæˆ")
    
    async def close(self):
        """å…³é—­BBSçˆ¬è™«"""
        # BBSç‰¹æœ‰æ¸…ç†
        storage.close()
        
        # è¾“å‡ºå»é‡ç»Ÿè®¡
        logger.info(f"ğŸ”„ å»é‡ç»Ÿè®¡: {self.deduplicator.get_stats()}")
        
        # è°ƒç”¨åŸºç±»å…³é—­
        await super().close()
    
    async def process_images(self, images: List[str]) -> List[str]:
        """
        å¤„ç†å›¾ç‰‡URLï¼ˆé’©å­æ–¹æ³•ï¼‰
        
        å­ç±»å¯é‡å†™æ­¤æ–¹æ³•å®ç°è®ºå›ç‰¹å®šçš„å›¾ç‰‡å¤„ç†é€»è¾‘
        
        Args:
            images: åŸå§‹å›¾ç‰‡URLåˆ—è¡¨
        
        Returns:
            å¤„ç†åçš„å›¾ç‰‡URLåˆ—è¡¨
        """
        return images
    
    async def crawl_board(self, board_url: str, board_name: str, max_pages: Optional[int] = None):
        """
        çˆ¬å–æ¿å—
        
        Args:
            board_url: æ¿å—URL
            board_name: æ¿å—åç§°
            max_pages: æœ€å¤§é¡µæ•°
        """
        logger.info(f"ğŸ“š å¼€å§‹çˆ¬å–æ¿å—: {board_name}")
        
        current_url = board_url
        page_count = 0
        
        while current_url and (max_pages is None or page_count < max_pages):
            page_count += 1
            logger.info(f"ğŸ“„ çˆ¬å–ç¬¬ {page_count} é¡µ: {current_url}")
            
            # è·å–åˆ—è¡¨é¡µ
            html = await self.fetch_page(current_url)
            if not html:
                break
            
            # è§£æå¸–å­åˆ—è¡¨
            threads = self.parser.parse_thread_list(html, current_url)
            logger.info(f"âœ… å‘ç° {len(threads)} ä¸ªå¸–å­")
            
            # çˆ¬å–æ¯ä¸ªå¸–å­
            for thread in threads:
                thread['board'] = board_name
                await self.crawl_thread(thread)
            
            # æŸ¥æ‰¾ä¸‹ä¸€é¡µ
            current_url = self.parser.find_next_page(html, current_url)
            if not current_url:
                logger.info("ğŸ“Œ æ²¡æœ‰æ›´å¤šé¡µé¢")
                break
        
        logger.success(f"ğŸ‰ æ¿å—çˆ¬å–å®Œæˆ: {board_name}, æ€»é¡µæ•°: {page_count}")
    
    async def crawl_thread(self, thread_info: Dict[str, Any]):
        """
        çˆ¬å–å•ä¸ªå¸–å­
        
        Args:
            thread_info: å¸–å­ä¿¡æ¯å­—å…¸
        """
        thread_url = thread_info['url']
        thread_id = thread_info['thread_id']
        
        # æ£€æŸ¥æ˜¯å¦å·²çˆ¬å–
        if storage.thread_exists(thread_id):
            logger.info(f"â­ï¸  å¸–å­ {thread_id} å·²çˆ¬å–ï¼Œè·³è¿‡")
            return
        
        logger.info(f"ğŸ“ çˆ¬å–å¸–å­: {thread_info.get('title', thread_id)}")
        
        # è·å–å¸–å­é¡µé¢
        html = await self.fetch_page(thread_url)
        if not html:
            return
        
        # è§£æå¸–å­å†…å®¹
        thread_data = self.parser.parse_thread_page(html, thread_url)
        thread_data['board'] = thread_info.get('board')
        thread_data['title'] = thread_info.get('title')
        
        # è®ºå›ç‰¹å®šå¤„ç†ï¼ˆç­–ç•¥æ¨¡å¼ - å­ç±»å¯é‡å†™ï¼‰
        thread_data['images'] = await self.process_images(thread_data['images'])
        
        # æ›´æ–°ç»Ÿè®¡
        self.stats['threads_crawled'] += 1
        self.stats['images_found'] += len(thread_data['images'])
        
        logger.info(f"ğŸ–¼ï¸  å‘ç° {len(thread_data['images'])} å¼ å›¾ç‰‡")
        
        # ä¸‹è½½å›¾ç‰‡
        if thread_data['images']:
            await self.download_thread_images(thread_data)
        
        # ä¿å­˜å¸–å­æ•°æ®
        storage.save_thread(thread_data)
    
    async def download_thread_images(self, thread_data: Dict[str, Any]):
        """ä¸‹è½½å¸–å­ä¸­çš„å›¾ç‰‡"""
        images = thread_data['images']
        thread_id = thread_data['thread_id']
        board = thread_data.get('board', 'unknown')
        
        # è¿‡æ»¤é‡å¤URL
        unique_images = []
        for img_url in images:
            if self.config.image.enable_deduplication:
                if not self.deduplicator.is_duplicate_url(img_url):
                    unique_images.append(img_url)
                else:
                    self.stats['duplicates_skipped'] += 1
            else:
                unique_images.append(img_url)
        
        if not unique_images:
            logger.info(f"â­ï¸  æ²¡æœ‰æ–°å›¾ç‰‡éœ€è¦ä¸‹è½½")
            return
        
        logger.info(f"â¬‡ï¸  ä¸‹è½½ {len(unique_images)} å¼ å›¾ç‰‡...")
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        save_dir = self.config.image.download_dir / board / thread_id
        save_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¸‹è½½å›¾ç‰‡
        async with ImageDownloader() as downloader:
            metadata = {
                'board': board,
                'thread_id': thread_id,
                'thread_url': thread_data['url']
            }
            
            results = await downloader.download_batch(
                unique_images,
                save_dir,
                metadata
            )
            
            # ç»Ÿè®¡ç»“æœ
            for result in results:
                if result.get('success'):
                    self.stats['images_downloaded'] += 1
                    
                    # æ£€æŸ¥æ–‡ä»¶å»é‡
                    if self.config.image.enable_deduplication:
                        file_path = Path(result['save_path'])
                        if self.deduplicator.is_duplicate_file(file_path):
                            self.deduplicator.remove_duplicate_file(file_path)
                            self.stats['duplicates_skipped'] += 1
                            self.stats['images_downloaded'] -= 1
                            continue
                    
                    # ä¿å­˜å›¾ç‰‡è®°å½•
                    storage.save_image_record(result)
                elif result.get('skipped'):
                    # è¢«è·³è¿‡çš„å›¾ç‰‡ï¼ˆå·²å­˜åœ¨/å°ºå¯¸ä¸ç¬¦ç­‰ï¼‰
                    self.stats['duplicates_skipped'] += 1
                else:
                    # çœŸæ­£ä¸‹è½½å¤±è´¥çš„å›¾ç‰‡
                    self.stats['images_failed'] += 1
    
    async def crawl_threads_from_list(self, thread_urls: List[str]):
        """
        ä»URLåˆ—è¡¨çˆ¬å–å¸–å­
        
        Args:
            thread_urls: å¸–å­URLåˆ—è¡¨
        """
        logger.info(f"ğŸ“‹ æ‰¹é‡çˆ¬å– {len(thread_urls)} ä¸ªå¸–å­")
        
        for url in tqdm(thread_urls, desc="çˆ¬å–è¿›åº¦"):
            thread_info = {
                'url': url,
                'thread_id': self.parser._extract_thread_id(url)
            }
            await self.crawl_thread(thread_info)
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.stats.copy()
        stats['deduplication'] = self.deduplicator.get_stats()
        stats['storage'] = storage.get_statistics()
        return stats


# ============================================================================
# è®ºå›ç‰¹å®šçˆ¬è™«
# ============================================================================

class DiscuzSpider(BBSSpider):
    """
    Discuzè®ºå›ä¸“ç”¨çˆ¬è™«
    
    å¤„ç†Discuzç‰¹æœ‰çš„å›¾ç‰‡é“¾æ¥æ ¼å¼å’Œé™„ä»¶ç³»ç»Ÿ
    """
    
    async def process_images(self, images: List[str]) -> List[str]:
        """
        å¤„ç†Discuzç‰¹æ®Šå›¾ç‰‡é“¾æ¥
        
        Discuzçš„é™„ä»¶é“¾æ¥æ ¼å¼: forum.php?mod=attachment&aid=xxx
        éœ€è¦æ·»åŠ  &nothumb=yes å‚æ•°è·å–åŸå›¾
        """
        processed_images = []
        
        for img_url in images:
            # å¤„ç†ç›¸å¯¹è·¯å¾„
            if img_url.startswith('forum.php') or img_url.startswith('/forum.php'):
                img_url = f"{self.config.bbs.base_url}/{img_url.lstrip('/')}"
            
            # Discuzé™„ä»¶é“¾æ¥éœ€è¦æ·»åŠ åŸå›¾å‚æ•°
            if 'mod=attachment' in img_url and 'nothumb' not in img_url:
                img_url += '&nothumb=yes'
            
            processed_images.append(img_url)
        
        return processed_images


class PhpBBSpider(BBSSpider):
    """
    phpBBè®ºå›ä¸“ç”¨çˆ¬è™«
    
    å¤„ç†phpBBç‰¹æœ‰çš„ç»“æ„
    """
    pass  # æš‚æ—¶ä½¿ç”¨åŸºç±»é€»è¾‘ï¼Œå¯æ ¹æ®éœ€è¦æ‰©å±•


class VBulletinSpider(BBSSpider):
    """
    vBulletinè®ºå›ä¸“ç”¨çˆ¬è™«
    
    å¤„ç†vBulletinç‰¹æœ‰çš„ç»“æ„
    """
    pass  # æš‚æ—¶ä½¿ç”¨åŸºç±»é€»è¾‘ï¼Œå¯æ ¹æ®éœ€è¦æ‰©å±•
