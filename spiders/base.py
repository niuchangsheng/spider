"""
çˆ¬è™«åŸºç±»æ¨¡å—

åŒ…å«çˆ¬è™«çš„æŠ½è±¡åŸºç±»ï¼š
- BaseSpider: çˆ¬è™«åŸºç±»
"""
import asyncio
import aiohttp
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional
from loguru import logger
from fake_useragent import UserAgent

from config import Config


class BaseSpider(ABC):
    """
    çˆ¬è™«åŸºç±»
    
    æ‰€æœ‰çˆ¬è™«çš„å…¬å…±åŸºç±»ï¼Œæä¾›ï¼š
    - HTTP Session ç®¡ç†
    - é¡µé¢è·å–
    - ç»Ÿè®¡ä¿¡æ¯
    - å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†
    
    å­ç±»éœ€è¦å®ç°:
    - get_statistics(): è·å–ç»Ÿè®¡ä¿¡æ¯
    """
    
    def __init__(self, config: Config):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            config: é…ç½®å¯¹è±¡
        """
        self.config = config
        self.session: Optional[aiohttp.ClientSession] = None
        self.ua = UserAgent()
        
        # åŸºç¡€ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'pages_fetched': 0,
            'requests_failed': 0,
        }
    
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        await self.init()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        await self.close()
    
    async def init(self):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        å­ç±»åº”è¯¥è°ƒç”¨ super().init() å¹¶æ·»åŠ ç‰¹å®šåˆå§‹åŒ–é€»è¾‘
        """
        logger.info("âš™ï¸  åˆå§‹åŒ–çˆ¬è™«ç»„ä»¶...")
        
        # åˆå§‹åŒ–HTTPä¼šè¯
        timeout = aiohttp.ClientTimeout(total=self.config.crawler.request_timeout)
        self.session = aiohttp.ClientSession(timeout=timeout)
    
    async def close(self):
        """
        å…³é—­çˆ¬è™«
        
        å­ç±»åº”è¯¥å…ˆæ‰§è¡Œç‰¹å®šæ¸…ç†é€»è¾‘ï¼Œå†è°ƒç”¨ super().close()
        """
        logger.info("ğŸ”’ å…³é—­çˆ¬è™«...")
        
        if self.session:
            await self.session.close()
        
        logger.info(f"ğŸ“Š çˆ¬è™«ç»Ÿè®¡: {self.get_statistics()}")
    
    def get_headers(self) -> Dict[str, str]:
        """
        è·å–è¯·æ±‚å¤´
        
        å­ç±»å¯é‡å†™æ­¤æ–¹æ³•æ·»åŠ ç‰¹å®šè¯·æ±‚å¤´
        """
        headers = {
            "User-Agent": self.ua.random if self.config.crawler.rotate_user_agent else self.ua.chrome,
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Accept-Encoding": "gzip, deflate",
            "Connection": "keep-alive",
        }
        
        if self.config.bbs.base_url:
            headers["Referer"] = self.config.bbs.base_url
        
        return headers
    
    async def fetch_page(self, url: str, headers: Optional[Dict] = None) -> Optional[str]:
        """
        è·å–é¡µé¢å†…å®¹
        
        Args:
            url: é¡µé¢URL
            headers: å¯é€‰çš„é¢å¤–è¯·æ±‚å¤´
        
        Returns:
            HTMLå†…å®¹ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            logger.debug(f"ğŸ“„ è·å–é¡µé¢: {url}")
            
            request_headers = self.get_headers()
            if headers:
                request_headers.update(headers)
            
            async with self.session.get(url, headers=request_headers) as response:
                if response.status == 200:
                    self.stats['pages_fetched'] += 1
                    html = await response.text()
                    await asyncio.sleep(self.config.crawler.download_delay)
                    return html
                else:
                    logger.warning(f"âš ï¸  è·å–å¤±è´¥ {url}: HTTP {response.status}")
                    return None
        
        except asyncio.TimeoutError:
            self.stats['requests_failed'] += 1
            logger.error(f"âŒ è¶…æ—¶: {url}")
            return None
        except Exception as e:
            self.stats['requests_failed'] += 1
            logger.error(f"âŒ è·å–å‡ºé”™ {url}: {e}")
            return None
    
    @abstractmethod
    def get_statistics(self) -> Dict[str, Any]:
        """
        è·å–ç»Ÿè®¡ä¿¡æ¯
        
        å­ç±»å¿…é¡»å®ç°æ­¤æ–¹æ³•
        """
        pass
