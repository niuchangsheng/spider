"""
åŠ¨æ€é¡µé¢è§£æå™¨
ç”¨äºè§£æä½¿ç”¨Ajaxå¼‚æ­¥åŠ è½½å†…å®¹çš„åŠ¨æ€ç½‘é¡µ
"""
from typing import List, Dict, Optional
from bs4 import BeautifulSoup
from loguru import logger

from core.parser import BBSParser


class DynamicPageParser(BBSParser):
    """
    åŠ¨æ€é¡µé¢è§£æå™¨
    
    ä¸“é—¨ç”¨äºè§£æé€šè¿‡Ajax/JavaScriptåŠ¨æ€åŠ è½½å†…å®¹çš„ç½‘é¡µï¼Œ
    å¦‚æ–°é—»åˆ—è¡¨ã€å…¬å‘Šé¡µé¢ç­‰ã€‚
    
    ä¸»è¦åŠŸèƒ½ï¼š
    - è§£ææ–‡ç« åˆ—è¡¨
    - æå–æ–‡ç« åŸºæœ¬ä¿¡æ¯ï¼ˆæ ‡é¢˜ã€ä½œè€…ã€æ—¥æœŸã€æ‘˜è¦ã€é“¾æ¥ï¼‰
    - æ£€æµ‹"æŸ¥çœ‹æ›´å¤š"æŒ‰é’®åŠå…¶åŠ è½½å‚æ•°
    - æ”¯æŒå¤šç§æ—¥æœŸæ ¼å¼ï¼ˆç›¸å¯¹æ—¶é—´ã€ç»å¯¹æ—¶é—´ï¼‰
    
    Example:
        parser = DynamicPageParser(config)
        articles = parser.parse_articles(html)
        has_more = parser.has_load_more_button(html)
    """
    
    def __init__(self, config):
        """
        åˆå§‹åŒ–åŠ¨æ€é¡µé¢è§£æå™¨
        
        Args:
            config: é…ç½®å¯¹è±¡ï¼ŒåŒ…å«é€‰æ‹©å™¨ç­‰ä¿¡æ¯
        """
        super().__init__()
        
        # ä¿å­˜configå¼•ç”¨
        self.config = config
        
        # é»˜è®¤æ–‡ç« é€‰æ‹©å™¨ï¼ˆå¯é€šè¿‡é…ç½®è¦†ç›–ï¼‰
        self.article_selector = getattr(config.bbs, 'article_selector', '.article')
        self.title_selector = getattr(config.bbs, 'title_selector', '.title')
        self.author_selector = getattr(config.bbs, 'author_selector', '.author')
        self.date_selector = getattr(config.bbs, 'date_selector', '.date')
        self.summary_selector = getattr(config.bbs, 'summary_selector', '.body')
        self.link_selector = getattr(config.bbs, 'link_selector', 'a[href]')
        
        logger.debug(f"ğŸ”§ åŠ¨æ€é¡µé¢è§£æå™¨åˆå§‹åŒ–å®Œæˆ")
        logger.debug(f"   æ–‡ç« é€‰æ‹©å™¨: {self.article_selector}")
    
    def parse_articles(self, html: str) -> List[Dict]:
        """
        è§£ææ–‡ç« åˆ—è¡¨
        
        ä»HTMLä¸­æå–æ‰€æœ‰æ–‡ç« çš„åŸºæœ¬ä¿¡æ¯ã€‚
        
        Args:
            html: HTMLå†…å®¹
        
        Returns:
            æ–‡ç« ä¿¡æ¯åˆ—è¡¨ï¼Œæ¯ä¸ªæ–‡ç« æ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«ï¼š
            - article_id: æ–‡ç« ID
            - title: æ ‡é¢˜
            - author: ä½œè€…
            - date: å‘å¸ƒæ—¥æœŸ
            - summary: æ‘˜è¦
            - url: è¯¦æƒ…é“¾æ¥
        
        Example:
            articles = parser.parse_articles(html)
            # [
            #   {
            #     'article_id': '15537',
            #     'title': 'è®¸æ„¿æ ‘å’Œä¸€é”¤å®šéŸ³å³å°†å¼€å¯ï¼',
            #     'author': 'ã€Šç¥ä»™é“ã€‹è¿è¥å›¢é˜Ÿ',
            #     'date': '5å¤©å‰',
            #     'summary': 'äº²çˆ±çš„ä»™å‹ä»¬...',
            #     'url': 'https://sxd.xd.com/15537'
            #   },
            #   ...
            # ]
        """
        soup = BeautifulSoup(html, 'html.parser')
        articles = []
        
        # æŸ¥æ‰¾æ‰€æœ‰æ–‡ç« å…ƒç´ 
        article_elements = soup.select(self.article_selector)
        
        logger.debug(f"ğŸ” æ‰¾åˆ° {len(article_elements)} ä¸ªæ–‡ç« å…ƒç´ ")
        
        for i, elem in enumerate(article_elements, 1):
            try:
                article = self._extract_article_info(elem)
                if article:
                    articles.append(article)
                    logger.debug(f"   âœ“ æ–‡ç«  #{i}: {article.get('title', 'N/A')[:30]}")
                else:
                    logger.debug(f"   âœ— æ–‡ç«  #{i}: æå–å¤±è´¥ï¼ˆç¼ºå°‘å¿…è¦å­—æ®µï¼‰")
            except Exception as e:
                logger.error(f"âŒ è§£ææ–‡ç«  #{i} å¤±è´¥: {e}")
                continue
        
        logger.info(f"ğŸ“„ æˆåŠŸè§£æ {len(articles)}/{len(article_elements)} ç¯‡æ–‡ç« ")
        
        return articles
    
    def _extract_article_info(self, element) -> Optional[Dict]:
        """
        ä»å•ä¸ªæ–‡ç« å…ƒç´ ä¸­æå–ä¿¡æ¯
        
        Args:
            element: BeautifulSoupå…ƒç´ å¯¹è±¡
        
        Returns:
            æ–‡ç« ä¿¡æ¯å­—å…¸ï¼Œå¦‚æœæå–å¤±è´¥è¿”å›None
        """
        # æå–æ ‡é¢˜
        title_elem = element.select_one(self.title_selector)
        title = title_elem.get_text(strip=True) if title_elem else None
        
        # æå–ä½œè€…
        author_elem = element.select_one(self.author_selector)
        author = author_elem.get_text(strip=True) if author_elem else "æœªçŸ¥ä½œè€…"
        
        # æå–æ—¥æœŸ
        date_elem = element.select_one(self.date_selector)
        date = date_elem.get_text(strip=True) if date_elem else None
        
        # æå–æ‘˜è¦
        summary_elem = element.select_one(self.summary_selector)
        summary = summary_elem.get_text(strip=True) if summary_elem else ""
        
        # æå–é“¾æ¥
        link_elem = element.select_one(self.link_selector)
        url = link_elem.get('href') if link_elem else None
        
        # éªŒè¯å¿…éœ€å­—æ®µ
        if not title or not url:
            logger.debug(f"   âš ï¸  ç¼ºå°‘å¿…éœ€å­—æ®µ: title={bool(title)}, url={bool(url)}")
            return None
        
        # æå–æ–‡ç« IDï¼ˆä»URLä¸­ï¼‰
        article_id = self._extract_article_id(url)
        
        # ç¡®ä¿URLæ˜¯å®Œæ•´çš„
        if url and not url.startswith('http'):
            base_url = self.config.bbs.base_url
            if url.startswith('/'):
                url = f"{base_url}{url}"
            else:
                url = f"{base_url}/{url}"
        
        return {
            'article_id': article_id,
            'title': title,
            'author': author,
            'date': date,
            'summary': summary,
            'url': url
        }
    
    def _extract_article_id(self, url: str) -> str:
        """
        ä»URLä¸­æå–æ–‡ç« ID
        
        Args:
            url: æ–‡ç« URL
        
        Returns:
            æ–‡ç« IDå­—ç¬¦ä¸²
        
        Example:
            'https://sxd.xd.com/15537' -> '15537'
            '/article/15537' -> '15537'
            '/news?id=15537' -> '15537'
        """
        import re
        
        # å°è¯•å¤šç§æ¨¡å¼
        patterns = [
            r'/(\d+)/?$',           # æœ«å°¾çš„æ•°å­—: /15537
            r'/(\d+)[?&#]',         # æ•°å­—åè·Ÿå‚æ•°: /15537?xx
            r'[?&]id=(\d+)',        # URLå‚æ•°: ?id=15537
            r'[?&]article_id=(\d+)', # URLå‚æ•°: ?article_id=15537
            r'/article/(\d+)',      # è·¯å¾„ä¸­: /article/15537
            r'/news/(\d+)',         # è·¯å¾„ä¸­: /news/15537
        ]
        
        for pattern in patterns:
            match = re.search(pattern, url)
            if match:
                return match.group(1)
        
        # å¦‚æœéƒ½ä¸åŒ¹é…ï¼Œä½¿ç”¨URLçš„MD5
        import hashlib
        return hashlib.md5(url.encode()).hexdigest()[:16]
    
    def has_load_more_button(self, html: str) -> bool:
        """
        æ£€æŸ¥é¡µé¢æ˜¯å¦è¿˜æœ‰"æŸ¥çœ‹æ›´å¤š"æŒ‰é’®
        
        Args:
            html: HTMLå†…å®¹
        
        Returns:
            å¦‚æœå­˜åœ¨"æŸ¥çœ‹æ›´å¤š"æŒ‰é’®è¿”å›Trueï¼Œå¦åˆ™False
        
        Example:
            if parser.has_load_more_button(html):
                print("è¿˜æœ‰æ›´å¤šå†…å®¹å¯ä»¥åŠ è½½")
        """
        soup = BeautifulSoup(html, 'html.parser')
        
        # æŸ¥æ‰¾"æŸ¥çœ‹æ›´å¤š"æŒ‰é’®ï¼ˆå¤šç§å¯èƒ½çš„é€‰æ‹©å™¨ï¼‰
        load_more_selectors = [
            'a.more[data-action="switch_page"]',  # ç¥ä»™é“å®˜ç½‘æ ¼å¼
            'a.load-more',                        # é€šç”¨æ ¼å¼1
            'button.load-more',                   # é€šç”¨æ ¼å¼2
            '[data-action*="load"]',              # åŒ…å«loadçš„data-action
            '[data-action*="more"]',              # åŒ…å«moreçš„data-action
        ]
        
        for selector in load_more_selectors:
            element = soup.select_one(selector)
            if element:
                logger.debug(f"âœ“ æ‰¾åˆ°'æŸ¥çœ‹æ›´å¤š'æŒ‰é’®: {selector}")
                return True
        
        # ä¹Ÿæ£€æŸ¥æ–‡æœ¬å†…å®¹
        more_texts = ['æŸ¥çœ‹æ›´å¤š', 'load more', 'åŠ è½½æ›´å¤š', 'show more', 'æ›´å¤š']
        for text in more_texts:
            if soup.find(string=lambda t: text in t.lower() if t else False):
                logger.debug(f"âœ“ æ‰¾åˆ°'æŸ¥çœ‹æ›´å¤š'æ–‡æœ¬: {text}")
                return True
        
        logger.debug("âœ— æœªæ‰¾åˆ°'æŸ¥çœ‹æ›´å¤š'æŒ‰é’®")
        return False
    
    def get_next_page_number(self, html: str) -> Optional[int]:
        """
        è·å–ä¸‹ä¸€é¡µçš„é¡µç 
        
        ä»"æŸ¥çœ‹æ›´å¤š"æŒ‰é’®çš„data-pageå±æ€§ä¸­æå–é¡µç ã€‚
        
        Args:
            html: HTMLå†…å®¹
        
        Returns:
            ä¸‹ä¸€é¡µé¡µç ï¼Œå¦‚æœæ²¡æœ‰è¿”å›None
        
        Example:
            next_page = parser.get_next_page_number(html)
            if next_page:
                url = f"{base_url}?page={next_page}"
        """
        soup = BeautifulSoup(html, 'html.parser')
        
        # æŸ¥æ‰¾å¸¦æœ‰data-pageå±æ€§çš„å…ƒç´ 
        load_more = soup.select_one('[data-page]')
        if load_more:
            try:
                page_num = int(load_more.get('data-page'))
                logger.debug(f"âœ“ ä¸‹ä¸€é¡µé¡µç : {page_num}")
                return page_num
            except (ValueError, TypeError) as e:
                logger.warning(f"âš ï¸  æ— æ³•è§£æé¡µç : {e}")
                return None
        
        logger.debug("âœ— æœªæ‰¾åˆ°data-pageå±æ€§")
        return None
    
    def parse_article_detail(self, html: str, url: str) -> Dict:
        """
        è§£ææ–‡ç« è¯¦æƒ…é¡µ
        
        Args:
            html: æ–‡ç« è¯¦æƒ…é¡µHTML
            url: æ–‡ç« URL
        
        Returns:
            æ–‡ç« è¯¦æƒ…å­—å…¸ï¼ŒåŒ…å«å®Œæ•´å†…å®¹å’Œå›¾ç‰‡åˆ—è¡¨
        """
        soup = BeautifulSoup(html, 'html.parser')
        
        # æŸ¥æ‰¾æ–‡ç« å†…å®¹å®¹å™¨ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼Œè¶Šç²¾ç¡®çš„é€‰æ‹©å™¨è¶Šé å‰ï¼‰
        content_selectors = [
            '.article .body',           # ç¥ä»™é“å®˜ç½‘ - æ–‡ç« æ­£æ–‡ï¼ˆæœ€ç²¾ç¡®ï¼‰
            '#single .article .body',   # ç¥ä»™é“å®˜ç½‘ - å¸¦IDçš„æ›´ç²¾ç¡®é€‰æ‹©å™¨
            '.article-body',            # å¸¸è§å˜ä½“
            '.article-content',         # å¸¸è§å˜ä½“
            '.post-body',               # åšå®¢ç±»
            '.post-content',            # åšå®¢ç±»
            '.news-detail',             # æ–°é—»ç±»
            '.news-content',            # æ–°é—»ç±»
            '.detail-content',          # è¯¦æƒ…é¡µ
            'article .content',         # HTML5 articleæ ‡ç­¾
            '#content',                 # é€šç”¨ID
            '.content',                 # é€šç”¨ç±»ï¼ˆå¯èƒ½å¤ªå®½æ³›ï¼‰
            # ä»¥ä¸‹æ˜¯å¤‡é€‰ï¼Œå¯èƒ½åŒ…å«è¿‡å¤šå†…å®¹
            # '.widget_body',           # å¯èƒ½åŒ…å«æ•´ä¸ªé¡µé¢
            # '.block-body',            # å¯èƒ½åŒ…å«æ•´ä¸ªé¡µé¢
        ]
        
        content_elem = None
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                logger.info(f"âœ“ æ‰¾åˆ°å†…å®¹å®¹å™¨: {selector}")
                # æ‰“å°å®¹å™¨çš„å‰100ä¸ªå­—ç¬¦ï¼Œç”¨äºè°ƒè¯•
                text = content_elem.get_text(strip=True)[:100]
                logger.info(f"   å®¹å™¨å†…å®¹é¢„è§ˆ: {text}...")
                
                # è°ƒè¯•å›¾ç‰‡æ•°é‡
                imgs = content_elem.find_all('img')
                logger.info(f"   å®¹å™¨å†…å›¾ç‰‡æ•°: {len(imgs)}")
                break
        
        if not content_elem:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰¹å®šå®¹å™¨ï¼Œä½¿ç”¨æ•´ä¸ªbody
            content_elem = soup.find('body')
            logger.info("âš ï¸  æœªæ‰¾åˆ°ç‰¹å®šå®¹å™¨ï¼Œä½¿ç”¨æ•´ä¸ªbodyä½œä¸ºå†…å®¹")
            
            if content_elem:
                imgs = content_elem.find_all('img')
                logger.info(f"   Bodyå†…å›¾ç‰‡æ•°: {len(imgs)}")
        
        # æå–æ–‡æœ¬å†…å®¹
        content = content_elem.get_text(strip=True) if content_elem else ""
        
        # æå–å›¾ç‰‡
        images = []
        if content_elem:
            img_tags = content_elem.find_all('img')
            for img in img_tags:
                src = img.get('src') or img.get('data-src')
                if src:
                    # è½¬æ¢ä¸ºå®Œæ•´URL
                    if not src.startswith('http'):
                        base_url = self.config.bbs.base_url
                        if src.startswith('/'):
                            src = f"{base_url}{src}"
                        else:
                            src = f"{base_url}/{src}"
                    images.append(src)
        
        logger.debug(f"âœ“ æå–åˆ° {len(images)} å¼ å›¾ç‰‡")
        
        # æå–æ–‡ç« ID
        article_id = self._extract_article_id(url)
        
        return {
            'article_id': article_id,
            'url': url,
            'content': content,
            'images': images
        }
