"""
åŸºç±»æ¨¡å—

åŒ…å«æ‰€æœ‰çˆ¬è™«å’Œè§£æå™¨çš„æŠ½è±¡åŸºç±»ï¼š
- BaseParser: è§£æå™¨åŸºç±»
- BaseSpider: çˆ¬è™«åŸºç±»
"""
import asyncio
import aiohttp
import re
import hashlib
from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
from loguru import logger
from fake_useragent import UserAgent

from config import Config


# ============================================================================
# è§£æå™¨åŸºç±»
# ============================================================================

class BaseParser(ABC):
    """
    è§£æå™¨åŸºç±»
    
    æ‰€æœ‰è§£æå™¨çš„å…¬å…±åŸºç±»ï¼Œæä¾›ï¼š
    - åŸºç¡€HTMLè§£æ
    - URLå¤„ç†
    - å›¾ç‰‡æå–
    - IDæå–
    
    å­ç±»éœ€è¦å®ç°:
    - parse(): è§£æé¡µé¢çš„ä¸»æ–¹æ³•ï¼ˆå¯é€‰ï¼‰
    """
    
    def __init__(self, parser_config=None):
        """
        åˆå§‹åŒ–è§£æå™¨
        
        Args:
            parser_config: é…ç½®å¯¹è±¡ï¼Œå¯é€‰
        """
        self._config = parser_config
    
    def _extract_id(self, url: str, patterns: List[str]) -> str:
        """
        ä»URLä¸­æå–ID
        
        Args:
            url: é¡µé¢URL
            patterns: æ­£åˆ™è¡¨è¾¾å¼åˆ—è¡¨
        
        Returns:
            æå–çš„IDï¼Œå¤±è´¥è¿”å›URLçš„MD5å“ˆå¸Œï¼ˆå‰16ä½ï¼‰
        """
        for pattern in patterns:
            match = re.search(pattern, url)
            if match:
                return match.group(1)
        
        # å›é€€ï¼šä½¿ç”¨URLçš„MD5
        return hashlib.md5(url.encode()).hexdigest()[:16]
    
    def _extract_images_from_soup(
        self, 
        soup: BeautifulSoup, 
        selectors: List[str], 
        base_url: str
    ) -> List[str]:
        """
        ä»HTMLä¸­æå–å›¾ç‰‡URL
        
        Args:
            soup: BeautifulSoupå¯¹è±¡
            selectors: CSSé€‰æ‹©å™¨åˆ—è¡¨
            base_url: åŸºç¡€URLï¼ˆç”¨äºå¤„ç†ç›¸å¯¹è·¯å¾„ï¼‰
        
        Returns:
            å›¾ç‰‡URLåˆ—è¡¨ï¼ˆå·²å»é‡ï¼‰
        """
        images = []
        for selector in selectors:
            for img in soup.select(selector):
                src = self._get_image_url(img)
                if src:
                    # å¤„ç†ç›¸å¯¹è·¯å¾„
                    if not src.startswith('http'):
                        src = urljoin(base_url, src)
                    # å»é‡
                    if src not in images:
                        images.append(src)
        return images
    
    def _get_image_url(self, img_tag) -> Optional[str]:
        """
        ä»imgæ ‡ç­¾è·å–å›¾ç‰‡URL
        
        å­ç±»å¯é‡å†™æ­¤æ–¹æ³•å®ç°ç‰¹å®šçš„å›¾ç‰‡URLæå–é€»è¾‘
        ï¼ˆå¦‚ï¼šè·å–åŸå›¾ã€å¤„ç†æ‡’åŠ è½½ç­‰ï¼‰
        
        Args:
            img_tag: BeautifulSoup imgæ ‡ç­¾
        
        Returns:
            å›¾ç‰‡URLï¼Œå¦‚æœæ— æ³•è·å–è¿”å›None
        """
        return img_tag.get('src') or img_tag.get('data-src') or img_tag.get('data-original')
    
    def _is_valid_image_url(self, url: str, allowed_formats: Optional[List[str]] = None) -> bool:
        """
        éªŒè¯å›¾ç‰‡URLæ˜¯å¦æœ‰æ•ˆ
        
        Args:
            url: å›¾ç‰‡URL
            allowed_formats: å…è®¸çš„å›¾ç‰‡æ ¼å¼åˆ—è¡¨
        
        Returns:
            URLæ˜¯å¦æœ‰æ•ˆ
        """
        if not url:
            return False
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„URL
        try:
            result = urlparse(url)
            if not result.scheme or not result.netloc:
                return False
        except:
            return False
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯å›¾ç‰‡æ–‡ä»¶
        image_extensions = allowed_formats or ['jpg', 'jpeg', 'png', 'gif', 'bmp', 'webp']
        url_lower = url.lower()
        
        # æ£€æŸ¥æ‰©å±•å
        if any(url_lower.endswith(f'.{ext}') for ext in image_extensions):
            return True
        
        # æ£€æŸ¥URLä¸­æ˜¯å¦åŒ…å«å›¾ç‰‡ç›¸å…³å…³é”®è¯
        if any(keyword in url_lower for keyword in ['image', 'img', 'photo', 'pic', 'attachment']):
            return True
        
        return False


# ============================================================================
# çˆ¬è™«åŸºç±»
# ============================================================================

class BaseSpider(ABC):
    """
    çˆ¬è™«åŸºç±»
    
    æ‰€æœ‰çˆ¬è™«çš„å…¬å…±åŸºç±»ï¼Œæä¾›ï¼š
    - HTTP Session ç®¡ç†
    - é¡µé¢è·å–
    - ç»Ÿè®¡ä¿¡æ¯
    - å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†
    
    å­ç±»éœ€è¦å®ç°:
    - get_statistics(): è·å–ç»Ÿè®¡ä¿¡æ¯
    """
    
    def __init__(self, config: Config):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            config: é…ç½®å¯¹è±¡
        """
        self.config = config
        self.session: Optional[aiohttp.ClientSession] = None
        self.ua = UserAgent()
        
        # åŸºç¡€ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'pages_fetched': 0,
            'requests_failed': 0,
        }
    
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        await self.init()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        await self.close()
    
    async def init(self):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        å­ç±»åº”è¯¥è°ƒç”¨ super().init() å¹¶æ·»åŠ ç‰¹å®šåˆå§‹åŒ–é€»è¾‘
        """
        logger.info("âš™ï¸  åˆå§‹åŒ–çˆ¬è™«ç»„ä»¶...")
        
        # åˆå§‹åŒ–HTTPä¼šè¯
        timeout = aiohttp.ClientTimeout(total=self.config.crawler.request_timeout)
        self.session = aiohttp.ClientSession(timeout=timeout)
    
    async def close(self):
        """
        å…³é—­çˆ¬è™«
        
        å­ç±»åº”è¯¥å…ˆæ‰§è¡Œç‰¹å®šæ¸…ç†é€»è¾‘ï¼Œå†è°ƒç”¨ super().close()
        """
        logger.info("ğŸ”’ å…³é—­çˆ¬è™«...")
        
        if self.session:
            await self.session.close()
        
        logger.info(f"ğŸ“Š çˆ¬è™«ç»Ÿè®¡: {self.get_statistics()}")
    
    def get_headers(self) -> Dict[str, str]:
        """
        è·å–è¯·æ±‚å¤´
        
        å­ç±»å¯é‡å†™æ­¤æ–¹æ³•æ·»åŠ ç‰¹å®šè¯·æ±‚å¤´
        """
        headers = {
            "User-Agent": self.ua.random if self.config.crawler.rotate_user_agent else self.ua.chrome,
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Accept-Encoding": "gzip, deflate",
            "Connection": "keep-alive",
        }
        
        if self.config.bbs.base_url:
            headers["Referer"] = self.config.bbs.base_url
        
        return headers
    
    async def fetch_page(self, url: str, headers: Optional[Dict] = None) -> Optional[str]:
        """
        è·å–é¡µé¢å†…å®¹
        
        Args:
            url: é¡µé¢URL
            headers: å¯é€‰çš„é¢å¤–è¯·æ±‚å¤´
        
        Returns:
            HTMLå†…å®¹ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            logger.debug(f"ğŸ“„ è·å–é¡µé¢: {url}")
            
            request_headers = self.get_headers()
            if headers:
                request_headers.update(headers)
            
            async with self.session.get(url, headers=request_headers) as response:
                if response.status == 200:
                    self.stats['pages_fetched'] += 1
                    html = await response.text()
                    await asyncio.sleep(self.config.crawler.download_delay)
                    return html
                else:
                    logger.warning(f"âš ï¸  è·å–å¤±è´¥ {url}: HTTP {response.status}")
                    return None
        
        except asyncio.TimeoutError:
            self.stats['requests_failed'] += 1
            logger.error(f"âŒ è¶…æ—¶: {url}")
            return None
        except Exception as e:
            self.stats['requests_failed'] += 1
            logger.error(f"âŒ è·å–å‡ºé”™ {url}: {e}")
            return None
    
    @abstractmethod
    def get_statistics(self) -> Dict[str, Any]:
        """
        è·å–ç»Ÿè®¡ä¿¡æ¯
        
        å­ç±»å¿…é¡»å®ç°æ­¤æ–¹æ³•
        """
        pass
