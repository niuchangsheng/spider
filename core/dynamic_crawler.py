"""
åŠ¨æ€æ–°é—»é¡µé¢çˆ¬è™«
ç”¨äºçˆ¬å–ä½¿ç”¨Ajaxå¼‚æ­¥åŠ è½½å†…å®¹çš„æ–°é—»/å…¬å‘Šé¡µé¢
"""
import asyncio
import sys
from typing import List, Dict, Optional, TYPE_CHECKING
from loguru import logger
from pathlib import Path

from config import Config
from core.dynamic_parser import DynamicPageParser

# é¿å…å¾ªç¯å¯¼å…¥
if TYPE_CHECKING:
    from spider import BaseSpider


class DynamicNewsCrawler:
    """
    åŠ¨æ€æ–°é—»é¡µé¢çˆ¬è™«
    
    ç»§æ‰¿ BaseSpider çš„è®¾è®¡ç†å¿µï¼Œä½†ä¸ºäº†é¿å…å¾ªç¯å¯¼å…¥ï¼Œ
    é‡‡ç”¨ç»„åˆæ–¹å¼å¤ç”¨åŸºç¡€åŠŸèƒ½ã€‚
    
    ç‰¹ç‚¹ï¼š
    - æ”¯æŒAjaxæ–¹å¼å¿«é€Ÿçˆ¬å–
    - æ”¯æŒSeleniumæ–¹å¼å¤‡ç”¨ï¼ˆå¯é æ€§é«˜ï¼‰
    - è‡ªåŠ¨å¤„ç†"æŸ¥çœ‹æ›´å¤š"åˆ†é¡µ
    - æ”¯æŒæ‰¹é‡ä¸‹è½½æ–‡ç« è¯¦æƒ…å’Œå›¾ç‰‡
    
    Example:
        config = Config(bbs={"base_url": "https://example.com"})
        crawler = DynamicNewsCrawler(config)
        
        async with crawler:
            articles = await crawler.crawl_dynamic_page_ajax(
                "https://example.com/news",
                max_pages=5
            )
    """
    
    def __init__(self, config: Config):
        """
        åˆå§‹åŒ–åŠ¨æ€æ–°é—»çˆ¬è™«
        
        Args:
            config: é…ç½®å¯¹è±¡
        """
        self.config = config
        self.parser = DynamicPageParser(config)
        self.session = None
        self.ua = None
        
        # ç»Ÿè®¡ä¿¡æ¯ï¼ˆä¸ BaseSpider ä¿æŒä¸€è‡´çš„ç»“æ„ï¼‰
        self.stats = {
            'pages_fetched': 0,       # åŸºç¡€ç»Ÿè®¡
            'requests_failed': 0,     # åŸºç¡€ç»Ÿè®¡
            'articles_found': 0,      # å‘ç°çš„æ–‡ç« æ•°
            'articles_crawled': 0,    # æˆåŠŸçˆ¬å–çš„æ–‡ç« æ•°
            'articles_failed': 0,     # å¤±è´¥çš„æ–‡ç« æ•°
            'images_downloaded': 0,   # ä¸‹è½½çš„å›¾ç‰‡æ•°
            'images_failed': 0,       # å¤±è´¥çš„å›¾ç‰‡æ•°
        }
        
        logger.info(f"ğŸš€ åˆå§‹åŒ–åŠ¨æ€æ–°é—»çˆ¬è™«: {config.bbs.name}")
    
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        await self.init()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        await self.close()
    
    async def init(self):
        """åˆå§‹åŒ–çˆ¬è™«"""
        import aiohttp
        from fake_useragent import UserAgent
        
        logger.info("âš™ï¸  åˆå§‹åŒ–çˆ¬è™«ç»„ä»¶...")
        
        self.ua = UserAgent()
        
        # åˆ›å»ºHTTPä¼šè¯
        timeout = aiohttp.ClientTimeout(total=self.config.crawler.request_timeout)
        self.session = aiohttp.ClientSession(timeout=timeout)
        
        logger.debug("âœ“ HTTPä¼šè¯å·²åˆ›å»º")
    
    async def close(self):
        """å…³é—­çˆ¬è™«"""
        logger.info("ğŸ”’ å…³é—­çˆ¬è™«...")
        
        if self.session:
            await self.session.close()
            logger.debug("âœ“ HTTPä¼šè¯å·²å…³é—­")
        
        logger.info(f"ğŸ“Š çˆ¬è™«ç»Ÿè®¡: {self.get_statistics()}")
    
    def get_headers(self) -> Dict[str, str]:
        """è·å–è¯·æ±‚å¤´"""
        return {
            "User-Agent": self.ua.random if self.config.crawler.rotate_user_agent else self.ua.chrome,
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Accept-Encoding": "gzip, deflate",
            "Connection": "keep-alive",
        }
    
    async def fetch_page(self, url: str, headers: Optional[Dict] = None, is_ajax: bool = False) -> Optional[str]:
        """
        è·å–é¡µé¢å†…å®¹
        
        Args:
            url: é¡µé¢URL
            headers: å¯é€‰çš„HTTPå¤´
            is_ajax: æ˜¯å¦ä¸ºAjaxè¯·æ±‚ï¼ˆä¼šæ·»åŠ X-Requested-Withå¤´ï¼‰
        
        Returns:
            HTMLå†…å®¹ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            logger.debug(f"ğŸ“„ è·å–é¡µé¢: {url} (Ajax: {is_ajax})")
            
            # è·å–åŸºç¡€è¯·æ±‚å¤´
            request_headers = self.get_headers()
            
            # åˆå¹¶è‡ªå®šä¹‰ headers
            if headers:
                request_headers.update(headers)
            
            # Ajaxè¯·æ±‚éœ€è¦ç‰¹æ®Šå¤´
            if is_ajax:
                request_headers["X-Requested-With"] = "XMLHttpRequest"
            
            async with self.session.get(url, headers=request_headers) as response:
                if response.status == 200:
                    self.stats['pages_fetched'] += 1
                    html = await response.text()
                    await asyncio.sleep(self.config.crawler.download_delay)
                    return html
                else:
                    logger.warning(f"âš ï¸  HTTP {response.status}: {url}")
                    return None
        
        except asyncio.TimeoutError:
            self.stats['requests_failed'] += 1
            logger.error(f"âŒ è¶…æ—¶: {url}")
            return None
        except Exception as e:
            self.stats['requests_failed'] += 1
            logger.error(f"âŒ è·å–å¤±è´¥ {url}: {e}")
            return None
    
    async def crawl_dynamic_page_ajax(
        self, 
        base_url: str, 
        max_pages: Optional[int] = None
    ) -> List[Dict]:
        """
        ä½¿ç”¨Ajaxæ–¹å¼çˆ¬å–åŠ¨æ€é¡µé¢
        
        é€šè¿‡ç›´æ¥è¯·æ±‚åˆ†é¡µURLï¼ˆå¦‚ ?page=2ï¼‰æ¥è·å–æ›´å¤šå†…å®¹ã€‚
        è¿™ç§æ–¹å¼é€Ÿåº¦å¿«ã€èµ„æºå ç”¨å°‘ã€‚
        
        Args:
            base_url: åŸºç¡€URL
            max_pages: æœ€å¤§é¡µæ•°ï¼ŒNoneè¡¨ç¤ºä¸é™åˆ¶
        
        Returns:
            æ–‡ç« åˆ—è¡¨
        
        Example:
            articles = await crawler.crawl_dynamic_page_ajax(
                "https://sxd.xd.com/",
                max_pages=5
            )
        """
        logger.info(f"ğŸš€ å¼€å§‹çˆ¬å–åŠ¨æ€é¡µé¢ï¼ˆAjaxæ–¹å¼ï¼‰")
        logger.info(f"   URL: {base_url}")
        logger.info(f"   æœ€å¤§é¡µæ•°: {max_pages if max_pages else 'ä¸é™åˆ¶'}")
        
        page = 1
        all_articles = []
        
        # å»é‡é›†åˆï¼Œå­˜å‚¨å·²çˆ¬å–çš„æ–‡ç« ID
        seen_article_ids = set()
        
        while True:
            # æ„é€ åˆ†é¡µURL
            if page == 1:
                page_url = base_url
            else:
                # å°è¯•å¤šç§åˆ†é¡µURLæ ¼å¼
                separator = '&' if '?' in base_url else '?'
                page_url = f"{base_url}{separator}page={page}"
            
            logger.info(f"\nğŸ“„ çˆ¬å–ç¬¬ {page} é¡µ: {page_url}")
            
            # è·å–é¡µé¢å†…å®¹ï¼ˆåˆ†é¡µè¯·æ±‚éœ€è¦Ajaxå¤´ï¼‰
            html = await self.fetch_page(page_url, is_ajax=(page > 1))
            
            if not html:
                logger.warning(f"âš ï¸  ç¬¬{page}é¡µè·å–å¤±è´¥ï¼Œåœæ­¢çˆ¬å–")
                break
            
            # è§£ææ–‡ç« åˆ—è¡¨
            articles = self.parser.parse_articles(html)
            
            if not articles:
                logger.info(f"âœ… ç¬¬{page}é¡µæ²¡æœ‰æ–‡ç« ï¼Œåœæ­¢çˆ¬å–")
                break
            
            # è¿‡æ»¤é‡å¤æ–‡ç« 
            new_articles = []
            for article in articles:
                if article['article_id'] not in seen_article_ids:
                    seen_article_ids.add(article['article_id'])
                    new_articles.append(article)
            
            if not new_articles:
                logger.info(f"âœ… ç¬¬{page}é¡µæ²¡æœ‰æ–°æ–‡ç« ï¼ˆå…¨éƒ¨é‡å¤ï¼‰ï¼Œåœæ­¢çˆ¬å–")
                break
            
            logger.info(f"   âœ“ å‘ç° {len(new_articles)} ç¯‡æ–°æ–‡ç«  (æœ¬é¡µå…± {len(articles)} ç¯‡)")
            all_articles.extend(new_articles)
            self.stats['articles_found'] += len(new_articles)
            
            # æ£€æŸ¥é¡µæ•°é™åˆ¶
            if max_pages and page >= max_pages:
                logger.info(f"âœ… è¾¾åˆ°æœ€å¤§é¡µæ•°é™åˆ¶: {max_pages}")
                break
            
            # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰"æŸ¥çœ‹æ›´å¤š"æŒ‰é’® (è¾…åŠ©åˆ¤æ–­)
            has_more = self.parser.has_load_more_button(html)
            if not has_more:
                logger.info("âœ… æ²¡æœ‰æ›´å¤šå†…å®¹æ ‡è¯†ï¼Œåœæ­¢çˆ¬å–")
                break
            
            page += 1
        
        logger.success(f"ğŸ‰ å®Œæˆçˆ¬å–ï¼æ€»å…±å‘ç° {len(all_articles)} ç¯‡æ–°æ–‡ç« ")
        
        return all_articles
    
    async def crawl_dynamic_page_selenium(
        self, 
        url: str, 
        max_clicks: Optional[int] = None
    ) -> List[Dict]:
        """
        ä½¿ç”¨Seleniumæ–¹å¼çˆ¬å–åŠ¨æ€é¡µé¢ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
        
        é€šè¿‡æ¨¡æ‹Ÿæµè§ˆå™¨ç‚¹å‡»"æŸ¥çœ‹æ›´å¤š"æŒ‰é’®æ¥åŠ è½½å†…å®¹ã€‚
        è¿™ç§æ–¹å¼æ›´å¯é ï¼Œä½†é€Ÿåº¦è¾ƒæ…¢ã€èµ„æºå ç”¨å¤§ã€‚
        
        Args:
            url: é¡µé¢URL
            max_clicks: æœ€å¤§ç‚¹å‡»æ¬¡æ•°ï¼ŒNoneè¡¨ç¤ºä¸é™åˆ¶
        
        Returns:
            æ–‡ç« åˆ—è¡¨
        
        Note:
            éœ€è¦å®‰è£…selenium: pip install selenium
        
        Example:
            articles = await crawler.crawl_dynamic_page_selenium(
                "https://sxd.xd.com/",
                max_clicks=10
            )
        """
        logger.info(f"ğŸš€ å¼€å§‹çˆ¬å–åŠ¨æ€é¡µé¢ï¼ˆSeleniumæ–¹å¼ï¼‰")
        logger.info(f"   URL: {url}")
        logger.info(f"   æœ€å¤§ç‚¹å‡»æ¬¡æ•°: {max_clicks if max_clicks else 'ä¸é™åˆ¶'}")
        
        try:
            from selenium import webdriver
            from selenium.webdriver.common.by import By
            from selenium.webdriver.support.ui import WebDriverWait
            from selenium.webdriver.support import expected_conditions as EC
            from selenium.common.exceptions import TimeoutException, NoSuchElementException
        except ImportError:
            logger.error("âŒ ç¼ºå°‘Seleniumä¾èµ–ï¼Œè¯·å®‰è£…: pip install selenium")
            return []
        
        # é…ç½®Selenium
        options = webdriver.ChromeOptions()
        options.add_argument('--headless')  # æ— å¤´æ¨¡å¼
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        
        driver = None
        
        try:
            driver = webdriver.Chrome(options=options)
            driver.get(url)
            
            logger.info("âœ“ æµè§ˆå™¨å·²å¯åŠ¨")
            
            clicks = 0
            # è®°å½•ä¸Šä¸€æ¬¡çš„æ–‡ç« æ•°é‡ï¼Œç”¨äºåˆ¤æ–­æ˜¯å¦åŠ è½½æˆåŠŸ
            last_article_count = 0
            
            while True:
                # 1. æ£€æŸ¥å½“å‰æ–‡ç« æ•°é‡
                html = driver.page_source
                current_articles = self.parser.parse_articles(html)
                current_count = len(current_articles)
                logger.debug(f"å½“å‰æ–‡ç« æ•°: {current_count}")
                
                # å¦‚æœè¿™æ˜¯ç¬¬ä¸€æ¬¡ï¼Œåˆå§‹åŒ–last_article_count
                if clicks == 0:
                    last_article_count = current_count
                
                # 2. æ£€æŸ¥ç‚¹å‡»é™åˆ¶
                if max_clicks and clicks >= max_clicks:
                    logger.info(f"âœ… è¾¾åˆ°æœ€å¤§ç‚¹å‡»æ¬¡æ•°: {max_clicks}")
                    break
                
                # 3. å°è¯•ç‚¹å‡»"æŸ¥çœ‹æ›´å¤š"
                try:
                    # ç­‰å¾…æŒ‰é’®å‡ºç°ï¼ˆæ”¾å®½æ¡ä»¶ï¼Œä½¿ç”¨é€šç”¨é€‰æ‹©å™¨ï¼‰
                    load_more = WebDriverWait(driver, 10).until(
                        EC.presence_of_element_located((
                            By.CSS_SELECTOR, 
                            'a.more, .load-more, .btn-more'
                        ))
                    )
                    
                    # æ£€æŸ¥æŒ‰é’®æ˜¯å¦å¯è§
                    if not load_more.is_displayed():
                        logger.info("âš ï¸  'æŸ¥çœ‹æ›´å¤š'æŒ‰é’®ä¸å¯è§ï¼Œå¯èƒ½å·²åŠ è½½å®Œæ¯•")
                        break
                    
                    # æ»šåŠ¨åˆ°æŒ‰é’®ä½ç½®ï¼ˆå±…ä¸­æ˜¾ç¤ºï¼‰
                    driver.execute_script("arguments[0].scrollIntoView({behavior: 'smooth', block: 'center'});", load_more)
                    await asyncio.sleep(1)  # ç­‰å¾…æ»šåŠ¨å®Œæˆ
                    
                    # å¼ºåŠ›ç‚¹å‡»ï¼ˆJavaScriptç‚¹å‡»æ¯”åŸç”Ÿclickæ›´å¯é ï¼‰
                    driver.execute_script("arguments[0].click();", load_more)
                    clicks += 1
                    logger.info(f"ğŸ”„ ç‚¹å‡»'æŸ¥çœ‹æ›´å¤š' ç¬¬{clicks}æ¬¡")
                    
                    # 4. æ™ºèƒ½ç­‰å¾…å†…å®¹åŠ è½½
                    # è½®è¯¢æ£€æŸ¥æ–‡ç« æ•°é‡æ˜¯å¦å¢åŠ 
                    wait_time = 0
                    loaded = False
                    while wait_time < 10:  # æœ€å¤šç­‰å¾…10ç§’
                        await asyncio.sleep(1)
                        wait_time += 1
                        
                        new_html = driver.page_source
                        new_articles = self.parser.parse_articles(new_html)
                        new_count = len(new_articles)
                        
                        if new_count > last_article_count:
                            logger.info(f"   âœ“ åŠ è½½æˆåŠŸï¼æ–°å¢ {new_count - last_article_count} ç¯‡æ–‡ç« ")
                            last_article_count = new_count
                            loaded = True
                            break
                    
                    if not loaded:
                        logger.warning(f"âš ï¸  ç­‰å¾…10ç§’åæ–‡ç« æ•°é‡æœªå¢åŠ ï¼Œå¯èƒ½å·²åˆ°åº•æˆ–åŠ è½½å¤±è´¥")
                        # å°è¯•å†ç­‰ä¸€ä¼šå„¿ï¼Œæˆ–è€…é‡è¯•ä¸€æ¬¡
                        
                except TimeoutException:
                    logger.info("âœ… æ²¡æœ‰æ‰¾åˆ°'æŸ¥çœ‹æ›´å¤š'æŒ‰é’®ï¼Œåœæ­¢åŠ è½½")
                    break
                except Exception as e:
                    logger.error(f"âŒ ç‚¹å‡»è¿‡ç¨‹å‡ºé”™: {e}")
                    break
            
            # è·å–æœ€ç»ˆé¡µé¢HTML
            html = driver.page_source
            
            # è§£ææ‰€æœ‰æ–‡ç« 
            articles = self.parser.parse_articles(html)
            self.stats['articles_found'] = len(articles)
            
            logger.success(f"ğŸ‰ å®Œæˆçˆ¬å–ï¼æ€»å…±å‘ç° {len(articles)} ç¯‡æ–‡ç« ")
            
            return articles
            
        except Exception as e:
            logger.error(f"âŒ Seleniumçˆ¬å–å¤±è´¥: {e}")
            return []
        
        finally:
            if driver:
                driver.quit()
                logger.debug("âœ“ æµè§ˆå™¨å·²å…³é—­")
    
    async def crawl_article_detail(self, article: Dict) -> Optional[Dict]:
        """
        çˆ¬å–å•ç¯‡æ–‡ç« è¯¦æƒ…
        
        Args:
            article: æ–‡ç« åŸºæœ¬ä¿¡æ¯å­—å…¸
        
        Returns:
            æ–‡ç« è¯¦ç»†ä¿¡æ¯ï¼ˆåŒ…å«å®Œæ•´å†…å®¹å’Œå›¾ç‰‡åˆ—è¡¨ï¼‰
        """
        url = article.get('url')
        if not url:
            logger.warning("âš ï¸  æ–‡ç« ç¼ºå°‘URLï¼Œè·³è¿‡")
            return None
        
        logger.info(f"ğŸ“ çˆ¬å–æ–‡ç« è¯¦æƒ…: {article.get('title', 'N/A')[:50]}")
        
        try:
            html = await self.fetch_page(url)
            
            if not html:
                logger.error(f"âŒ æ— æ³•è·å–æ–‡ç« è¯¦æƒ…: {url}")
                self.stats['articles_failed'] += 1
                return None
            
            # è°ƒè¯•ï¼šä¿å­˜HTML
            if '15539' in url:
                with open('/tmp/debug_detail.html', 'w', encoding='utf-8') as f:
                    f.write(html)
                logger.info(f"ğŸ› å·²ä¿å­˜è°ƒè¯•HTMLåˆ° /tmp/debug_detail.html")
            
            # è§£ææ–‡ç« è¯¦æƒ…
            detail = self.parser.parse_article_detail(html, url)
            
            # åˆå¹¶åŸºæœ¬ä¿¡æ¯å’Œè¯¦æƒ…
            full_article = {**article, **detail}
            
            self.stats['articles_crawled'] += 1
            logger.success(f"   âœ“ æˆåŠŸï¼Œå‘ç° {len(detail.get('images', []))} å¼ å›¾ç‰‡")
            
            return full_article
            
        except Exception as e:
            logger.error(f"âŒ çˆ¬å–æ–‡ç« è¯¦æƒ…å¤±è´¥: {e}")
            self.stats['articles_failed'] += 1
            return None
    
    async def crawl_articles_batch(self, articles: List[Dict]) -> List[Dict]:
        """
        æ‰¹é‡çˆ¬å–æ–‡ç« è¯¦æƒ…
        
        Args:
            articles: æ–‡ç« åŸºæœ¬ä¿¡æ¯åˆ—è¡¨
        
        Returns:
            æ–‡ç« è¯¦ç»†ä¿¡æ¯åˆ—è¡¨
        """
        logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡çˆ¬å– {len(articles)} ç¯‡æ–‡ç« è¯¦æƒ…")
        
        tasks = [self.crawl_article_detail(article) for article in articles]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # è¿‡æ»¤æ‰å¤±è´¥çš„ç»“æœ
        full_articles = [r for r in results if r and not isinstance(r, Exception)]
        
        logger.success(f"âœ… æˆåŠŸçˆ¬å– {len(full_articles)}/{len(articles)} ç¯‡æ–‡ç« è¯¦æƒ…")
        
        return full_articles
    
    def get_statistics(self) -> Dict:
        """
        è·å–ç»Ÿè®¡ä¿¡æ¯
        
        ä¸ BaseSpider.get_statistics() ä¿æŒä¸€è‡´çš„æ¥å£
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        return self.stats.copy()
