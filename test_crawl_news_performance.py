#!/usr/bin/env python3
"""
æµ‹è¯• crawl-news æ€§èƒ½çš„è„šæœ¬

æµ‹è¯•å¼‚æ­¥é˜Ÿåˆ—çš„æ€§èƒ½æå‡æ•ˆæœ
"""
import asyncio
import time
from loguru import logger
from spiders.dynamic_news_spider import DynamicNewsCrawler
from config import Config, ConfigLoader

# æµ‹è¯•URLï¼ˆä½¿ç”¨ä¸€ä¸ªç®€å•çš„æµ‹è¯•é¡µé¢ï¼‰
TEST_URL = "https://sxd.xd.com/"  # å¯ä»¥æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹


async def test_performance(url: str, max_pages: int = 3, use_queue: bool = True, max_workers: int = 5):
    """
    æµ‹è¯•çˆ¬å–æ€§èƒ½
    
    Args:
        url: æµ‹è¯•URL
        max_pages: æœ€å¤§é¡µæ•°
        use_queue: æ˜¯å¦ä½¿ç”¨å¼‚æ­¥é˜Ÿåˆ—
        max_workers: å¹¶å‘æ•°
    """
    logger.info("=" * 60)
    logger.info(f"ğŸš€ å¼€å§‹æ€§èƒ½æµ‹è¯•")
    logger.info(f"   URL: {url}")
    logger.info(f"   æœ€å¤§é¡µæ•°: {max_pages}")
    logger.info(f"   ä½¿ç”¨å¼‚æ­¥é˜Ÿåˆ—: {use_queue}")
    logger.info(f"   å¹¶å‘æ•°: {max_workers}")
    logger.info("=" * 60)
    
    # åˆ›å»ºé…ç½®
    config = Config(
        bbs={
            "name": "æµ‹è¯•ç½‘ç«™",
            "base_url": url,
            "forum_type": "custom"
        },
        crawler={
            "max_concurrent_requests": max_workers,
            "use_async_queue": use_queue,
            "use_adaptive_queue": False,
            "download_delay": 0.5  # è¾ƒçŸ­çš„å»¶è¿Ÿç”¨äºæµ‹è¯•
        }
    )
    
    # åˆ›å»ºçˆ¬è™«
    crawler = DynamicNewsCrawler(config)
    
    start_time = time.time()
    
    async with crawler:
        # çˆ¬å–æ–‡ç« åˆ—è¡¨
        articles = await crawler.crawl_dynamic_page_ajax(
            url,
            max_pages=max_pages,
            resume=False,  # ä¸ä½¿ç”¨æ£€æŸ¥ç‚¹
            start_page=1
        )
        
        logger.info(f"âœ… å‘ç° {len(articles)} ç¯‡æ–‡ç« ")
        
        # æµ‹è¯•çˆ¬å–æ–‡ç« è¯¦æƒ…ï¼ˆä½¿ç”¨é˜Ÿåˆ—ï¼‰
        if articles:
            logger.info(f"ğŸš€ å¼€å§‹çˆ¬å–æ–‡ç« è¯¦æƒ…ï¼ˆä½¿ç”¨é˜Ÿåˆ—: {use_queue}ï¼‰...")
            detail_start = time.time()
            
            full_articles = await crawler.crawl_articles_batch(
                articles[:10],  # åªæµ‹è¯•å‰10ç¯‡
                use_queue=use_queue,
                max_workers=max_workers,
                use_adaptive=False
            )
            
            detail_time = time.time() - detail_start
            logger.info(f"âœ… å®Œæˆæ–‡ç« è¯¦æƒ…çˆ¬å–: {len(full_articles)} ç¯‡")
            logger.info(f"â±ï¸  è€—æ—¶: {detail_time:.2f} ç§’")
            logger.info(f"ğŸ“Š é€Ÿåº¦: {len(full_articles)/detail_time:.2f} ç¯‡/ç§’")
    
    total_time = time.time() - start_time
    
    # è·å–ç»Ÿè®¡ä¿¡æ¯
    stats = crawler.get_statistics()
    
    logger.info("=" * 60)
    logger.info("ğŸ“Š æ€§èƒ½æµ‹è¯•ç»“æœ:")
    logger.info(f"   æ€»è€—æ—¶: {total_time:.2f} ç§’")
    logger.info(f"   å‘ç°æ–‡ç« : {stats.get('articles_found', 0)}")
    logger.info(f"   çˆ¬å–æ–‡ç« : {stats.get('articles_crawled', 0)}")
    logger.info(f"   å¤±è´¥æ–‡ç« : {stats.get('articles_failed', 0)}")
    logger.info(f"   è·å–é¡µé¢: {stats.get('pages_fetched', 0)}")
    logger.info(f"   è¯·æ±‚å¤±è´¥: {stats.get('requests_failed', 0)}")
    logger.info("=" * 60)
    
    return {
        'total_time': total_time,
        'articles_found': stats.get('articles_found', 0),
        'articles_crawled': stats.get('articles_crawled', 0),
        'articles_failed': stats.get('articles_failed', 0),
        'pages_fetched': stats.get('pages_fetched', 0),
        'requests_failed': stats.get('requests_failed', 0)
    }


async def compare_performance(url: str, max_pages: int = 3):
    """å¯¹æ¯”ä¸²è¡Œå’Œå¹¶è¡Œçš„æ€§èƒ½"""
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ“Š æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
    logger.info("=" * 60)
    
    # æµ‹è¯•1: ä¸²è¡Œæ¨¡å¼ï¼ˆä¸ä½¿ç”¨é˜Ÿåˆ—ï¼‰
    logger.info("\nğŸ”¹ æµ‹è¯•1: ä¸²è¡Œæ¨¡å¼ï¼ˆä¸ä½¿ç”¨å¼‚æ­¥é˜Ÿåˆ—ï¼‰")
    result_serial = await test_performance(url, max_pages, use_queue=False, max_workers=1)
    
    # ç­‰å¾…ä¸€ä¸‹ï¼Œé¿å…å½±å“
    await asyncio.sleep(2)
    
    # æµ‹è¯•2: å¹¶è¡Œæ¨¡å¼ï¼ˆä½¿ç”¨é˜Ÿåˆ—ï¼Œ5ä¸ªå¹¶å‘ï¼‰
    logger.info("\nğŸ”¹ æµ‹è¯•2: å¹¶è¡Œæ¨¡å¼ï¼ˆä½¿ç”¨å¼‚æ­¥é˜Ÿåˆ—ï¼Œ5ä¸ªå¹¶å‘ï¼‰")
    result_parallel_5 = await test_performance(url, max_pages, use_queue=True, max_workers=5)
    
    # ç­‰å¾…ä¸€ä¸‹
    await asyncio.sleep(2)
    
    # æµ‹è¯•3: å¹¶è¡Œæ¨¡å¼ï¼ˆä½¿ç”¨é˜Ÿåˆ—ï¼Œ10ä¸ªå¹¶å‘ï¼‰
    logger.info("\nğŸ”¹ æµ‹è¯•3: å¹¶è¡Œæ¨¡å¼ï¼ˆä½¿ç”¨å¼‚æ­¥é˜Ÿåˆ—ï¼Œ10ä¸ªå¹¶å‘ï¼‰")
    result_parallel_10 = await test_performance(url, max_pages, use_queue=True, max_workers=10)
    
    # å¯¹æ¯”ç»“æœ
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ“ˆ æ€§èƒ½å¯¹æ¯”æ€»ç»“")
    logger.info("=" * 60)
    
    if result_serial['articles_crawled'] > 0:
        serial_speed = result_serial['articles_crawled'] / result_serial['total_time']
        parallel_5_speed = result_parallel_5['articles_crawled'] / result_parallel_5['total_time']
        parallel_10_speed = result_parallel_10['articles_crawled'] / result_parallel_10['total_time']
        
        logger.info(f"\nä¸²è¡Œæ¨¡å¼:")
        logger.info(f"  è€—æ—¶: {result_serial['total_time']:.2f} ç§’")
        logger.info(f"  é€Ÿåº¦: {serial_speed:.2f} ç¯‡/ç§’")
        
        logger.info(f"\nå¹¶è¡Œæ¨¡å¼ï¼ˆ5å¹¶å‘ï¼‰:")
        logger.info(f"  è€—æ—¶: {result_parallel_5['total_time']:.2f} ç§’")
        logger.info(f"  é€Ÿåº¦: {parallel_5_speed:.2f} ç¯‡/ç§’")
        logger.info(f"  æå‡: {parallel_5_speed/serial_speed:.2f}x")
        
        logger.info(f"\nå¹¶è¡Œæ¨¡å¼ï¼ˆ10å¹¶å‘ï¼‰:")
        logger.info(f"  è€—æ—¶: {result_parallel_10['total_time']:.2f} ç§’")
        logger.info(f"  é€Ÿåº¦: {parallel_10_speed:.2f} ç¯‡/ç§’")
        logger.info(f"  æå‡: {parallel_10_speed/serial_speed:.2f}x")
    
    logger.info("=" * 60)


if __name__ == "__main__":
    import sys
    
    # ä»å‘½ä»¤è¡Œè·å–URLï¼ˆå¯é€‰ï¼‰
    test_url = sys.argv[1] if len(sys.argv) > 1 else TEST_URL
    max_pages = int(sys.argv[2]) if len(sys.argv) > 2 else 3
    
    logger.info(f"ğŸ¯ æµ‹è¯•URL: {test_url}")
    logger.info(f"ğŸ“„ æœ€å¤§é¡µæ•°: {max_pages}")
    
    # è¿è¡Œæ€§èƒ½å¯¹æ¯”æµ‹è¯•
    asyncio.run(compare_performance(test_url, max_pages))
