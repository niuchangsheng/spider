"""
BBSå›¾ç‰‡çˆ¬è™« - CLIå…¥å£

v2.3 æ–‡ä»¶ç»“æ„é‡æ„åï¼Œspider.py ä»…ä½œä¸ºCLIå…¥å£ç‚¹ã€‚

æ¶æ„:
- core/base.py: BaseSpider + BaseParser åŸºç±»
- parsers/: è§£æå™¨æ¨¡å—
- spiders/: çˆ¬è™«æ¨¡å—
- cli/: CLIå¤„ç†æ¨¡å—
"""
import asyncio
import sys
from pathlib import Path
from loguru import logger

from cli import (
    create_parser, 
    handle_crawl_url, 
    handle_crawl_urls, 
    handle_crawl_board, 
    handle_crawl_boards, 
    handle_crawl_news,
    handle_checkpoint_status
)


async def main():
    """ä¸»å‡½æ•° - CLIå…¥å£"""
    # åˆ›å»ºå‚æ•°è§£æå™¨
    parser = create_parser()
    args = parser.parse_args()
    
    # é…ç½®æ—¥å¿—
    logger.remove()
    logger.add(
        sys.stderr,
        format="<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | <level>{message}</level>",
        level="INFO",
        colorize=True
    )
    
    log_file = Path(__file__).parent / "logs" / "spider.log"
    log_file.parent.mkdir(parents=True, exist_ok=True)
    logger.add(
        log_file,
        rotation="100 MB",
        retention="30 days",
        encoding="utf-8",
        level="DEBUG"
    )
    
    print("\n" + "=" * 60)
    print("ğŸ•·ï¸  BBSå›¾ç‰‡çˆ¬è™« (v2.3 - æ–‡ä»¶ç»“æ„é‡æ„)")
    print("=" * 60)
    
    # æ ¹æ®å­å‘½ä»¤æ‰§è¡Œç›¸åº”æ“ä½œ
    if args.command == 'crawl-url':
        await handle_crawl_url(args)
    elif args.command == 'crawl-urls':
        await handle_crawl_urls(args)
    elif args.command == 'crawl-board':
        await handle_crawl_board(args)
    elif args.command == 'crawl-boards':
        await handle_crawl_boards(args)
    elif args.command == 'crawl-news':
        await handle_crawl_news(args)
    elif args.command == 'checkpoint-status':
        await handle_checkpoint_status(args)


if __name__ == "__main__":
    asyncio.run(main())
