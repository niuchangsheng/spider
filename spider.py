"""
BBSå›¾ç‰‡çˆ¬è™« - ç»Ÿä¸€æ¶æ„
æ”¯æŒå¤šç§è®ºå›ç³»ç»Ÿï¼šDiscuzã€phpBBã€vBulletinç­‰
"""
import asyncio
import sys
import aiohttp
from typing import List, Dict, Any, Optional, Type
from loguru import logger
from pathlib import Path
from tqdm import tqdm
from fake_useragent import UserAgent

from config import Config, ConfigLoader, ForumPresets, get_example_config, get_forum_boards, get_forum_urls
from core.downloader import ImageDownloader
from core.parser import BBSParser
from core.storage import storage
from core.deduplicator import ImageDeduplicator


class BBSSpider:
    """
    BBSå›¾ç‰‡çˆ¬è™«åŸºç±»
    
    æä¾›é€šç”¨çš„çˆ¬å–é€»è¾‘ï¼Œå­ç±»å¯é‡å†™ç‰¹å®šæ–¹æ³•å®ç°è®ºå›ç‰¹å®šå¤„ç†
    """
    
    def __init__(self, config: Optional[Config] = None, url: Optional[str] = None, preset: Optional[str] = None):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            config: æ‰‹åŠ¨é…ç½®ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
            url: è®ºå›URLï¼Œè‡ªåŠ¨æ£€æµ‹é…ç½®
            preset: è®ºå›ç±»å‹é¢„è®¾ (discuz/phpbb/vbulletin)
        
        Note:
            æ¨èä½¿ç”¨ SpiderFactory.create() è€Œä¸æ˜¯ç›´æ¥å®ä¾‹åŒ–
        
        Examples:
            # âœ… æ¨èï¼šä½¿ç”¨é…ç½®æ–‡ä»¶ï¼ˆè‡ªåŠ¨åŠ è½½ configs/xindong.jsonï¼‰
            config = get_example_config("xindong")
            spider = SpiderFactory.create(config=config)
            
            # âœ… æ¨èï¼šä½¿ç”¨è®ºå›ç±»å‹é¢„è®¾
            spider = SpiderFactory.create(preset="discuz")
            
            # âœ… æ¨èï¼šè‡ªåŠ¨æ£€æµ‹
            spider = SpiderFactory.create(url="https://forum.com/board")
            
            # âš ï¸ ä¸æ¨èï¼šç›´æ¥å®ä¾‹åŒ–ï¼ˆé™¤éè‡ªå®šä¹‰å­ç±»ï¼‰
            spider = BBSSpider(preset="discuz")
        """
        # é…ç½®ä¼˜å…ˆçº§: config > preset > url
        if config:
            self.config = config
        elif preset:
            self.config = ConfigLoader.load(preset)
        elif url:
            self.config = ConfigLoader.auto_detect(url)
        else:
            raise ValueError("å¿…é¡»æä¾› configã€preset æˆ– url å‚æ•°ä¹‹ä¸€")
        
        self.parser = BBSParser()
        self.deduplicator = ImageDeduplicator(use_perceptual_hash=True)
        self.ua = UserAgent()
        self.session: Optional[aiohttp.ClientSession] = None
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "threads_crawled": 0,
            "images_found": 0,
            "images_downloaded": 0,
            "images_failed": 0,
            "duplicates_skipped": 0
        }
        
        logger.info(f"ğŸš€ åˆå§‹åŒ–çˆ¬è™«: {self.config.bbs.name} ({self.config.bbs.forum_type})")
    
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        await self.init()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨é€€å‡º"""
        await self.close()
    
    async def init(self):
        """åˆå§‹åŒ–çˆ¬è™«"""
        logger.info("âš™ï¸  åˆå§‹åŒ–çˆ¬è™«ç»„ä»¶...")
        
        # åˆå§‹åŒ–HTTPä¼šè¯
        timeout = aiohttp.ClientTimeout(total=self.config.crawler.request_timeout)
        self.session = aiohttp.ClientSession(timeout=timeout)
        
        # è¿æ¥æ•°æ®åº“
        storage.connect()
        
        # åŠ è½½å·²å­˜åœ¨çš„æ–‡ä»¶å“ˆå¸Œ
        if self.config.image.enable_deduplication:
            self.deduplicator.load_existing_hashes(self.config.image.download_dir)
        
        logger.success("âœ… çˆ¬è™«åˆå§‹åŒ–å®Œæˆ")
    
    async def close(self):
        """å…³é—­çˆ¬è™«"""
        logger.info("ğŸ”’ å…³é—­çˆ¬è™«...")
        
        if self.session:
            await self.session.close()
        
        storage.close()
        
        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        logger.info(f"ğŸ“Š çˆ¬è™«ç»Ÿè®¡: {self.stats}")
        logger.info(f"ğŸ”„ å»é‡ç»Ÿè®¡: {self.deduplicator.get_stats()}")
    
    def get_headers(self) -> Dict[str, str]:
        """è·å–è¯·æ±‚å¤´"""
        headers = {
            "User-Agent": self.ua.random if self.config.crawler.rotate_user_agent else self.ua.chrome,
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Accept-Encoding": "gzip, deflate",
            "Connection": "keep-alive",
        }
        
        if self.config.bbs.base_url:
            headers["Referer"] = self.config.bbs.base_url
        
        return headers
    
    async def fetch_page(self, url: str) -> Optional[str]:
        """è·å–é¡µé¢å†…å®¹"""
        try:
            logger.debug(f"ğŸ“„ è·å–é¡µé¢: {url}")
            
            async with self.session.get(url, headers=self.get_headers()) as response:
                if response.status == 200:
                    html = await response.text()
                    await asyncio.sleep(self.config.crawler.download_delay)
                    return html
                else:
                    logger.warning(f"âš ï¸  è·å–å¤±è´¥ {url}: HTTP {response.status}")
                    return None
        
        except Exception as e:
            logger.error(f"âŒ è·å–å‡ºé”™ {url}: {e}")
            return None
    
    async def process_images(self, images: List[str]) -> List[str]:
        """
        å¤„ç†å›¾ç‰‡URLï¼ˆé’©å­æ–¹æ³•ï¼‰
        
        å­ç±»å¯é‡å†™æ­¤æ–¹æ³•å®ç°è®ºå›ç‰¹å®šçš„å›¾ç‰‡å¤„ç†é€»è¾‘
        
        Args:
            images: åŸå§‹å›¾ç‰‡URLåˆ—è¡¨
        
        Returns:
            å¤„ç†åçš„å›¾ç‰‡URLåˆ—è¡¨
        """
        return images
    
    async def crawl_board(self, board_url: str, board_name: str, max_pages: Optional[int] = None):
        """
        çˆ¬å–æ¿å—
        
        Args:
            board_url: æ¿å—URL
            board_name: æ¿å—åç§°
            max_pages: æœ€å¤§é¡µæ•°
        """
        logger.info(f"ğŸ“š å¼€å§‹çˆ¬å–æ¿å—: {board_name}")
        
        current_url = board_url
        page_count = 0
        
        while current_url and (max_pages is None or page_count < max_pages):
            page_count += 1
            logger.info(f"ğŸ“„ çˆ¬å–ç¬¬ {page_count} é¡µ: {current_url}")
            
            # è·å–åˆ—è¡¨é¡µ
            html = await self.fetch_page(current_url)
            if not html:
                break
            
            # è§£æå¸–å­åˆ—è¡¨
            threads = self.parser.parse_thread_list(html, current_url)
            logger.info(f"âœ… å‘ç° {len(threads)} ä¸ªå¸–å­")
            
            # çˆ¬å–æ¯ä¸ªå¸–å­
            for thread in threads:
                thread['board'] = board_name
                await self.crawl_thread(thread)
            
            # æŸ¥æ‰¾ä¸‹ä¸€é¡µ
            current_url = self.parser.find_next_page(html, current_url)
            if not current_url:
                logger.info("ğŸ“Œ æ²¡æœ‰æ›´å¤šé¡µé¢")
                break
        
        logger.success(f"ğŸ‰ æ¿å—çˆ¬å–å®Œæˆ: {board_name}, æ€»é¡µæ•°: {page_count}")
    
    async def crawl_thread(self, thread_info: Dict[str, Any]):
        """
        çˆ¬å–å•ä¸ªå¸–å­
        
        Args:
            thread_info: å¸–å­ä¿¡æ¯å­—å…¸
        """
        thread_url = thread_info['url']
        thread_id = thread_info['thread_id']
        
        # æ£€æŸ¥æ˜¯å¦å·²çˆ¬å–
        if storage.thread_exists(thread_id):
            logger.info(f"â­ï¸  å¸–å­ {thread_id} å·²çˆ¬å–ï¼Œè·³è¿‡")
            return
        
        logger.info(f"ğŸ“ çˆ¬å–å¸–å­: {thread_info.get('title', thread_id)}")
        
        # è·å–å¸–å­é¡µé¢
        html = await self.fetch_page(thread_url)
        if not html:
            return
        
        # è§£æå¸–å­å†…å®¹
        thread_data = self.parser.parse_thread_page(html, thread_url)
        thread_data['board'] = thread_info.get('board')
        thread_data['title'] = thread_info.get('title')
        
        # è®ºå›ç‰¹å®šå¤„ç†ï¼ˆç­–ç•¥æ¨¡å¼ - å­ç±»å¯é‡å†™ï¼‰
        thread_data['images'] = await self.process_images(thread_data['images'])
        
        # æ›´æ–°ç»Ÿè®¡
        self.stats['threads_crawled'] += 1
        self.stats['images_found'] += len(thread_data['images'])
        
        logger.info(f"ğŸ–¼ï¸  å‘ç° {len(thread_data['images'])} å¼ å›¾ç‰‡")
        
        # ä¸‹è½½å›¾ç‰‡
        if thread_data['images']:
            await self.download_thread_images(thread_data)
        
        # ä¿å­˜å¸–å­æ•°æ®
        storage.save_thread(thread_data)
    
    async def download_thread_images(self, thread_data: Dict[str, Any]):
        """ä¸‹è½½å¸–å­ä¸­çš„å›¾ç‰‡"""
        images = thread_data['images']
        thread_id = thread_data['thread_id']
        board = thread_data.get('board', 'unknown')
        
        # è¿‡æ»¤é‡å¤URL
        unique_images = []
        for img_url in images:
            if self.config.image.enable_deduplication:
                if not self.deduplicator.is_duplicate_url(img_url):
                    unique_images.append(img_url)
                else:
                    self.stats['duplicates_skipped'] += 1
            else:
                unique_images.append(img_url)
        
        if not unique_images:
            logger.info(f"â­ï¸  æ²¡æœ‰æ–°å›¾ç‰‡éœ€è¦ä¸‹è½½")
            return
        
        logger.info(f"â¬‡ï¸  ä¸‹è½½ {len(unique_images)} å¼ å›¾ç‰‡...")
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        save_dir = self.config.image.download_dir / board / thread_id
        save_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¸‹è½½å›¾ç‰‡
        async with ImageDownloader() as downloader:
            metadata = {
                'board': board,
                'thread_id': thread_id,
                'thread_url': thread_data['url']
            }
            
            results = await downloader.download_batch(
                unique_images,
                save_dir,
                metadata
            )
            
            # ç»Ÿè®¡ç»“æœ
            for result in results:
                if result.get('success'):
                    self.stats['images_downloaded'] += 1
                    
                    # æ£€æŸ¥æ–‡ä»¶å»é‡
                    if self.config.image.enable_deduplication:
                        file_path = Path(result['save_path'])
                        if self.deduplicator.is_duplicate_file(file_path):
                            self.deduplicator.remove_duplicate_file(file_path)
                            self.stats['duplicates_skipped'] += 1
                            self.stats['images_downloaded'] -= 1
                            continue
                    
                    # ä¿å­˜å›¾ç‰‡è®°å½•
                    storage.save_image_record(result)
                elif result.get('skipped'):
                    # è¢«è·³è¿‡çš„å›¾ç‰‡ï¼ˆå·²å­˜åœ¨/å°ºå¯¸ä¸ç¬¦ç­‰ï¼‰
                    self.stats['duplicates_skipped'] += 1
                else:
                    # çœŸæ­£ä¸‹è½½å¤±è´¥çš„å›¾ç‰‡
                    self.stats['images_failed'] += 1
    
    async def crawl_threads_from_list(self, thread_urls: List[str]):
        """
        ä»URLåˆ—è¡¨çˆ¬å–å¸–å­
        
        Args:
            thread_urls: å¸–å­URLåˆ—è¡¨
        """
        logger.info(f"ğŸ“‹ æ‰¹é‡çˆ¬å– {len(thread_urls)} ä¸ªå¸–å­")
        
        for url in tqdm(thread_urls, desc="çˆ¬å–è¿›åº¦"):
            thread_info = {
                'url': url,
                'thread_id': self.parser._extract_thread_id(url)
            }
            await self.crawl_thread(thread_info)
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.stats.copy()
        stats['deduplication'] = self.deduplicator.get_stats()
        stats['storage'] = storage.get_statistics()
        return stats


class DiscuzSpider(BBSSpider):
    """
    Discuzè®ºå›ä¸“ç”¨çˆ¬è™«
    
    å¤„ç†Discuzç‰¹æœ‰çš„å›¾ç‰‡é“¾æ¥æ ¼å¼å’Œé™„ä»¶ç³»ç»Ÿ
    """
    
    async def process_images(self, images: List[str]) -> List[str]:
        """
        å¤„ç†Discuzç‰¹æ®Šå›¾ç‰‡é“¾æ¥
        
        Discuzçš„é™„ä»¶é“¾æ¥æ ¼å¼: forum.php?mod=attachment&aid=xxx
        éœ€è¦æ·»åŠ  &nothumb=yes å‚æ•°è·å–åŸå›¾
        """
        processed_images = []
        
        for img_url in images:
            # å¤„ç†ç›¸å¯¹è·¯å¾„
            if img_url.startswith('forum.php') or img_url.startswith('/forum.php'):
                img_url = f"{self.config.bbs.base_url}/{img_url.lstrip('/')}"
            
            # Discuzé™„ä»¶é“¾æ¥éœ€è¦æ·»åŠ åŸå›¾å‚æ•°
            if 'mod=attachment' in img_url and 'nothumb' not in img_url:
                img_url += '&nothumb=yes'
            
            processed_images.append(img_url)
        
        return processed_images


class PhpBBSpider(BBSSpider):
    """
    phpBBè®ºå›ä¸“ç”¨çˆ¬è™«
    
    å¤„ç†phpBBç‰¹æœ‰çš„ç»“æ„
    """
    pass  # æš‚æ—¶ä½¿ç”¨åŸºç±»é€»è¾‘ï¼Œå¯æ ¹æ®éœ€è¦æ‰©å±•


class VBulletinSpider(BBSSpider):
    """
    vBulletinè®ºå›ä¸“ç”¨çˆ¬è™«
    
    å¤„ç†vBulletinç‰¹æœ‰çš„ç»“æ„
    """
    pass  # æš‚æ—¶ä½¿ç”¨åŸºç±»é€»è¾‘ï¼Œå¯æ ¹æ®éœ€è¦æ‰©å±•


# ============================================================================
# çˆ¬è™«å·¥å‚
# ============================================================================

class SpiderFactory:
    """
    çˆ¬è™«å·¥å‚ç±»
    
    æ ¹æ®é…ç½®çš„è®ºå›ç±»å‹è‡ªåŠ¨åˆ›å»ºåˆé€‚çš„çˆ¬è™«å®ä¾‹
    """
    
    # çˆ¬è™«ç±»å‹æ³¨å†Œè¡¨
    _registry: Dict[str, Type[BBSSpider]] = {
        'discuz': DiscuzSpider,
        'phpbb': PhpBBSpider,
        'vbulletin': VBulletinSpider,
        'generic': BBSSpider,
    }
    
    @classmethod
    def register(cls, forum_type: str, spider_class: Type[BBSSpider]):
        """
        æ³¨å†Œæ–°çš„çˆ¬è™«ç±»å‹
        
        Args:
            forum_type: è®ºå›ç±»å‹æ ‡è¯†
            spider_class: çˆ¬è™«ç±»
        
        Examples:
            SpiderFactory.register('mybb', MyBBSpider)
        """
        cls._registry[forum_type] = spider_class
        logger.info(f"âœ… æ³¨å†Œçˆ¬è™«ç±»å‹: {forum_type} -> {spider_class.__name__}")
    
    @classmethod
    def create(cls, config: Optional[Config] = None, url: Optional[str] = None, preset: Optional[str] = None) -> BBSSpider:
        """
        åˆ›å»ºçˆ¬è™«å®ä¾‹ï¼ˆå·¥å‚æ–¹æ³•ï¼‰
        
        Args:
            config: é…ç½®å¯¹è±¡ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
            url: è®ºå›URLï¼Œè‡ªåŠ¨æ£€æµ‹é…ç½®
            preset: è®ºå›ç±»å‹é¢„è®¾ (discuz/phpbb/vbulletin)
        
        Returns:
            å¯¹åº”ç±»å‹çš„çˆ¬è™«å®ä¾‹ (BBSSpider æˆ–å…¶å­ç±»)
        
        Examples:
            # âœ… æ–¹å¼1: ä½¿ç”¨é…ç½®æ–‡ä»¶ï¼ˆæ¨èï¼‰
            from config import get_example_config
            config = get_example_config("xindong")  # è‡ªåŠ¨åŠ è½½ configs/xindong.json
            spider = SpiderFactory.create(config=config)
            
            # âœ… æ–¹å¼2: ä½¿ç”¨è®ºå›ç±»å‹é¢„è®¾
            spider = SpiderFactory.create(preset="discuz")
            
            # âœ… æ–¹å¼3: è‡ªåŠ¨æ£€æµ‹è®ºå›ç±»å‹
            spider = SpiderFactory.create(url="https://forum.com/board")
            
            # âœ… æ–¹å¼4: å®Œå…¨è‡ªå®šä¹‰é…ç½®
            from config import Config
            custom_config = Config(bbs={...}, crawler={...})
            spider = SpiderFactory.create(config=custom_config)
        """
        # å…ˆè·å–é…ç½®
        if config:
            final_config = config
        elif preset:
            final_config = ConfigLoader.load(preset)
        elif url:
            final_config = ConfigLoader.auto_detect(url)
        else:
            raise ValueError("å¿…é¡»æä¾› configã€preset æˆ– url å‚æ•°ä¹‹ä¸€")
        
        # æ ¹æ®forum_typeé€‰æ‹©çˆ¬è™«ç±»
        forum_type = final_config.bbs.forum_type.lower()
        spider_class = cls._registry.get(forum_type, BBSSpider)
        
        logger.info(f"ğŸ­ åˆ›å»ºçˆ¬è™«: {spider_class.__name__}")
        
        return spider_class(config=final_config)


# ============================================================================
# CLI æ¥å£ - v2.1 å­å‘½ä»¤æ¨¡å¼
# ============================================================================

async def handle_crawl_url(args):
    """å¤„ç† crawl-url å­å‘½ä»¤"""
    from config import ConfigLoader, get_example_config
    
    print(f"\nğŸ“Œ å‘½ä»¤: çˆ¬å–å•ä¸ªURL")
    print(f"URL: {args.url}")
    
    # 1. åŠ è½½é…ç½®
    if args.auto_detect:
        logger.info(f"ğŸŒ è‡ªåŠ¨æ£€æµ‹é…ç½®: {args.url}")
        config = ConfigLoader.auto_detect(args.url)
    elif args.preset:
        logger.info(f"ğŸ“‹ ä½¿ç”¨è®ºå›ç±»å‹é¢„è®¾: {args.preset}")
        config = ConfigLoader.load(args.preset)
    elif args.config:
        logger.info(f"ğŸ“ ä½¿ç”¨é…ç½®æ–‡ä»¶: {args.config}")
        config = get_example_config(args.config)
    else:
        logger.error("âŒ è¯·æŒ‡å®šé…ç½®æ¥æº: --auto-detect æˆ– --preset æˆ– --config")
        return
    
    # 2. åˆ›å»ºçˆ¬è™«
    spider = SpiderFactory.create(config=config)
    
    # 3. çˆ¬å–URL
    async with spider:
        thread_id = spider.parser._extract_thread_id(args.url)
        thread_info = {
            'url': args.url,
            'thread_id': thread_id,
            'title': f'Thread-{thread_id}',
            'board': config.bbs.name
        }
        
        logger.info(f"ğŸš€ å¼€å§‹çˆ¬å–URL...")
        await spider.crawl_thread(thread_info)
        logger.info(f"âœ… çˆ¬å–å®Œæˆ")
        
        # è¾“å‡ºç»Ÿè®¡
        print_statistics(spider)


async def handle_crawl_urls(args):
    """å¤„ç† crawl-urls å­å‘½ä»¤"""
    from config import get_example_config, get_forum_urls
    
    print(f"\nğŸ“Œ å‘½ä»¤: çˆ¬å–é…ç½®ä¸­çš„URLåˆ—è¡¨")
    print(f"é…ç½®: {args.config}")
    
    # 1. åŠ è½½é…ç½®
    logger.info(f"ğŸ“ ä½¿ç”¨é…ç½®æ–‡ä»¶: {args.config}")
    config = get_example_config(args.config)
    
    # 2. è·å–URLåˆ—è¡¨
    urls = get_forum_urls(args.config)
    logger.info(f"ğŸ“ ä»é…ç½®æ–‡ä»¶åŠ è½½URL: {len(urls)} ä¸ª")
    
    if not urls:
        logger.error("âŒ é…ç½®æ–‡ä»¶ä¸­æ²¡æœ‰URLsï¼")
        return
    
    # 3. åˆ›å»ºçˆ¬è™«å¹¶å¹¶å‘çˆ¬å–
    spider = SpiderFactory.create(config=config)
    
    async with spider:
        logger.info(f"ğŸš€ å¼€å§‹å¹¶å‘çˆ¬å– {len(urls)} ä¸ªURL...")
        tasks = []
        for url in urls:
            thread_id = spider.parser._extract_thread_id(url)
            thread_info = {
                'url': url,
                'thread_id': thread_id,
                'title': f'Thread-{thread_id}',
                'board': config.bbs.name
            }
            tasks.append(spider.crawl_thread(thread_info))
        
        # ä½¿ç”¨ asyncio.gather å¹¶å‘æ‰§è¡Œ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for r in results if not isinstance(r, Exception))
        failed_count = len(results) - success_count
        logger.info(f"âœ… å®Œæˆ: æˆåŠŸ {success_count}, å¤±è´¥ {failed_count}")
        
        # è¾“å‡ºç»Ÿè®¡
        print_statistics(spider)


async def handle_crawl_board(args):
    """å¤„ç† crawl-board å­å‘½ä»¤"""
    from config import ConfigLoader, get_example_config
    
    print(f"\nğŸ“Œ å‘½ä»¤: çˆ¬å–å•ä¸ªæ¿å—")
    print(f"æ¿å—URL: {args.board_url}")
    if args.max_pages:
        print(f"æœ€å¤§é¡µæ•°: {args.max_pages}")
    else:
        print(f"æœ€å¤§é¡µæ•°: ä¸é™åˆ¶ï¼ˆçˆ¬å–æ‰€æœ‰é¡µï¼‰")
    
    # 1. åŠ è½½é…ç½®
    if args.auto_detect:
        logger.info(f"ğŸŒ è‡ªåŠ¨æ£€æµ‹é…ç½®: {args.board_url}")
        config = ConfigLoader.auto_detect(args.board_url)
    elif args.preset:
        logger.info(f"ğŸ“‹ ä½¿ç”¨è®ºå›ç±»å‹é¢„è®¾: {args.preset}")
        config = ConfigLoader.load(args.preset)
    elif args.config:
        logger.info(f"ğŸ“ ä½¿ç”¨é…ç½®æ–‡ä»¶: {args.config}")
        config = get_example_config(args.config)
    else:
        logger.error("âŒ è¯·æŒ‡å®šé…ç½®æ¥æº: --auto-detect æˆ– --preset æˆ– --config")
        return
    
    # 2. åˆ›å»ºçˆ¬è™«
    spider = SpiderFactory.create(config=config)
    
    # 3. çˆ¬å–æ¿å—
    async with spider:
        logger.info(f"ğŸš€ å¼€å§‹çˆ¬å–æ¿å—...")
        await spider.crawl_board(
            board_url=args.board_url,
            board_name=config.bbs.name,
            max_pages=args.max_pages
        )
        logger.info(f"âœ… çˆ¬å–å®Œæˆ")
        
        # è¾“å‡ºç»Ÿè®¡
        print_statistics(spider)


async def handle_crawl_boards(args):
    """å¤„ç† crawl-boards å­å‘½ä»¤"""
    from config import get_example_config, get_forum_boards
    
    print(f"\nğŸ“Œ å‘½ä»¤: çˆ¬å–é…ç½®ä¸­çš„æ‰€æœ‰æ¿å—")
    print(f"é…ç½®: {args.config}")
    if args.max_pages:
        print(f"æ¯ä¸ªæ¿å—æœ€å¤§é¡µæ•°: {args.max_pages}")
    else:
        print(f"æ¯ä¸ªæ¿å—æœ€å¤§é¡µæ•°: ä¸é™åˆ¶ï¼ˆçˆ¬å–æ‰€æœ‰é¡µï¼‰")
    
    # 1. åŠ è½½é…ç½®
    logger.info(f"ğŸ“ ä½¿ç”¨é…ç½®æ–‡ä»¶: {args.config}")
    config = get_example_config(args.config)
    
    # 2. è·å–æ¿å—åˆ—è¡¨
    boards_info = get_forum_boards(args.config)
    logger.info(f"ğŸ“ ä»é…ç½®æ–‡ä»¶åŠ è½½æ¿å—: {len(boards_info)} ä¸ª")
    
    if not boards_info:
        logger.error("âŒ é…ç½®æ–‡ä»¶ä¸­æ²¡æœ‰æ¿å—ï¼")
        return
    
    # 3. åˆ›å»ºçˆ¬è™«å¹¶å¹¶å‘çˆ¬å–
    spider = SpiderFactory.create(config=config)
    
    async with spider:
        if args.max_pages:
            logger.info(f"ğŸš€ å¼€å§‹å¹¶å‘çˆ¬å– {len(boards_info)} ä¸ªæ¿å—ï¼ˆæ¯ä¸ªæœ€å¤š {args.max_pages} é¡µï¼‰...")
        else:
            logger.info(f"ğŸš€ å¼€å§‹å¹¶å‘çˆ¬å– {len(boards_info)} ä¸ªæ¿å—ï¼ˆçˆ¬å–æ‰€æœ‰é¡µé¢ï¼‰...")
        
        tasks = []
        for board in boards_info:
            logger.info(f"ğŸ“ æ¿å—: {board['name']} - {board['url']}")
            tasks.append(spider.crawl_board(
                board_url=board['url'],
                board_name=board['name'],
                max_pages=args.max_pages
            ))
        
        # ä½¿ç”¨ asyncio.gather å¹¶å‘æ‰§è¡Œ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for r in results if not isinstance(r, Exception))
        failed_count = len(results) - success_count
        logger.info(f"âœ… å®Œæˆ: æˆåŠŸ {success_count}, å¤±è´¥ {failed_count}")
        
        # è¾“å‡ºç»Ÿè®¡
        print_statistics(spider)


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

async def main():
    """ä¸»å‡½æ•° - å­å‘½ä»¤æ¨¡å¼ (v2.1)"""
    # ä¸»è§£æå™¨
    parser = argparse.ArgumentParser(
        prog='spider.py',
        description='BBSå›¾ç‰‡çˆ¬è™« (v2.1 - å­å‘½ä»¤æ¨¡å¼)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
ç¤ºä¾‹:
  # çˆ¬å–å•ä¸ªURLï¼ˆè‡ªåŠ¨æ£€æµ‹é…ç½®ï¼‰
  python spider.py crawl-url "https://bbs.xd.com/thread/123" --auto-detect
  
  # çˆ¬å–å•ä¸ªURLï¼ˆä½¿ç”¨é…ç½®ï¼‰
  python spider.py crawl-url "https://bbs.xd.com/thread/123" --config xindong
  
  # çˆ¬å–é…ç½®ä¸­çš„æ‰€æœ‰URLs
  python spider.py crawl-urls --config xindong
  
  # çˆ¬å–æ¿å—ï¼ˆå‰5é¡µï¼‰
  python spider.py crawl-board "https://bbs.xd.com/forum?fid=21" --config xindong --max-pages 5
  
  # çˆ¬å–é…ç½®ä¸­çš„æ‰€æœ‰æ¿å—
  python spider.py crawl-boards --config xindong
        '''
    )
    
    # åˆ›å»ºå­å‘½ä»¤
    subparsers = parser.add_subparsers(dest='command', help='å­å‘½ä»¤', required=True)
    
    # ============================================================================
    # å­å‘½ä»¤1: crawl-url - çˆ¬å–å•ä¸ªURL
    # ============================================================================
    parser_url = subparsers.add_parser('crawl-url', help='çˆ¬å–å•ä¸ªURL')
    parser_url.add_argument('url', type=str, help='å¸–å­URL')
    
    config_group_url = parser_url.add_mutually_exclusive_group()
    config_group_url.add_argument('--auto-detect', action='store_true',
                                  help='è‡ªåŠ¨æ£€æµ‹è®ºå›ç±»å‹')
    config_group_url.add_argument('--preset', type=str,
                                  help='è®ºå›ç±»å‹é¢„è®¾ (discuz/phpbb/vbulletin)')
    config_group_url.add_argument('--config', type=str,
                                  help='é…ç½®æ–‡ä»¶å (ä» configs/ åŠ è½½)')
    
    # ============================================================================
    # å­å‘½ä»¤2: crawl-urls - çˆ¬å–é…ç½®ä¸­çš„URLåˆ—è¡¨
    # ============================================================================
    parser_urls = subparsers.add_parser('crawl-urls', help='çˆ¬å–é…ç½®ä¸­çš„URLåˆ—è¡¨')
    parser_urls.add_argument('--config', type=str, required=True,
                            help='é…ç½®æ–‡ä»¶å (å¿…éœ€)')
    
    # ============================================================================
    # å­å‘½ä»¤3: crawl-board - çˆ¬å–å•ä¸ªæ¿å—
    # ============================================================================
    parser_board = subparsers.add_parser('crawl-board', help='çˆ¬å–å•ä¸ªæ¿å—')
    parser_board.add_argument('board_url', type=str, help='æ¿å—URL')
    parser_board.add_argument('--max-pages', type=int, default=None,
                             help='æœ€å¤§é¡µæ•°ï¼ˆé»˜è®¤ï¼šçˆ¬å–æ‰€æœ‰é¡µï¼‰')
    
    config_group_board = parser_board.add_mutually_exclusive_group()
    config_group_board.add_argument('--auto-detect', action='store_true',
                                   help='è‡ªåŠ¨æ£€æµ‹è®ºå›ç±»å‹')
    config_group_board.add_argument('--preset', type=str,
                                   help='è®ºå›ç±»å‹é¢„è®¾ (discuz/phpbb/vbulletin)')
    config_group_board.add_argument('--config', type=str,
                                   help='é…ç½®æ–‡ä»¶å (ä» configs/ åŠ è½½)')
    
    # ============================================================================
    # å­å‘½ä»¤4: crawl-boards - çˆ¬å–é…ç½®ä¸­çš„æ‰€æœ‰æ¿å—
    # ============================================================================
    parser_boards = subparsers.add_parser('crawl-boards', help='çˆ¬å–é…ç½®ä¸­çš„æ‰€æœ‰æ¿å—')
    parser_boards.add_argument('--config', type=str, required=True,
                              help='é…ç½®æ–‡ä»¶å (å¿…éœ€)')
    parser_boards.add_argument('--max-pages', type=int, default=None,
                              help='æ¯ä¸ªæ¿å—æœ€å¤§é¡µæ•°ï¼ˆé»˜è®¤ï¼šçˆ¬å–æ‰€æœ‰é¡µï¼‰')
    
    args = parser.parse_args()
    
    # é…ç½®æ—¥å¿—
    logger.remove()
    logger.add(
        sys.stderr,
        format="<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | <level>{message}</level>",
        level="INFO",
        colorize=True
    )
    
    log_file = Path(__file__).parent / "logs" / "spider.log"
    log_file.parent.mkdir(parents=True, exist_ok=True)
    logger.add(
        log_file,
        rotation="100 MB",
        retention="30 days",
        encoding="utf-8",
        level="DEBUG"
    )
    
    print("\n" + "=" * 60)
    print("ğŸ•·ï¸  BBSå›¾ç‰‡çˆ¬è™« (v2.1 - å­å‘½ä»¤æ¨¡å¼)")
    print("=" * 60)
    
    # æ ¹æ®å­å‘½ä»¤æ‰§è¡Œç›¸åº”æ“ä½œ
    if args.command == 'crawl-url':
        await handle_crawl_url(args)
    elif args.command == 'crawl-urls':
        await handle_crawl_urls(args)
    elif args.command == 'crawl-board':
        await handle_crawl_board(args)
    elif args.command == 'crawl-boards':
        await handle_crawl_boards(args)


# ============================================================================
# è¾…åŠ©å‡½æ•°
# ============================================================================

def print_statistics(spider):
    """è¾“å‡ºç»Ÿè®¡ä¿¡æ¯"""
    stats = spider.get_statistics()
    print("\n" + "=" * 60)
    print("ğŸ“Š çˆ¬å–ç»Ÿè®¡:")
    print(f"  å¸–å­æ•°: {stats['threads_crawled']}")
    print(f"  å‘ç°å›¾ç‰‡: {stats['images_found']}")
    print(f"  ä¸‹è½½æˆåŠŸ: {stats['images_downloaded']}")
    print(f"  ä¸‹è½½å¤±è´¥: {stats['images_failed']}")
    print(f"  å»é‡è·³è¿‡: {stats['duplicates_skipped']}")
    print("=" * 60)


if __name__ == "__main__":
    asyncio.run(main())
