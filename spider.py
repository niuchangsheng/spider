# è¿™æ˜¯æ–°çš„ main å‡½æ•° - å­å‘½ä»¤æ¨¡å¼
# å®Œæˆæµ‹è¯•åå°†æ›¿æ¢ spider.py ä¸­çš„ main å‡½æ•°

import asyncio
import argparse
from pathlib import Path
import sys
from loguru import logger

# å¯¼å…¥å¿…è¦çš„æ¨¡å—ï¼ˆå®é™…ä½¿ç”¨æ—¶éœ€è¦ï¼‰
# from config import Config, ConfigLoader, ForumPresets, get_example_config, get_forum_boards, get_forum_urls
# from spider import SpiderFactory

async def main():
    """ä¸»å‡½æ•° - å­å‘½ä»¤æ¨¡å¼ (v2.1)"""
    # ä¸»è§£æå™¨
    parser = argparse.ArgumentParser(
        prog='spider.py',
        description='BBSå›¾ç‰‡çˆ¬è™« (v2.1 - å­å‘½ä»¤æ¨¡å¼)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
ç¤ºä¾‹:
  # çˆ¬å–å•ä¸ªURLï¼ˆè‡ªåŠ¨æ£€æµ‹é…ç½®ï¼‰
  python spider.py crawl-url "https://bbs.xd.com/thread/123" --auto-detect
  
  # çˆ¬å–å•ä¸ªURLï¼ˆä½¿ç”¨é…ç½®ï¼‰
  python spider.py crawl-url "https://bbs.xd.com/thread/123" --config xindong
  
  # çˆ¬å–é…ç½®ä¸­çš„æ‰€æœ‰URLs
  python spider.py crawl-urls --config xindong
  
  # çˆ¬å–æ¿å—ï¼ˆå‰5é¡µï¼‰
  python spider.py crawl-board "https://bbs.xd.com/forum?fid=21" --config xindong --max-pages 5
  
  # çˆ¬å–é…ç½®ä¸­çš„æ‰€æœ‰æ¿å—
  python spider.py crawl-boards --config xindong
        '''
    )
    
    # åˆ›å»ºå­å‘½ä»¤
    subparsers = parser.add_subparsers(dest='command', help='å­å‘½ä»¤', required=True)
    
    # ============================================================================
    # å­å‘½ä»¤1: crawl-url - çˆ¬å–å•ä¸ªURL
    # ============================================================================
    parser_url = subparsers.add_parser('crawl-url', help='çˆ¬å–å•ä¸ªURL')
    parser_url.add_argument('url', type=str, help='å¸–å­URL')
    
    config_group_url = parser_url.add_mutually_exclusive_group()
    config_group_url.add_argument('--auto-detect', action='store_true',
                                  help='è‡ªåŠ¨æ£€æµ‹è®ºå›ç±»å‹')
    config_group_url.add_argument('--preset', type=str,
                                  help='è®ºå›ç±»å‹é¢„è®¾ (discuz/phpbb/vbulletin)')
    config_group_url.add_argument('--config', type=str,
                                  help='é…ç½®æ–‡ä»¶å (ä» configs/ åŠ è½½)')
    
    # ============================================================================
    # å­å‘½ä»¤2: crawl-urls - çˆ¬å–é…ç½®ä¸­çš„URLåˆ—è¡¨
    # ============================================================================
    parser_urls = subparsers.add_parser('crawl-urls', help='çˆ¬å–é…ç½®ä¸­çš„URLåˆ—è¡¨')
    parser_urls.add_argument('--config', type=str, required=True,
                            help='é…ç½®æ–‡ä»¶å (å¿…éœ€)')
    
    # ============================================================================
    # å­å‘½ä»¤3: crawl-board - çˆ¬å–å•ä¸ªæ¿å—
    # ============================================================================
    parser_board = subparsers.add_parser('crawl-board', help='çˆ¬å–å•ä¸ªæ¿å—')
    parser_board.add_argument('board_url', type=str, help='æ¿å—URL')
    parser_board.add_argument('--max-pages', type=int, default=None,
                             help='æœ€å¤§é¡µæ•°ï¼ˆé»˜è®¤ï¼šçˆ¬å–æ‰€æœ‰é¡µï¼‰')
    
    config_group_board = parser_board.add_mutually_exclusive_group()
    config_group_board.add_argument('--auto-detect', action='store_true',
                                   help='è‡ªåŠ¨æ£€æµ‹è®ºå›ç±»å‹')
    config_group_board.add_argument('--preset', type=str,
                                   help='è®ºå›ç±»å‹é¢„è®¾ (discuz/phpbb/vbulletin)')
    config_group_board.add_argument('--config', type=str,
                                   help='é…ç½®æ–‡ä»¶å (ä» configs/ åŠ è½½)')
    
    # ============================================================================
    # å­å‘½ä»¤4: crawl-boards - çˆ¬å–é…ç½®ä¸­çš„æ‰€æœ‰æ¿å—
    # ============================================================================
    parser_boards = subparsers.add_parser('crawl-boards', help='çˆ¬å–é…ç½®ä¸­çš„æ‰€æœ‰æ¿å—')
    parser_boards.add_argument('--config', type=str, required=True,
                              help='é…ç½®æ–‡ä»¶å (å¿…éœ€)')
    parser_boards.add_argument('--max-pages', type=int, default=None,
                              help='æ¯ä¸ªæ¿å—æœ€å¤§é¡µæ•°ï¼ˆé»˜è®¤ï¼šçˆ¬å–æ‰€æœ‰é¡µï¼‰')
    
    args = parser.parse_args()
    
    # é…ç½®æ—¥å¿—
    logger.remove()
    logger.add(
        sys.stderr,
        format="<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | <level>{message}</level>",
        level="INFO",
        colorize=True
    )
    
    log_file = Path(__file__).parent / "logs" / "spider.log"
    log_file.parent.mkdir(parents=True, exist_ok=True)
    logger.add(
        log_file,
        rotation="100 MB",
        retention="30 days",
        encoding="utf-8",
        level="DEBUG"
    )
    
    print("\n" + "=" * 60)
    print("ğŸ•·ï¸  BBSå›¾ç‰‡çˆ¬è™« (v2.1 - å­å‘½ä»¤æ¨¡å¼)")
    print("=" * 60)
    
    # æ ¹æ®å­å‘½ä»¤æ‰§è¡Œç›¸åº”æ“ä½œ
    if args.command == 'crawl-url':
        await handle_crawl_url(args)
    elif args.command == 'crawl-urls':
        await handle_crawl_urls(args)
    elif args.command == 'crawl-board':
        await handle_crawl_board(args)
    elif args.command == 'crawl-boards':
        await handle_crawl_boards(args)


async def handle_crawl_url(args):
    """å¤„ç† crawl-url å­å‘½ä»¤"""
    from config import ConfigLoader, get_example_config
    from spider import SpiderFactory
    
    print(f"\nğŸ“Œ å‘½ä»¤: çˆ¬å–å•ä¸ªURL")
    print(f"URL: {args.url}")
    
    # 1. åŠ è½½é…ç½®
    if args.auto_detect:
        logger.info(f"ğŸŒ è‡ªåŠ¨æ£€æµ‹é…ç½®: {args.url}")
        config = await ConfigLoader.auto_detect_config(args.url)
    elif args.preset:
        logger.info(f"ğŸ“‹ ä½¿ç”¨è®ºå›ç±»å‹é¢„è®¾: {args.preset}")
        config = ConfigLoader.load(args.preset)
    elif args.config:
        logger.info(f"ğŸ“ ä½¿ç”¨é…ç½®æ–‡ä»¶: {args.config}")
        config = get_example_config(args.config)
    else:
        logger.error("âŒ è¯·æŒ‡å®šé…ç½®æ¥æº: --auto-detect æˆ– --preset æˆ– --config")
        return
    
    # 2. åˆ›å»ºçˆ¬è™«
    spider = SpiderFactory.create(config=config)
    
    # 3. çˆ¬å–URL
    async with spider:
        thread_id = spider.parser._extract_thread_id(args.url)
        thread_info = {
            'url': args.url,
            'thread_id': thread_id,
            'title': f'Thread-{thread_id}',
            'board': config.bbs.name
        }
        
        logger.info(f"ğŸš€ å¼€å§‹çˆ¬å–URL...")
        await spider.crawl_thread(thread_info)
        logger.info(f"âœ… çˆ¬å–å®Œæˆ")
        
        # è¾“å‡ºç»Ÿè®¡
        print_statistics(spider)


async def handle_crawl_urls(args):
    """å¤„ç† crawl-urls å­å‘½ä»¤"""
    from config import get_example_config, get_forum_urls
    from spider import SpiderFactory
    
    print(f"\nğŸ“Œ å‘½ä»¤: çˆ¬å–é…ç½®ä¸­çš„URLåˆ—è¡¨")
    print(f"é…ç½®: {args.config}")
    
    # 1. åŠ è½½é…ç½®
    logger.info(f"ğŸ“ ä½¿ç”¨é…ç½®æ–‡ä»¶: {args.config}")
    config = get_example_config(args.config)
    
    # 2. è·å–URLåˆ—è¡¨
    urls = get_forum_urls(args.config)
    logger.info(f"ğŸ“ ä»é…ç½®æ–‡ä»¶åŠ è½½URL: {len(urls)} ä¸ª")
    
    if not urls:
        logger.error("âŒ é…ç½®æ–‡ä»¶ä¸­æ²¡æœ‰URLsï¼")
        return
    
    # 3. åˆ›å»ºçˆ¬è™«å¹¶å¹¶å‘çˆ¬å–
    spider = SpiderFactory.create(config=config)
    
    async with spider:
        logger.info(f"ğŸš€ å¼€å§‹å¹¶å‘çˆ¬å– {len(urls)} ä¸ªURL...")
        tasks = []
        for url in urls:
            thread_id = spider.parser._extract_thread_id(url)
            thread_info = {
                'url': url,
                'thread_id': thread_id,
                'title': f'Thread-{thread_id}',
                'board': config.bbs.name
            }
            tasks.append(spider.crawl_thread(thread_info))
        
        # ä½¿ç”¨ asyncio.gather å¹¶å‘æ‰§è¡Œ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for r in results if not isinstance(r, Exception))
        failed_count = len(results) - success_count
        logger.info(f"âœ… å®Œæˆ: æˆåŠŸ {success_count}, å¤±è´¥ {failed_count}")
        
        # è¾“å‡ºç»Ÿè®¡
        print_statistics(spider)


async def handle_crawl_board(args):
    """å¤„ç† crawl-board å­å‘½ä»¤"""
    from config import ConfigLoader, get_example_config
    from spider import SpiderFactory
    
    print(f"\nğŸ“Œ å‘½ä»¤: çˆ¬å–å•ä¸ªæ¿å—")
    print(f"æ¿å—URL: {args.board_url}")
    if args.max_pages:
        print(f"æœ€å¤§é¡µæ•°: {args.max_pages}")
    else:
        print(f"æœ€å¤§é¡µæ•°: ä¸é™åˆ¶ï¼ˆçˆ¬å–æ‰€æœ‰é¡µï¼‰")
    
    # 1. åŠ è½½é…ç½®
    if args.auto_detect:
        logger.info(f"ğŸŒ è‡ªåŠ¨æ£€æµ‹é…ç½®: {args.board_url}")
        config = await ConfigLoader.auto_detect_config(args.board_url)
    elif args.preset:
        logger.info(f"ğŸ“‹ ä½¿ç”¨è®ºå›ç±»å‹é¢„è®¾: {args.preset}")
        config = ConfigLoader.load(args.preset)
    elif args.config:
        logger.info(f"ğŸ“ ä½¿ç”¨é…ç½®æ–‡ä»¶: {args.config}")
        config = get_example_config(args.config)
    else:
        logger.error("âŒ è¯·æŒ‡å®šé…ç½®æ¥æº: --auto-detect æˆ– --preset æˆ– --config")
        return
    
    # 2. åˆ›å»ºçˆ¬è™«
    spider = SpiderFactory.create(config=config)
    
    # 3. çˆ¬å–æ¿å—
    async with spider:
        logger.info(f"ğŸš€ å¼€å§‹çˆ¬å–æ¿å—...")
        await spider.crawl_board(
            board_url=args.board_url,
            board_name=config.bbs.name,
            max_pages=args.max_pages
        )
        logger.info(f"âœ… çˆ¬å–å®Œæˆ")
        
        # è¾“å‡ºç»Ÿè®¡
        print_statistics(spider)


async def handle_crawl_boards(args):
    """å¤„ç† crawl-boards å­å‘½ä»¤"""
    from config import get_example_config, get_forum_boards
    from spider import SpiderFactory
    
    print(f"\nğŸ“Œ å‘½ä»¤: çˆ¬å–é…ç½®ä¸­çš„æ‰€æœ‰æ¿å—")
    print(f"é…ç½®: {args.config}")
    if args.max_pages:
        print(f"æ¯ä¸ªæ¿å—æœ€å¤§é¡µæ•°: {args.max_pages}")
    else:
        print(f"æ¯ä¸ªæ¿å—æœ€å¤§é¡µæ•°: ä¸é™åˆ¶ï¼ˆçˆ¬å–æ‰€æœ‰é¡µï¼‰")
    
    # 1. åŠ è½½é…ç½®
    logger.info(f"ğŸ“ ä½¿ç”¨é…ç½®æ–‡ä»¶: {args.config}")
    config = get_example_config(args.config)
    
    # 2. è·å–æ¿å—åˆ—è¡¨
    boards_info = get_forum_boards(args.config)
    logger.info(f"ğŸ“ ä»é…ç½®æ–‡ä»¶åŠ è½½æ¿å—: {len(boards_info)} ä¸ª")
    
    if not boards_info:
        logger.error("âŒ é…ç½®æ–‡ä»¶ä¸­æ²¡æœ‰æ¿å—ï¼")
        return
    
    # 3. åˆ›å»ºçˆ¬è™«å¹¶å¹¶å‘çˆ¬å–
    spider = SpiderFactory.create(config=config)
    
    async with spider:
        if args.max_pages:
            logger.info(f"ğŸš€ å¼€å§‹å¹¶å‘çˆ¬å– {len(boards_info)} ä¸ªæ¿å—ï¼ˆæ¯ä¸ªæœ€å¤š {args.max_pages} é¡µï¼‰...")
        else:
            logger.info(f"ğŸš€ å¼€å§‹å¹¶å‘çˆ¬å– {len(boards_info)} ä¸ªæ¿å—ï¼ˆçˆ¬å–æ‰€æœ‰é¡µé¢ï¼‰...")
        
        tasks = []
        for board in boards_info:
            logger.info(f"ğŸ“ æ¿å—: {board['name']} - {board['url']}")
            tasks.append(spider.crawl_board(
                board_url=board['url'],
                board_name=board['name'],
                max_pages=args.max_pages
            ))
        
        # ä½¿ç”¨ asyncio.gather å¹¶å‘æ‰§è¡Œ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for r in results if not isinstance(r, Exception))
        failed_count = len(results) - success_count
        logger.info(f"âœ… å®Œæˆ: æˆåŠŸ {success_count}, å¤±è´¥ {failed_count}")
        
        # è¾“å‡ºç»Ÿè®¡
        print_statistics(spider)


def print_statistics(spider):
    """è¾“å‡ºç»Ÿè®¡ä¿¡æ¯"""
    stats = spider.get_statistics()
    print("\n" + "=" * 60)
    print("ğŸ“Š çˆ¬å–ç»Ÿè®¡:")
    print(f"  å¸–å­æ•°: {stats['threads_crawled']}")
    print(f"  å‘ç°å›¾ç‰‡: {stats['images_found']}")
    print(f"  ä¸‹è½½æˆåŠŸ: {stats['images_downloaded']}")
    print(f"  ä¸‹è½½å¤±è´¥: {stats['images_failed']}")
    print(f"  å»é‡è·³è¿‡: {stats['duplicates_skipped']}")
    print("=" * 60)


if __name__ == "__main__":
    asyncio.run(main())
