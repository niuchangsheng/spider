"""
BBSå›¾ç‰‡çˆ¬è™« - ç»Ÿä¸€æ¶æ„
æ”¯æŒå¤šç§è®ºå›ç³»ç»Ÿï¼šDiscuzã€phpBBã€vBulletinç­‰
"""
import asyncio
import sys
import aiohttp
from typing import List, Dict, Any, Optional, Type
from loguru import logger
from pathlib import Path
from tqdm import tqdm
from fake_useragent import UserAgent

from config import Config, ConfigLoader, ForumPresets, get_example_config, get_forum_boards, get_forum_urls
from core.downloader import ImageDownloader
from core.parser import BBSParser
from core.storage import storage
from core.deduplicator import ImageDeduplicator


class BBSSpider:
    """
    BBSå›¾ç‰‡çˆ¬è™«åŸºç±»
    
    æä¾›é€šç”¨çš„çˆ¬å–é€»è¾‘ï¼Œå­ç±»å¯é‡å†™ç‰¹å®šæ–¹æ³•å®ç°è®ºå›ç‰¹å®šå¤„ç†
    """
    
    def __init__(self, config: Optional[Config] = None, url: Optional[str] = None, preset: Optional[str] = None):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            config: æ‰‹åŠ¨é…ç½®ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
            url: è®ºå›URLï¼Œè‡ªåŠ¨æ£€æµ‹é…ç½®
            preset: è®ºå›ç±»å‹é¢„è®¾ (discuz/phpbb/vbulletin)
        
        Note:
            æ¨èä½¿ç”¨ SpiderFactory.create() è€Œä¸æ˜¯ç›´æ¥å®ä¾‹åŒ–
        
        Examples:
            # âœ… æ¨èï¼šä½¿ç”¨é…ç½®æ–‡ä»¶ï¼ˆè‡ªåŠ¨åŠ è½½ configs/xindong.jsonï¼‰
            config = get_example_config("xindong")
            spider = SpiderFactory.create(config=config)
            
            # âœ… æ¨èï¼šä½¿ç”¨è®ºå›ç±»å‹é¢„è®¾
            spider = SpiderFactory.create(preset="discuz")
            
            # âœ… æ¨èï¼šè‡ªåŠ¨æ£€æµ‹
            spider = SpiderFactory.create(url="https://forum.com/board")
            
            # âš ï¸ ä¸æ¨èï¼šç›´æ¥å®ä¾‹åŒ–ï¼ˆé™¤éè‡ªå®šä¹‰å­ç±»ï¼‰
            spider = BBSSpider(preset="discuz")
        """
        # é…ç½®ä¼˜å…ˆçº§: config > preset > url
        if config:
            self.config = config
        elif preset:
            self.config = ConfigLoader.load(preset)
        elif url:
            self.config = ConfigLoader.auto_detect(url)
        else:
            raise ValueError("å¿…é¡»æä¾› configã€preset æˆ– url å‚æ•°ä¹‹ä¸€")
        
        self.parser = BBSParser()
        self.deduplicator = ImageDeduplicator(use_perceptual_hash=True)
        self.ua = UserAgent()
        self.session: Optional[aiohttp.ClientSession] = None
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "threads_crawled": 0,
            "images_found": 0,
            "images_downloaded": 0,
            "images_failed": 0,
            "duplicates_skipped": 0
        }
        
        logger.info(f"ğŸš€ åˆå§‹åŒ–çˆ¬è™«: {self.config.bbs.name} ({self.config.bbs.forum_type})")
    
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        await self.init()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨é€€å‡º"""
        await self.close()
    
    async def init(self):
        """åˆå§‹åŒ–çˆ¬è™«"""
        logger.info("âš™ï¸  åˆå§‹åŒ–çˆ¬è™«ç»„ä»¶...")
        
        # åˆå§‹åŒ–HTTPä¼šè¯
        timeout = aiohttp.ClientTimeout(total=self.config.crawler.request_timeout)
        self.session = aiohttp.ClientSession(timeout=timeout)
        
        # è¿æ¥æ•°æ®åº“
        storage.connect()
        
        # åŠ è½½å·²å­˜åœ¨çš„æ–‡ä»¶å“ˆå¸Œ
        if self.config.image.enable_deduplication:
            self.deduplicator.load_existing_hashes(self.config.image.download_dir)
        
        logger.success("âœ… çˆ¬è™«åˆå§‹åŒ–å®Œæˆ")
    
    async def close(self):
        """å…³é—­çˆ¬è™«"""
        logger.info("ğŸ”’ å…³é—­çˆ¬è™«...")
        
        if self.session:
            await self.session.close()
        
        storage.close()
        
        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        logger.info(f"ğŸ“Š çˆ¬è™«ç»Ÿè®¡: {self.stats}")
        logger.info(f"ğŸ”„ å»é‡ç»Ÿè®¡: {self.deduplicator.get_stats()}")
    
    def get_headers(self) -> Dict[str, str]:
        """è·å–è¯·æ±‚å¤´"""
        headers = {
            "User-Agent": self.ua.random if self.config.crawler.rotate_user_agent else self.ua.chrome,
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Accept-Encoding": "gzip, deflate",
            "Connection": "keep-alive",
        }
        
        if self.config.bbs.base_url:
            headers["Referer"] = self.config.bbs.base_url
        
        return headers
    
    async def fetch_page(self, url: str) -> Optional[str]:
        """è·å–é¡µé¢å†…å®¹"""
        try:
            logger.debug(f"ğŸ“„ è·å–é¡µé¢: {url}")
            
            async with self.session.get(url, headers=self.get_headers()) as response:
                if response.status == 200:
                    html = await response.text()
                    await asyncio.sleep(self.config.crawler.download_delay)
                    return html
                else:
                    logger.warning(f"âš ï¸  è·å–å¤±è´¥ {url}: HTTP {response.status}")
                    return None
        
        except Exception as e:
            logger.error(f"âŒ è·å–å‡ºé”™ {url}: {e}")
            return None
    
    async def process_images(self, images: List[str]) -> List[str]:
        """
        å¤„ç†å›¾ç‰‡URLï¼ˆé’©å­æ–¹æ³•ï¼‰
        
        å­ç±»å¯é‡å†™æ­¤æ–¹æ³•å®ç°è®ºå›ç‰¹å®šçš„å›¾ç‰‡å¤„ç†é€»è¾‘
        
        Args:
            images: åŸå§‹å›¾ç‰‡URLåˆ—è¡¨
        
        Returns:
            å¤„ç†åçš„å›¾ç‰‡URLåˆ—è¡¨
        """
        return images
    
    async def crawl_board(self, board_url: str, board_name: str, max_pages: Optional[int] = None):
        """
        çˆ¬å–æ¿å—
        
        Args:
            board_url: æ¿å—URL
            board_name: æ¿å—åç§°
            max_pages: æœ€å¤§é¡µæ•°
        """
        logger.info(f"ğŸ“š å¼€å§‹çˆ¬å–æ¿å—: {board_name}")
        
        current_url = board_url
        page_count = 0
        
        while current_url and (max_pages is None or page_count < max_pages):
            page_count += 1
            logger.info(f"ğŸ“„ çˆ¬å–ç¬¬ {page_count} é¡µ: {current_url}")
            
            # è·å–åˆ—è¡¨é¡µ
            html = await self.fetch_page(current_url)
            if not html:
                break
            
            # è§£æå¸–å­åˆ—è¡¨
            threads = self.parser.parse_thread_list(html, current_url)
            logger.info(f"âœ… å‘ç° {len(threads)} ä¸ªå¸–å­")
            
            # çˆ¬å–æ¯ä¸ªå¸–å­
            for thread in threads:
                thread['board'] = board_name
                await self.crawl_thread(thread)
            
            # æŸ¥æ‰¾ä¸‹ä¸€é¡µ
            current_url = self.parser.find_next_page(html, current_url)
            if not current_url:
                logger.info("ğŸ“Œ æ²¡æœ‰æ›´å¤šé¡µé¢")
                break
        
        logger.success(f"ğŸ‰ æ¿å—çˆ¬å–å®Œæˆ: {board_name}, æ€»é¡µæ•°: {page_count}")
    
    async def crawl_thread(self, thread_info: Dict[str, Any]):
        """
        çˆ¬å–å•ä¸ªå¸–å­
        
        Args:
            thread_info: å¸–å­ä¿¡æ¯å­—å…¸
        """
        thread_url = thread_info['url']
        thread_id = thread_info['thread_id']
        
        # æ£€æŸ¥æ˜¯å¦å·²çˆ¬å–
        if storage.thread_exists(thread_id):
            logger.info(f"â­ï¸  å¸–å­ {thread_id} å·²çˆ¬å–ï¼Œè·³è¿‡")
            return
        
        logger.info(f"ğŸ“ çˆ¬å–å¸–å­: {thread_info.get('title', thread_id)}")
        
        # è·å–å¸–å­é¡µé¢
        html = await self.fetch_page(thread_url)
        if not html:
            return
        
        # è§£æå¸–å­å†…å®¹
        thread_data = self.parser.parse_thread_page(html, thread_url)
        thread_data['board'] = thread_info.get('board')
        thread_data['title'] = thread_info.get('title')
        
        # è®ºå›ç‰¹å®šå¤„ç†ï¼ˆç­–ç•¥æ¨¡å¼ - å­ç±»å¯é‡å†™ï¼‰
        thread_data['images'] = await self.process_images(thread_data['images'])
        
        # æ›´æ–°ç»Ÿè®¡
        self.stats['threads_crawled'] += 1
        self.stats['images_found'] += len(thread_data['images'])
        
        logger.info(f"ğŸ–¼ï¸  å‘ç° {len(thread_data['images'])} å¼ å›¾ç‰‡")
        
        # ä¸‹è½½å›¾ç‰‡
        if thread_data['images']:
            await self.download_thread_images(thread_data)
        
        # ä¿å­˜å¸–å­æ•°æ®
        storage.save_thread(thread_data)
    
    async def download_thread_images(self, thread_data: Dict[str, Any]):
        """ä¸‹è½½å¸–å­ä¸­çš„å›¾ç‰‡"""
        images = thread_data['images']
        thread_id = thread_data['thread_id']
        board = thread_data.get('board', 'unknown')
        
        # è¿‡æ»¤é‡å¤URL
        unique_images = []
        for img_url in images:
            if self.config.image.enable_deduplication:
                if not self.deduplicator.is_duplicate_url(img_url):
                    unique_images.append(img_url)
                else:
                    self.stats['duplicates_skipped'] += 1
            else:
                unique_images.append(img_url)
        
        if not unique_images:
            logger.info(f"â­ï¸  æ²¡æœ‰æ–°å›¾ç‰‡éœ€è¦ä¸‹è½½")
            return
        
        logger.info(f"â¬‡ï¸  ä¸‹è½½ {len(unique_images)} å¼ å›¾ç‰‡...")
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        save_dir = self.config.image.download_dir / board / thread_id
        save_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¸‹è½½å›¾ç‰‡
        async with ImageDownloader() as downloader:
            metadata = {
                'board': board,
                'thread_id': thread_id,
                'thread_url': thread_data['url']
            }
            
            results = await downloader.download_batch(
                unique_images,
                save_dir,
                metadata
            )
            
            # ç»Ÿè®¡ç»“æœ
            for result in results:
                if result.get('success'):
                    self.stats['images_downloaded'] += 1
                    
                    # æ£€æŸ¥æ–‡ä»¶å»é‡
                    if self.config.image.enable_deduplication:
                        file_path = Path(result['save_path'])
                        if self.deduplicator.is_duplicate_file(file_path):
                            self.deduplicator.remove_duplicate_file(file_path)
                            self.stats['duplicates_skipped'] += 1
                            self.stats['images_downloaded'] -= 1
                            continue
                    
                    # ä¿å­˜å›¾ç‰‡è®°å½•
                    storage.save_image_record(result)
                else:
                    self.stats['images_failed'] += 1
    
    async def crawl_threads_from_list(self, thread_urls: List[str]):
        """
        ä»URLåˆ—è¡¨çˆ¬å–å¸–å­
        
        Args:
            thread_urls: å¸–å­URLåˆ—è¡¨
        """
        logger.info(f"ğŸ“‹ æ‰¹é‡çˆ¬å– {len(thread_urls)} ä¸ªå¸–å­")
        
        for url in tqdm(thread_urls, desc="çˆ¬å–è¿›åº¦"):
            thread_info = {
                'url': url,
                'thread_id': self.parser._extract_thread_id(url)
            }
            await self.crawl_thread(thread_info)
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.stats.copy()
        stats['deduplication'] = self.deduplicator.get_stats()
        stats['storage'] = storage.get_statistics()
        return stats


class DiscuzSpider(BBSSpider):
    """
    Discuzè®ºå›ä¸“ç”¨çˆ¬è™«
    
    å¤„ç†Discuzç‰¹æœ‰çš„å›¾ç‰‡é“¾æ¥æ ¼å¼å’Œé™„ä»¶ç³»ç»Ÿ
    """
    
    async def process_images(self, images: List[str]) -> List[str]:
        """
        å¤„ç†Discuzç‰¹æ®Šå›¾ç‰‡é“¾æ¥
        
        Discuzçš„é™„ä»¶é“¾æ¥æ ¼å¼: forum.php?mod=attachment&aid=xxx
        éœ€è¦æ·»åŠ  &nothumb=yes å‚æ•°è·å–åŸå›¾
        """
        processed_images = []
        
        for img_url in images:
            # å¤„ç†ç›¸å¯¹è·¯å¾„
            if img_url.startswith('forum.php') or img_url.startswith('/forum.php'):
                img_url = f"{self.config.bbs.base_url}/{img_url.lstrip('/')}"
            
            # Discuzé™„ä»¶é“¾æ¥éœ€è¦æ·»åŠ åŸå›¾å‚æ•°
            if 'mod=attachment' in img_url and 'nothumb' not in img_url:
                img_url += '&nothumb=yes'
            
            processed_images.append(img_url)
        
        return processed_images


class PhpBBSpider(BBSSpider):
    """
    phpBBè®ºå›ä¸“ç”¨çˆ¬è™«
    
    å¤„ç†phpBBç‰¹æœ‰çš„ç»“æ„
    """
    pass  # æš‚æ—¶ä½¿ç”¨åŸºç±»é€»è¾‘ï¼Œå¯æ ¹æ®éœ€è¦æ‰©å±•


class VBulletinSpider(BBSSpider):
    """
    vBulletinè®ºå›ä¸“ç”¨çˆ¬è™«
    
    å¤„ç†vBulletinç‰¹æœ‰çš„ç»“æ„
    """
    pass  # æš‚æ—¶ä½¿ç”¨åŸºç±»é€»è¾‘ï¼Œå¯æ ¹æ®éœ€è¦æ‰©å±•


# ============================================================================
# çˆ¬è™«å·¥å‚
# ============================================================================

class SpiderFactory:
    """
    çˆ¬è™«å·¥å‚ç±»
    
    æ ¹æ®é…ç½®çš„è®ºå›ç±»å‹è‡ªåŠ¨åˆ›å»ºåˆé€‚çš„çˆ¬è™«å®ä¾‹
    """
    
    # çˆ¬è™«ç±»å‹æ³¨å†Œè¡¨
    _registry: Dict[str, Type[BBSSpider]] = {
        'discuz': DiscuzSpider,
        'phpbb': PhpBBSpider,
        'vbulletin': VBulletinSpider,
        'generic': BBSSpider,
    }
    
    @classmethod
    def register(cls, forum_type: str, spider_class: Type[BBSSpider]):
        """
        æ³¨å†Œæ–°çš„çˆ¬è™«ç±»å‹
        
        Args:
            forum_type: è®ºå›ç±»å‹æ ‡è¯†
            spider_class: çˆ¬è™«ç±»
        
        Examples:
            SpiderFactory.register('mybb', MyBBSpider)
        """
        cls._registry[forum_type] = spider_class
        logger.info(f"âœ… æ³¨å†Œçˆ¬è™«ç±»å‹: {forum_type} -> {spider_class.__name__}")
    
    @classmethod
    def create(cls, config: Optional[Config] = None, url: Optional[str] = None, preset: Optional[str] = None) -> BBSSpider:
        """
        åˆ›å»ºçˆ¬è™«å®ä¾‹ï¼ˆå·¥å‚æ–¹æ³•ï¼‰
        
        Args:
            config: é…ç½®å¯¹è±¡ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
            url: è®ºå›URLï¼Œè‡ªåŠ¨æ£€æµ‹é…ç½®
            preset: è®ºå›ç±»å‹é¢„è®¾ (discuz/phpbb/vbulletin)
        
        Returns:
            å¯¹åº”ç±»å‹çš„çˆ¬è™«å®ä¾‹ (BBSSpider æˆ–å…¶å­ç±»)
        
        Examples:
            # âœ… æ–¹å¼1: ä½¿ç”¨é…ç½®æ–‡ä»¶ï¼ˆæ¨èï¼‰
            from config import get_example_config
            config = get_example_config("xindong")  # è‡ªåŠ¨åŠ è½½ configs/xindong.json
            spider = SpiderFactory.create(config=config)
            
            # âœ… æ–¹å¼2: ä½¿ç”¨è®ºå›ç±»å‹é¢„è®¾
            spider = SpiderFactory.create(preset="discuz")
            
            # âœ… æ–¹å¼3: è‡ªåŠ¨æ£€æµ‹è®ºå›ç±»å‹
            spider = SpiderFactory.create(url="https://forum.com/board")
            
            # âœ… æ–¹å¼4: å®Œå…¨è‡ªå®šä¹‰é…ç½®
            from config import Config
            custom_config = Config(bbs={...}, crawler={...})
            spider = SpiderFactory.create(config=custom_config)
        """
        # å…ˆè·å–é…ç½®
        if config:
            final_config = config
        elif preset:
            final_config = ConfigLoader.load(preset)
        elif url:
            final_config = ConfigLoader.auto_detect(url)
        else:
            raise ValueError("å¿…é¡»æä¾› configã€preset æˆ– url å‚æ•°ä¹‹ä¸€")
        
        # æ ¹æ®forum_typeé€‰æ‹©çˆ¬è™«ç±»
        forum_type = final_config.bbs.forum_type.lower()
        spider_class = cls._registry.get(forum_type, BBSSpider)
        
        logger.info(f"ğŸ­ åˆ›å»ºçˆ¬è™«: {spider_class.__name__}")
        
        return spider_class(config=final_config)


# ============================================================================
# ä¾¿æ·å‡½æ•°
# ============================================================================

async def crawl_single_thread(thread_url: str, preset: str = "xindong"):
    """
    çˆ¬å–å•ä¸ªå¸–å­ï¼ˆä¾¿æ·å‡½æ•°ï¼‰
    
    Args:
        thread_url: å¸–å­URL
        preset: é¢„è®¾é…ç½®åç§°
    """
    async with SpiderFactory.create(preset=preset) as spider:
        thread_info = {
            'url': thread_url,
            'thread_id': spider.parser._extract_thread_id(thread_url),
        }
        await spider.crawl_thread(thread_info)
        return spider.get_statistics()


async def crawl_board(board_url: str, board_name: str, max_pages: int = 3, preset: str = "xindong"):
    """
    çˆ¬å–æ¿å—ï¼ˆä¾¿æ·å‡½æ•°ï¼‰
    
    Args:
        board_url: æ¿å—URL
        board_name: æ¿å—åç§°
        max_pages: æœ€å¤§é¡µæ•°
        preset: é¢„è®¾é…ç½®åç§°
    """
    async with SpiderFactory.create(preset=preset) as spider:
        await spider.crawl_board(board_url, board_name, max_pages)
        return spider.get_statistics()


# ============================================================================
# ä¸»å‡½æ•°ç¤ºä¾‹
# ============================================================================

async def main():
    """ä¸»å‡½æ•°ç¤ºä¾‹ - å±•ç¤ºå¤šç§ä½¿ç”¨æ–¹å¼"""
    import argparse
    
    # å‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(
        description='BBSå›¾ç‰‡çˆ¬è™« (v2.0)',
        epilog='ç¤ºä¾‹: python spider.py --config xindong --mode 1'
    )
    
    # é…ç½®æ¥æºï¼ˆäº’æ–¥ç»„ï¼‰
    config_group = parser.add_mutually_exclusive_group()
    config_group.add_argument('--preset', type=str,
                             help='è®ºå›ç±»å‹é¢„è®¾ (discuz/phpbb/vbulletin)')
    config_group.add_argument('--config', type=str, default="xindong",
                             help='é…ç½®æ–‡ä»¶å (ä» configs/ åŠ è½½ï¼Œå¦‚: xindong)')
    config_group.add_argument('--url', type=str,
                             help='è®ºå›URLï¼ˆè‡ªåŠ¨æ£€æµ‹é…ç½®ï¼‰')
    
    # å¤„ç†æ¨¡å¼
    parser.add_argument('--mode', type=int, default=1, choices=[1, 2],
                       help='å¤„ç†æ¨¡å¼: 1=URLåˆ—è¡¨, 2=æ¿å—åˆ—è¡¨')
    
    # å¯é€‰å‚æ•°
    parser.add_argument('--urls', type=str,
                       help='URLåˆ—è¡¨ï¼Œé€—å·åˆ†éš”ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰')
    parser.add_argument('--boards', type=str,
                       help='æ¿å—URLåˆ—è¡¨ï¼Œé€—å·åˆ†éš”ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰')
    parser.add_argument('--max-pages', type=int, default=None,
                       help='æ¯ä¸ªæ¿å—æœ€å¤§çˆ¬å–é¡µæ•°ï¼ˆmode 2ï¼Œé»˜è®¤ä¸é™åˆ¶ï¼Œçˆ¬å–æ‰€æœ‰é¡µï¼‰')
    
    args = parser.parse_args()
    
    # é…ç½®æ—¥å¿—
    logger.remove()
    logger.add(
        sys.stderr,
        format="<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | <level>{message}</level>",
        level="INFO",
        colorize=True
    )
    
    log_file = Path(__file__).parent / "logs" / "spider.log"
    log_file.parent.mkdir(parents=True, exist_ok=True)
    logger.add(
        log_file,
        rotation="100 MB",
        retention="30 days",
        encoding="utf-8",
        level="DEBUG"
    )
    
    print("\n" + "=" * 60)
    print("ğŸ•·ï¸  BBSå›¾ç‰‡çˆ¬è™« (v2.0)")
    print("=" * 60)
    
    # 1. åŠ è½½é…ç½®
    config_name = None
    if args.config:
        logger.info(f"ğŸ“ ä½¿ç”¨é…ç½®æ–‡ä»¶: {args.config}")
        config = get_example_config(args.config)
        config_name = args.config
    elif args.preset:
        logger.info(f"ğŸ“‹ ä½¿ç”¨è®ºå›ç±»å‹é¢„è®¾: {args.preset}")
        config = ConfigLoader.load(args.preset)
    elif args.url:
        logger.info(f"ğŸŒ è‡ªåŠ¨æ£€æµ‹é…ç½®: {args.url}")
        config = await ConfigLoader.auto_detect_config(args.url)
    else:
        # é»˜è®¤ä½¿ç”¨ xindong
        logger.info("ğŸ“ ä½¿ç”¨é»˜è®¤é…ç½®: xindong")
        config = get_example_config("xindong")
        config_name = "xindong"
    
    # 2. åˆ›å»ºçˆ¬è™«
    spider = await SpiderFactory.create(config=config)
    
    async with spider:
        # 3. æ ¹æ®æ¨¡å¼æ‰§è¡Œä»»åŠ¡
        if args.mode == 1:
            # æ¨¡å¼1: æ‰¹é‡çˆ¬å–URLåˆ—è¡¨
            print(f"\nğŸ“Œ æ¨¡å¼1: æ‰¹é‡çˆ¬å–URLåˆ—è¡¨")
            
            # è·å–URLåˆ—è¡¨
            if args.urls:
                urls = [u.strip() for u in args.urls.split(',')]
                logger.info(f"ğŸ“ ä½¿ç”¨å‘½ä»¤è¡ŒURL: {len(urls)} ä¸ª")
            elif config_name:
                urls = get_forum_urls(config_name)
                logger.info(f"ğŸ“ ä»é…ç½®æ–‡ä»¶åŠ è½½URL: {len(urls)} ä¸ª")
            else:
                urls = []
            
            if not urls:
                logger.error("âŒ æ²¡æœ‰URLå¯çˆ¬å–ï¼è¯·æä¾› --urls æˆ–åœ¨é…ç½®æ–‡ä»¶ä¸­å®šä¹‰")
                return
            
            # å¹¶å‘çˆ¬å–
            logger.info(f"ğŸš€ å¼€å§‹å¹¶å‘çˆ¬å– {len(urls)} ä¸ªURL...")
            tasks = []
            for url in urls:
                thread_id = spider.parser._extract_thread_id(url)
                thread_info = {
                    'url': url,
                    'thread_id': thread_id,
                    'title': f'Thread-{thread_id}',
                    'board': config.bbs.name
                }
                tasks.append(spider.crawl_thread(thread_info))
            
            # ä½¿ç”¨ asyncio.gather å¹¶å‘æ‰§è¡Œ
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # ç»Ÿè®¡ç»“æœ
            success_count = sum(1 for r in results if not isinstance(r, Exception))
            failed_count = len(results) - success_count
            logger.info(f"âœ… å®Œæˆ: æˆåŠŸ {success_count}, å¤±è´¥ {failed_count}")
        
        elif args.mode == 2:
            # æ¨¡å¼2: æ‰¹é‡çˆ¬å–æ¿å—åˆ—è¡¨
            print(f"\nğŸ“Œ æ¨¡å¼2: æ‰¹é‡çˆ¬å–æ¿å—åˆ—è¡¨")
            
            # è·å–æ¿å—åˆ—è¡¨
            if args.boards:
                board_urls = [u.strip() for u in args.boards.split(',')]
                boards_info = [{"name": f"Board-{i+1}", "url": url} for i, url in enumerate(board_urls)]
                logger.info(f"ğŸ“ ä½¿ç”¨å‘½ä»¤è¡Œæ¿å—: {len(boards_info)} ä¸ª")
            elif config_name:
                boards_info = get_forum_boards(config_name)
                logger.info(f"ğŸ“ ä»é…ç½®æ–‡ä»¶åŠ è½½æ¿å—: {len(boards_info)} ä¸ª")
            else:
                boards_info = []
            
            if not boards_info:
                logger.error("âŒ æ²¡æœ‰æ¿å—å¯çˆ¬å–ï¼è¯·æä¾› --boards æˆ–åœ¨é…ç½®æ–‡ä»¶ä¸­å®šä¹‰")
                return
            
            # å¹¶å‘çˆ¬å–æ¿å—
            if args.max_pages:
                logger.info(f"ğŸš€ å¼€å§‹å¹¶å‘çˆ¬å– {len(boards_info)} ä¸ªæ¿å—ï¼ˆæ¯ä¸ªæœ€å¤š {args.max_pages} é¡µï¼‰...")
            else:
                logger.info(f"ğŸš€ å¼€å§‹å¹¶å‘çˆ¬å– {len(boards_info)} ä¸ªæ¿å—ï¼ˆçˆ¬å–æ‰€æœ‰é¡µé¢ï¼‰...")
            tasks = []
            for board in boards_info:
                logger.info(f"ğŸ“ æ¿å—: {board['name']} - {board['url']}")
                tasks.append(spider.crawl_board(
                    board_url=board['url'],
                    board_name=board['name'],
                    max_pages=args.max_pages
                ))
            
            # ä½¿ç”¨ asyncio.gather å¹¶å‘æ‰§è¡Œ
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # ç»Ÿè®¡ç»“æœ
            success_count = sum(1 for r in results if not isinstance(r, Exception))
            failed_count = len(results) - success_count
            logger.info(f"âœ… å®Œæˆ: æˆåŠŸ {success_count}, å¤±è´¥ {failed_count}")
        
        # 4. è¾“å‡ºç»Ÿè®¡
        stats = spider.get_statistics()
        print("\n" + "=" * 60)
        print("ğŸ“Š çˆ¬å–ç»Ÿè®¡:")
        print(f"  å¸–å­æ•°: {stats['threads_crawled']}")
        print(f"  å‘ç°å›¾ç‰‡: {stats['images_found']}")
        print(f"  ä¸‹è½½æˆåŠŸ: {stats['images_downloaded']}")
        print(f"  ä¸‹è½½å¤±è´¥: {stats['images_failed']}")
        print(f"  å»é‡è·³è¿‡: {stats['duplicates_skipped']}")
        print("=" * 60)


if __name__ == "__main__":
    asyncio.run(main())
